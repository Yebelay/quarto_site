[
  {
    "objectID": "posts/rp_bundesliga-table/index.html",
    "href": "posts/rp_bundesliga-table/index.html",
    "title": "Excess attraction in the Bundesliga",
    "section": "",
    "text": "Footnotes\n\nTechnically, I calculate excess attraction as the difference between the actual number of members a club has and the number of members that we would expect if the total number of all club members are redistributed according to the share of population which live closest to each club.‚Ü©Ô∏é\nThe value calculated by Ansgar Wolsing.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/tt2022-08-09_ferriswheels/index.html",
    "href": "posts/tt2022-08-09_ferriswheels/index.html",
    "title": "How far does that ferris wheel roll?",
    "section": "",
    "text": "What I love about data visualization is how it combines creative thinking (what do I want to show?) with technical programming skills (how do I show this?). Nowhere do you combine these two things as elegantly as in the weekly tidy tuesday challenges. Here, data wizards around the word play and practice with fascinating datasets from chocolate bar ratings to the mobility of ERASMUS students to ‚Äì this week ‚Äì data on ferris wheels around the world.1 As someone who goes pale in any kind of rollercoaster, ferris wheels feel just about advantageous enough to start my own tidy tuesday journey.\n\n\n\nBefore opening R, I think about what I find fascinating about the topic and what I want to convey with my visualization. Here I am sticking to some descriptive analysis and won‚Äôt overthink it, so let‚Äôs just go with one of the first thing that came to my mind‚Ä¶ ferris wheels are really big!\n\nIf a ferris wheel would be an actual wheel, how far would it travel during one rotation?2\n\nThe message gives me part of the structure for the visualization. Here, I want to select a few examples from different countries and show the diameter on the x-axis. I‚Äôve created a lot of horizontal bar charts for a consultancy this summer, so naturally this is my starting point.\nA great thing about these just-for-fun challenges is that you can use them to add 1-2 new skills to your visualization toolbox. For this challenge I want to learn two things:\n\nHow do I add icons (ferris wheels üé°, obviously) instead of dots in the chart?\nHow do I add country flags on the y-axis.\n\nI don‚Äôt want to spend the entire evening on this, so I think it is important to limit myself to only these two new features. I will also concentrate on them and not care too much (yet) about other things, like titles, background, themes, etc‚Ä¶\n\n\n\n\n\n\nImportant\n\n\n\nBecause this isn‚Äôt a mystery novel, I am giving away how the visualization will look like in the end. Don‚Äôt click if you want to keep the suspense‚Ä¶\n\n\n\n\n\nWith these initial thoughts out of the way, I can roughly sketch the visualization process and outline of this post.\n\nA first look at the data. Choose which points to show.\nCreate the simplest graph that shows the intended message.\nAdd the ferris wheel icons.\nAdd the country flags.\nClean up the graph.\n\n\n\n\n\n\n\n\n\nLet‚Äôs first install the necessary packages and load and save the data.\n#install.packages(c(\"tidytuesdayR\", \"tidyverse\", \"here\", \"ggimage\", \"countrycode\", \"ggtext\"))\nlibrary(tidyverse)\nlibrary(here)\n#data &lt;- tidytuesdayR::tt_load(\"2022-08-09\")$wheels\n#write_rds(data, here(\"posts\", \"tt2022-08-09_ferriswheels\", \"data\", \"wheels\"))\ndata &lt;- read_rds(here(\"posts\", \"tt2022-08-09_ferriswheels\", \"data\", \"wheels\"))\n\n\n\nThe entire dataset collects 22 variables on 73 ferris wheels in 26 different countries. This is way to much information to display, so let‚Äôs keep only a handful of ferris wheels and only information on their name, country, height and diameter.\n\ndata &lt;- data |&gt; \n  select(country, name, height, diameter) |&gt; \n  group_by(country) |&gt; \n  filter(height == max(height, na.rm = T),\n         country %in% c(\"Austria\", \"Canada\", \"China\", \"Mexico\", \"UK\", \"USA\")) |&gt;\n  ungroup() |&gt; \n  mutate(across(c(height, diameter), round, 0),\n         country = factor(country))\n\nThis is the data that we will use. As a comparison, I will add data from a more typical wheel: my dutch bicycle wheel.\n\ndata &lt;- bind_rows(data, tibble(country = \"Netherlands\", name = \"Philipp's bike wheel\", height = 0.67, diameter = 0.67))\nknitr::kable(data)\n\n\n\n\ncountry\nname\nheight\ndiameter\n\n\n\n\nChina\nBeijing Great Wheel\n693.00\n643.00\n\n\nUSA\nGolden Gate Flyer\n728.00\n700.00\n\n\nUK\nLondon Eye\n443.00\n394.00\n\n\nCanada\nNiagara SkyWheel\n175.00\n167.00\n\n\nMexico\nStar of Puebla\n262.00\n229.00\n\n\nAustria\nWiener Riesenrad\n212.00\n200.00\n\n\nNetherlands\nPhilipp‚Äôs bike wheel\n0.67\n0.67\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data, aes(y = fct_rev(country), x = diameter)) + \n  labs(x = \"\", y = \"\",\n       # x = \"Diameter of the highest ferris wheel in the country (in meter)\"\n       title = \"How far do you travel during one rotation in the largest ferris wheels?\",\n       caption = \"Data from ferriswheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  scale_x_continuous(limits = c(0, 800)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\np +\n  geom_point() +\n  geom_segment(aes(y = country, yend = country, x = 0, xend = diameter), linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis isn‚Äôt pretty, but that‚Äôs not the point. Having a bare-minimum working example helps to think about whether the graph type fits and what can be done to emphasise the key message. It also helps to create a to-do list of things to improve: like a different order of ferris wheels, maybe adding an empty row to separate the bike data from the ferris wheel data, a label or subtitle that puts the ferris wheel in relation to the bike wheel, another background color, adding ‚Äòmeter‚Äô to the x-axis and more). But let us first tackle the two big learnings that I wanted to get out of the challenge: replacing the dots with icons (now I also need a bike, üö≤) and adding the country flags.\n\n\n\nAs always, there are many ways to Rome. Here I am going with the ggimage package maintained by Guangchuang YU. I have not used this before, but a look at the online vignette makes me hopeful that I can use this for the ferris wheel icons and the country flags üòÉ\nThere are two ways we can go. The first is to use the ggimage::geom_emoji and to insert the ferris wheel and bicycle emojis that I have used throughout the post. That seems simple, we just need the unicode to pass it to the function. That‚Äôs easy to look up at, e.g.¬†https://unicode.org/emoji/charts/full-emoji-list.html.\n\ndata$size = if_else(data$name == \"Philipp's bike wheel\", 0.075, data$diameter / max(data$diameter) * 0.25)\np +\n  geom_segment(aes(y = fct_rev(country), yend = fct_rev(country), x = 0, xend = diameter), linetype = \"dashed\", color = \"gray\", position = position_nudge(y = - 0.25)) +\n  ggimage::geom_emoji(aes(image = if_else(name == \"Philipp's bike wheel\", \"1f6b2\", \"1f3a1\")), position = position_nudge(y = (data$size / 0.3) - 0.25), size = data$size)\n\n\n\n\n\n\n\n\nThis looks OK. The most complicated part was actually that I wanted to scale the size of the image by the size of the ferris wheel, which required making an extra size column and experimenting a lot with position_nudge to let the dashed line go to the bottom of the icon, not to the middle. But I am not super happy with the emoji‚Äôs, so let‚Äôs try to download some images of ferris wheels and use those with ggimage::geom_image() instead.\nI‚Äôll first download a simple transparent ferris wheel png file (from here) and use a bike icon (from here).\n\nferris_path &lt;- here(\"posts\", \"tt2022-08-09_ferriswheels\", \"images\", \"ferriswheel.png\")\nbike_path &lt;- here(\"posts\", \"tt2022-08-09_ferriswheels\", \"images\", \"bike.png\")\n\np1 &lt;- p +\n  geom_segment(aes(y = fct_rev(country), yend = fct_rev(country), x = 0, xend = diameter),\n               linetype = \"dashed\", color = \"gray\", \n               position = position_nudge(y = - 0.25)) +\n  ggimage::geom_image(\n    aes(image = if_else(data$name == \"Philipp's bike wheel\", bike_path, ferris_path)), \n    position = position_nudge(y = (data$size / 0.27) - 0.25, \n                              x = if_else(data$name == \"Philipp's bike wheel\", 15, 0)), \n    size = data$size * 1.5)\np1\n\n\n\n\n\n\n\n\nI‚Äôm happy with this. Onwards to do the country flags!\n\n\n\nIn the plots so far, I kept the country names on the y-axis. However, I would prefer to use the actual names of the ferris wheel and replace the country names with flags. We‚Äôll continue with the ggimage package, which contains a geom_flag() function. The package expects country codes (e.g.¬†NL, US), so we need to transform the names by hand using the countrycodes package.\n\n# Create the country labels from the country names\ndata$code &lt;- countrycode::countrycode(data$country, origin = \"country.name\", destination = \"iso2c\") \n\n# Sort the factor levels of the 'name' variable alphabetically *by country* (the combination of arrange and fct_inorder does that, there are surely smarter ways)\ndata &lt;- data |&gt; arrange(country)\ndata$name &lt;- fct_inorder(factor(data$name))\n\n# Create the graph again from scratch because instead of mapping the country to the y axis, I now\n# map the name to the y axis. Again, surely there are smarter ways to do that...\n# I also move the flags a bit to the left of the x axis and expand the limits of the plot accordingly.\np2 &lt;- \n  ggplot(data, aes(y = fct_rev(name), x = diameter)) + \n  geom_segment(aes(y = fct_rev(name), yend = fct_rev(name), x = 0, xend = diameter),\n               linetype = \"dashed\", color = \"gray\", \n               position = position_nudge(y = - 0.25)) +\n  ggimage::geom_image(aes(image = if_else(name == \"Philipp's bike wheel\", bike_path, ferris_path)),\n                      position = position_nudge(y = (data$size / 0.27) - 0.25,\n                                                x = if_else(data$name == \"Philipp's bike wheel\", 15, 0)),\n                      size = data$size * 1.5) +\n  ggimage::geom_flag(x = -70, aes(image = code)) +\n  labs(x = \"\", y = \"\",\n       title = \"How far do you travel during one rotation in the largest ferris wheels?\",\n       caption = \"Data from ferriswheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  scale_x_continuous(limits = c(-50, 800)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\np2\n\n\n\n\n\n\n\n\nThis looks fine. The main headache in the code chunk above was replacing the mapping of the y axis to show the name (and not the country) and correcting the ordering of the observations.\n\n\n\nI am happy with how the flags and ferris wheels turned out and am now ready to go back to the to-do list from the beginning with some minor things to clean up the graph. This is a playful challenge and ferris wheels are fun, so let‚Äôs go for a very light pinkish background colour. I am also adding a text box (using the ggtext package) to use my bicycle wheel as an example to show just how big these ferris wheels are.\n\n\nCode\nturns &lt;- pull(round(data[which(data$diameter == max(data$diameter)), \"diameter\"] / data[data$name == \"Philipp's bike wheel\", \"diameter\"]))\ndata$name &lt;- fct_relevel(data$name, \"Philipp's bike wheel\")\n\np3 &lt;- \n  ggplot(data, aes(y = fct_rev(name), x = diameter)) + \n  geom_segment(aes(y = fct_rev(name), yend = fct_rev(name), x = 0, xend = diameter),\n               linetype = \"dashed\", color = \"gray\", alpha = 0.5,\n               position = position_nudge(y = -0.25)) +\n  ggimage::geom_image(aes(image = if_else(name == \"Philipp's bike wheel\", bike_path, ferris_path)),\n                      position = position_nudge(y = (data$size / 0.27) - 0.25,\n                                                x = if_else(data$name == \"Philipp's bike wheel\", 15, 0)),\n                      size = data$size * 1.5) +\n  ggimage::geom_flag(x = -70, aes(image = code)) +\n  labs(x = \"\", y = \"\",\n       title = \"How far do you travel with one rotation of the largest wheels in the world?\",\n       caption = \"Visualization by Philipp Kollenda / @pkollenda . Accompanying blog post on my website. \\n Data from ferriswheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  ggtext::geom_textbox(aes(x = 300, y = 6.75, hjust = 0,\n                           label = glue::glue(\"My bike wheel has to &lt;span style='color: red;'&gt;turn {turns} times&lt;/span&gt; to travel the same distance as one rotation with the largest ferris wheel.\")),\n                       fill = \"papayawhip\", width = unit(6, \"cm\"), size = 3) +\n  scale_x_continuous(limits = c(-50, 800), labels = scales::label_number(suffix = \" meter\")) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), legend.position = \"none\",\n        plot.title.position = \"plot\", plot.title = ggtext::element_textbox_simple(size = 20, padding = margin(c(10, 10, 20, 10))),\n        axis.text.y = element_text(size = 12)) +\n  theme(plot.background = element_rect(fill = \"#FFFAFA\"))\np3\n\n\n\n\n\n\n\n\n\n\n\n\nI am happy with how this visualization turned out and glad I learned something about how to insert flags, icons and other images with the ggimage package. I will now hop on my bicycle and think of the Golden Gate Flyer‚Ä¶ approximately every 1045 (bike) wheel turn."
  },
  {
    "objectID": "posts/qp2022-08-05_my-first-website/index.html",
    "href": "posts/qp2022-08-05_my-first-website/index.html",
    "title": "Build a simple website with R and Quarto in under 30 10 minutes.",
    "section": "",
    "text": "I am especially excited about the possibility to quickly build websites (for blogs, teaching, sharing notes, publishing research etc.) with Quarto, without having to leave my R environment where I do all my coding anyways. Let‚Äôs get started!\n\nLet‚Äôs start by building a very simple website. I have three things on my wishlist:\n\nthe website should have multiple pages,\nan intuitive way to navigate between them and\nshow some images, code and output.\n\nWe won‚Äôt worry about hosting yet (that‚Äôs for another post) and simply use quarto.pub (more on that below).\n\n\n\n\n\n\ntl;dr: Here is how to get a website up and running in three steps\n\n\n\n\nIn RStudio, start a new project and choose Quarto website.\nChange content in the .qmd files and configuration in the .yml file.\nPublish to quarto pub via the terminal.\n\n\n\nHere is the final website https://philipp.quarto.pub/my-first-website But, let‚Äôs look at each of the steps in more detail.\n\nStep 1: Create the Quarto website project\nIn RStudio, we create a new project and choose Quarto website from the list of available templates. If that does not show up for you, chances are that you need to update RStudio to the latest version (i.e.¬†RStudio 2022.07.1+554 ‚ÄúSpotted Wakerobin‚Äù works).\nA click on create project will create the scaffolding for our new website:\n\nTwo .qmd files that will become pages on our website.\nA .yml file that we use to keep track of the options.\nA styles.css file that we use to customize the appearance of the website1.\n\nIf we click on ‚ÄúRender‚Äù (or use Shift + Cmd + K) the document will (hopefully) render and show up in the Viewer pane of RStudio. That‚Äôs all and we could already publish this as a functioning website (ü•≥), but let‚Äôs add at least a minimal amount of our own content.\n\n\nStep 2: Changing the content (.qmd) and the structure (.yml) of our website.\nWhen we initialized the project, Quarto graciously already created two .qmd files which will be two pages on our website and a .yml file which records settings about the structure, general options, appearance and so on.\nWe are going to change the default content of the index.qmd file and the name and content of the about.qmd file. To do that, simply open the respective .qmd file and edit what you want to change. We won‚Äôt focus here on how to author Quarto documents, but the documentation on https://quarto.org/docs/authoring is amazing. For completeness, I put the code that I used in our website at the bottom of this post.\nNow, ideally we want to see the effects of our changes immediately. There are two options. We can switch on ‚ÄúRender on Save‚Äù and see the updated page in RStudios Viewer pane whenever we save. Or ‚Äì way cooler IMHO ‚Äì we can preview the entire website by typing quarto preview in the terminal field of RStudio (next to the console, where???).\n\n\n\n\n\n\nPreview a website without publishing it by typing quarto preview in the terminal.\n\n\n\nWhere the .qmd files tell the browser what the website should display, the .yml file tells the browser how to structure the website. This is where you change the design, add pages, choose a logo and so much more. We‚Äôll not go into detail on the site configuration yaml here and just focus on getting a simple website up-and-running, but luckily the Quarto team has you covered.\n\n\nStep 3: Publish your website.\nThis is it! It is time to let our creation leave the confines of the local computer and roam the world wide web. But we need a home address for our website and for the sake of using the simplest possible solution we will publish the website using quarto pub.\nIf you are already signed up and authenticated for Quarto Pub, great! Keep reading below.\n\n\nOtherwise click here to show the details.\n\nIf you still need to make an account, go to the quarto pub website (https://quartopub.com/), click Sign up, confirm your email and choose a username. The username will be in the URL of every website you create, so best to choose something highly professional ü§ì.\n\nOnce you created your account (or if you already had one), open https://quartopub.com (this should show an empty profile page if you have not previously created a website) and just keep it open.\nNow comes the magic. Head back to RStudio and type quarto publish quarto-pub in the terminal (termi-WHAT?). If you are logged in to quarto pub in a browser, the terminal will show ? Publish with account:, your email address on the next line and in gray the words Use another account‚Ä¶. Don‚Äôt be fooled (like I was). This is not an error message and the console is not trying to tell you to use another account, it‚Äôs merely asking‚Ä¶ So, confidently press Enter and it will ask about the site name. Press enter again and voil√†, Quarto will upload your .qmd and .yml files to quarto pub and publish your website.\nYou (and everyone else!) can now see your website at YOUR-USERNAME.quarto.pub/YOUR-WEBSITE-NAME/. Here is the site that I just created in during this post https://philipp.quarto.pub/my-first-website/.\n\n\n\n\nAppendix\n\nStupid mistakes that tripped me up and that you don‚Äôt need to repeat.\n\nWhere is the terminal in RStudio?\nIf you do not use the terminal often it is easy to overlook it even though it is hidden in plain sight.  ##### Why does quarto pub ask me a different name / email address‚Ä¶ When I first published this site, I diligently typed quarto publish quarto-pub in the terminal and was greeted by the message: ‚ÄúUse a different name‚Ä¶‚Äù.\n\n\nI fixed a typo but already published the site, what now?\nDon‚Äôt despair, unlike certain social media sites, quarto.pub does come with an edit button. Simply change what you want to change in the .qmd or .yml files and don‚Äôt forget to publish again via the termina. The console will ask you if you want to update the already published site‚Ä¶ yes, please! That last step is easy to miss though, because RStudio‚Äôs Render on Save and Preview make it so easy to expect that everything just updates automatically.\n\n\n\n\nThe content of the index.qmd file.\nNote that I am not actually including the first line, only the part beginning with the yaml dashes (---).\n```{r}\n---\ntitle: \"Welcome to my first website\"\n---\n\nThis is a Quarto website that I created in just a few minutes. Read about it here and find amazing resources on <https://quarto.org/docs/websites>.\n\n![](images/screenshot.png)\n```\n\n\nThe content of the about.qmd file.\n---\ntitle: \"Ferries Wheels\"\n---\n\n## Some fun with ferries wheels\n\n```{r}\n#| label: ferries-plot\n#| code-line-numbers: true\n#| code-fold: true\n#| message: false\n# install.packages(\"tidytuesdayR\") install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(ggplot2)\n#data <- tidytuesdayR::tt_load(\"2022-08-09\")$wheels\ndata <- read_rds(\"data/wheels\")\nhighest <- data |> \n  group_by(country) |> \n  filter(height == max(height, na.rm = T),\n         country %in% c(\"Canada\", \"China\", \"Mexico\", \"Taiwan\", \"Japan\", \"Philippines\", \"UK\", \"USA\")) |> \n  select(country, height, diameter)\nggplot(highest, aes(y = fct_rev(factor(country)), x = diameter)) + \n  geom_point() +\n  geom_segment(aes(y = country, yend = country, x = 0, xend = diameter), linetype = \"dashed\") +\n  labs(x = \"\", y = \"\",\n       # x = \"Diameter of the highest ferries wheel in the country (in meter)\"\n       title = \"How far do you travel during one rotation in the highest ferries wheels?\",\n       caption = \"Data from ferriewheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  scale_x_continuous(limits = c(0, 800)) +\n  theme_minimal()\n```\n\n\n\n\n\n\nFootnotes\n\n\nWe won‚Äôt worry about customizing anything yet and the file is empty anyways.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/tt2022-08-16_psychometric/index.html",
    "href": "posts/tt2022-08-16_psychometric/index.html",
    "title": "A fancy title",
    "section": "",
    "text": "# datalist &lt;- tidytuesdayR::tt_load(\"2022-08-16\")\n# write_rds(datalist, here(\"posts\", \"tt2022-08-16_psychometric\", \"data\", \"psychometric\"))\ndatalist &lt;- read_rds(here(\"posts\", \"tt2022-08-16_psychometric\", \"data\", \"psychometric\"))\ncharacters &lt;- datalist$characters\nstats &lt;- datalist$psych_stats\n\n\n\n\ndata &lt;- characters |&gt; filter()"
  },
  {
    "objectID": "posts/nt_alphabetic-sorting/index.html",
    "href": "posts/nt_alphabetic-sorting/index.html",
    "title": "Alphabetically sorting your source code character vectors",
    "section": "",
    "text": "For week 33 of #TidyTuesday 2022 I had to manually select names from a dataset. So, I typed something like\n\nfilter(df, name %in% c(\"The Witcher\", \"Hamilton\", \"Westworld\", \"Sense8\"))\n\nDo you see what‚Äôs wrong with this? No? Congratulations üéâ, you have a much healthier, more pragmatic approach to coding than I do.\nI, on the other hand, was bothered by the fact that the series names were not in alphabetical order. But, ‚Äúhey‚Äù, I thought, R is a ‚Äúwell-developed, simple and effective programming language‚Äù, so why not use it to bring my source code into alphabetical order.\n\n\n\n\n\n\n\n\n\n\n\nThe context is not really important, so I won‚Äôt go into detail on what this dataset was about. The trick is super generally and applies to many situations anyways. But you can see my final plot on Twitter and the accompanying blog post on my website.\n\n\n\n\nMy goal is to get alphabetically sorted output that I can simply copy + paste into my source file (or write to the file with write_lines()).\n\nx <- c(\"The Witcher\", \"Hamilton\", \"Westworld\", \"Sense8\")\nprint(x)\n\n[1] \"The Witcher\" \"Hamilton\"    \"Westworld\"   \"Sense8\"     \n\ndput(sort(x))\n\nc(\"Hamilton\", \"Sense8\", \"The Witcher\", \"Westworld\")\n\n\nSuccess! That last line is something I can copy. üòÉ And‚Ä¶it only took approximately 1000 hours of googling to find dput() üôà Eventually the Similar questions that pop-up just before you create a stackoverflow question came to the rescue.\nIt would be really cool to immediately copy the output of the dput() statement to the clipboard. There is the clipr package which looks promising, but that is something for another evening."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yebelay Berehan",
    "section": "",
    "text": "Biostatistician\nLearn more about me ‚Üí"
  },
  {
    "objectID": "pages/teaching.html",
    "href": "pages/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "PhD Level Courses (Tinbergen Institute)\n2015: Macroeconomics 1 (TA Professor Bruegemann)\n\n\nMaster Level Courses (Vrije Universiteit Amsterdam)\n2020: Introduction to Econometrics (TA for Professor Dobbelaere), Evaluation 4.3/5\n2020: Applied Econometrics (TA for Professor Dobbelaere), Evaluation 4.1/5\n\n\nBachelor Level Courses (Vrije Universiteit Amsterdam)\n2022: Macroeconomics I (TA for Professor Bruegemann), Evaluation 4.9/5\n2018-2020: Macroeconomics I (TA for Professor Bartelsman), Evaluation 3.5/5\n2018-2020: Foundations of Macroeconomics (TA for Professor Zant), Evaluation 4.1/5\n2018-2020: Inclusive Growth and Sustainability (TA for Professor Hoefkes and Professor Lanjouw), Evaluation 4.25/5\n\n\nThesis Supervision\n2020: Economics Bachelor Thesis: ‚ÄúDoes Foreign Direct Investment Promote Growth in Sub-Saharan Africa? Insights from Sectoral Decomposition‚Äù\n2020: Economics Master Thesis: ‚ÄúImpact of Chinese FDI on Debt and Imports from China‚Äù and ‚ÄúThe Effect of Educational Inequality on Income Inequality‚Äù\n2020: Economics Bachelor Thesis: ‚ÄúThe Relationship between Charity Advertisements and Misconceptions about Low-Income Countries‚Äù\n2019: Economics Master Thesis: ‚ÄúReassessing the Impact of Inequality on Economic Growth‚Äù"
  },
  {
    "objectID": "pages/research.html",
    "href": "pages/research.html",
    "title": "Research",
    "section": "",
    "text": "I focus on innovative forms of financing sustainable economic development, from impact investing to agricultural finance. I also study agricultural markets, with a focus on smallholder farmer value chains in East Africa."
  },
  {
    "objectID": "pages/research.html#peer-reviewed-publications",
    "href": "pages/research.html#peer-reviewed-publications",
    "title": "Research",
    "section": "Peer-reviewed Publications",
    "text": "Peer-reviewed Publications\n\nFinancial Returns or Social Impact? What Motivates Impact Investors‚Äô Lending to Firms in Low-Income Countries (Journal of Banking & Finance, 2022 | open-access link)\n\nI analyze 70,000 transactions by retail impact investors on a peer-to-peer lending platform that intermediates loans to firms in low-income countries. Loans pay interest to investors and publicize indicators of expected social impact. Financial returns significantly influence investors‚Äô decisions: a one percentage point increase in the interest rate increases funding speed seven-fold, investment probability two-fold and transaction size by 122 Euro. Expected social impact influences investors‚Äô perception but has no influence (for female empowerment, employees and beneficiaries) or limited influence (for turnover) on investors‚Äô funding decisions. When all available loans pay the same interest rates, female borrowers - but not firms with many employees or beneficiaries - are more likely to be chosen, suggesting that variation in financial returns can crowd out salient dimensions of social impact. The study implies that peer-to-peer lending platforms should function as gatekeepers of social impact and cannot outsource the evaluation of social impact to retail impact investors.\n\n\nI‚Äôm excited to share my first-ever publication ü•≥At times doing a PhD feels like filling a pool with water one cup at a time and it's difficult to see progress. I'm glad that my first publishing experience was smooth with great comments from editor & referee.A thread: [1/11] pic.twitter.com/72doP4ziiJ‚Äî Philipp Kollenda (@philippkollenda) July 23, 2021"
  },
  {
    "objectID": "pages/research.html#working-papers",
    "href": "pages/research.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\nBad News Travel Fast ‚Ä¶ and Decrease Credit Supply in Peer-to-Peer Lending\n\nUsing a new dataset of transactions on a Dutch peer-to-peer lending platform, I investigate the stability of credit supply when investors experience repayment delays. I exploiting a natural experiment, where some investors experience repayment delays in impact investment loans and show that the delays cause aÔ¨Äected investors to decrease their credit supply by 206‚Ç¨ on average, resulting in a shortage of credit of almost 400,000‚Ç¨ to borrowers unconnected to the delays. Repayment delays thus have substantial negative externalities on the ability of Ô¨Årms to raise capital through the peer-to-peer lending platform.\n\n\n\nRipe for Contract? Avocado contract farming in Kenya improves agricultural investments, knowledge and prices. Link to working paper and to presentation\n\nWe evaluate the impact of a multi-layered contract farming intervention that connected smallholder farmers in central Kenya to avocado exporting companies, provided training in good agricultural practices and certified farmer organizations according to the global good agricultural practices (GAP) production standard. Using panel data from 2015 and 2017 we show that the intervention was successful at delivering its immediate goals. It increased the share of farmers that sold to companies, were recently trained and received the GAP certification. Contract farming significantly improved the sales prices of farmers, and knowledge of avocado-farming practices and led to increased investments into the Hass avocado variety which is in higher demand in export markets. We find suggestive, albeit not statistically significant, evidence that contract farming improved farmer income and shifted labour from family to hired labour. At the same time, contract farmers produced less of the local avocado variety, leading to a significant decrease in total quantity sold during the transition to the export-oriented Hass avocado variety. We contribute to the literature by quantifying the impact of a multi-layered contract farming intervention. Panel data allows us to estimate a doubly-robust difference-in-differences design, giving us more confidence to interpret our estimates as causal evidence of contract farming than traditional cross-sectional studies allow for."
  },
  {
    "objectID": "pages/motivation.html",
    "href": "pages/motivation.html",
    "title": "Motivation",
    "section": "",
    "text": "I get energy from working together with others to build a better world, where prosperity is created sustainably, redistributed globally and shared by all.\nMy professional ambition is to help people and organizations combine data with rigorous methodologies to generate evidence that allows us to improve people‚Äôs livelihoods through sustainable economic development.\n\n\n\nTweets by philippkollenda"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About Me",
    "section": "",
    "text": "I have dual master‚Äôs degrees in Biostatistics from Stellenbosch University and Statistics from Addis Ababa University, coupled with 11+ years of experience in project leading, data management, analysis, and teaching.\nThese experiences equipped me with a deep understanding of complex data, its management, analysis and use with application of different statistical methods.\nI have a piece of statistical software training experiences (R, SAS, SPSS)."
  },
  {
    "objectID": "pages/presentations.html",
    "href": "pages/presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "You can also view the slides in full screen.\n\n\n\nDownload the workshop materials here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Excess attraction in the Bundesliga\n\n\nWhich teams would have the most members if location was the only thing that mattered?\n\n\n\ninteractive\n\n\nfootball\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nYebelay\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetically sorting your source code character vectors\n\n\nNeat little tricks, Episode 1\n\n\n\nneat-tricks\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nYebelay\n\n\n\n\n\n\n\n\n\n\n\n\nA fancy title\n\n\n#tidytuesday data visualization with R, ggplot and A COOL PACKAGE\n\n\n\ntidytuesday\n\n\ndataviz\n\n\nhow-to\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nYebelay\n\n\n\n\n\n\n\n\n\n\n\n\nHow far does that ferris wheel roll?\n\n\n#tidytuesday data visualization with R, ggplot and ggimage\n\n\n\ntidytuesday\n\n\ndataviz\n\n\nhow-to\n\n\n\n\n\n\n\n\n\nAug 9, 2022\n\n\nYebelay\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a simple website with R and Quarto in under 30 10 minutes.\n\n\nAnd then spend hours tweaking it.\n\n\n\nquarto\n\n\nhow-to\n\n\n\n\n\n\n\n\n\nAug 6, 2022\n\n\nPhilipp Kollenda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/education.html",
    "href": "pages/education.html",
    "title": "Education",
    "section": "",
    "text": "Stellenbosch University, South Africa| CapeTown\n\nMSc in Biostatistics|January 2020 - April 2022\n\nAddis Ababa University, Ethiopia| Addis Ababa, Ethiopia\n\nMSc in Statistics(Biostatistics)| Sptember 2015 - July 2017\n\nWollo University, Ethiopia| Dessie, Ethiopia\n\nBSc in Statistics | September 2009 - July 2012"
  },
  {
    "objectID": "pages/exprience.html",
    "href": "pages/exprience.html",
    "title": "Expreniece",
    "section": "",
    "text": "Survey Manager at: Center for Evaluation and Development (C4ED) from ‚Äò2023-05-01‚Äô to present\nSenior Research Officer at: Frontieri consulting Plc from ‚Äò2022-07-04‚Äô to ‚Äò2023-04-30‚Äô\nBiostatistics Experience and Technical Assistance (BETA) internship/consulting at: CDC Ethiopa from ‚Äò2021-07-01‚Äô to ‚Äò2022-07-12‚Äô\nLecturer at: ‚ÄúDebre Markos University‚Äù from ‚Äò2013-01-01‚Äô date_end: ‚Äô‚Äô"
  },
  {
    "objectID": "pages/publication.html",
    "href": "pages/publication.html",
    "title": "Publication",
    "section": "",
    "text": "I focus on innovative forms of financing sustainable economic development, from impact investing to agricultural finance. I also study agricultural markets, with a focus on smallholder farmer value chains in East Africa."
  },
  {
    "objectID": "pages/publication.html#peer-reviewed-publications",
    "href": "pages/publication.html#peer-reviewed-publications",
    "title": "Publication",
    "section": "Peer-reviewed Publications",
    "text": "Peer-reviewed Publications\n\nFinancial Returns or Social Impact? What Motivates Impact Investors‚Äô Lending to Firms in Low-Income Countries (Journal of Banking & Finance, 2022 | open-access link)\n\nI analyze 70,000 transactions by retail impact investors on a peer-to-peer lending platform that intermediates loans to firms in low-income countries. Loans pay interest to investors and publicize indicators of expected social impact. Financial returns significantly influence investors‚Äô decisions: a one percentage point increase in the interest rate increases funding speed seven-fold, investment probability two-fold and transaction size by 122 Euro. Expected social impact influences investors‚Äô perception but has no influence (for female empowerment, employees and beneficiaries) or limited influence (for turnover) on investors‚Äô funding decisions. When all available loans pay the same interest rates, female borrowers - but not firms with many employees or beneficiaries - are more likely to be chosen, suggesting that variation in financial returns can crowd out salient dimensions of social impact. The study implies that peer-to-peer lending platforms should function as gatekeepers of social impact and cannot outsource the evaluation of social impact to retail impact investors.\n\n\nI‚Äôm excited to share my first-ever publication ü•≥At times doing a PhD feels like filling a pool with water one cup at a time and it's difficult to see progress. I'm glad that my first publishing experience was smooth with great comments from editor & referee.A thread: [1/11] pic.twitter.com/72doP4ziiJ‚Äî Philipp Kollenda (@philippkollenda) July 23, 2021"
  },
  {
    "objectID": "pages/publication.html#working-papers",
    "href": "pages/publication.html#working-papers",
    "title": "Publication",
    "section": "Working Papers",
    "text": "Working Papers\n\nBad News Travel Fast ‚Ä¶ and Decrease Credit Supply in Peer-to-Peer Lending\n\nUsing a new dataset of transactions on a Dutch peer-to-peer lending platform, I investigate the stability of credit supply when investors experience repayment delays. I exploiting a natural experiment, where some investors experience repayment delays in impact investment loans and show that the delays cause aÔ¨Äected investors to decrease their credit supply by 206‚Ç¨ on average, resulting in a shortage of credit of almost 400,000‚Ç¨ to borrowers unconnected to the delays. Repayment delays thus have substantial negative externalities on the ability of Ô¨Årms to raise capital through the peer-to-peer lending platform.\n\n\n\nRipe for Contract? Avocado contract farming in Kenya improves agricultural investments, knowledge and prices. Link to working paper and to presentation\n\nWe evaluate the impact of a multi-layered contract farming intervention that connected smallholder farmers in central Kenya to avocado exporting companies, provided training in good agricultural practices and certified farmer organizations according to the global good agricultural practices (GAP) production standard. Using panel data from 2015 and 2017 we show that the intervention was successful at delivering its immediate goals. It increased the share of farmers that sold to companies, were recently trained and received the GAP certification. Contract farming significantly improved the sales prices of farmers, and knowledge of avocado-farming practices and led to increased investments into the Hass avocado variety which is in higher demand in export markets. We find suggestive, albeit not statistically significant, evidence that contract farming improved farmer income and shifted labour from family to hired labour. At the same time, contract farmers produced less of the local avocado variety, leading to a significant decrease in total quantity sold during the transition to the export-oriented Hass avocado variety. We contribute to the literature by quantifying the impact of a multi-layered contract farming intervention. Panel data allows us to estimate a doubly-robust difference-in-differences design, giving us more confidence to interpret our estimates as causal evidence of contract farming than traditional cross-sectional studies allow for."
  },
  {
    "objectID": "pages/puplications.html",
    "href": "pages/puplications.html",
    "title": "Publications",
    "section": "",
    "text": "I focus on innovative forms of financing sustainable economic development, from impact investing to agricultural finance. I also study agricultural markets, with a focus on smallholder farmer value chains in East Africa."
  },
  {
    "objectID": "pages/puplications.html#peer-reviewed-publications",
    "href": "pages/puplications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "Peer-reviewed Publications",
    "text": "Peer-reviewed Publications\n\nFinancial Returns or Social Impact? What Motivates Impact Investors‚Äô Lending to Firms in Low-Income Countries (Journal of Banking & Finance, 2022 | open-access link)\n\nI analyze 70,000 transactions by retail impact investors on a peer-to-peer lending platform that intermediates loans to firms in low-income countries. Loans pay interest to investors and publicize indicators of expected social impact. Financial returns significantly influence investors‚Äô decisions: a one percentage point increase in the interest rate increases funding speed seven-fold, investment probability two-fold and transaction size by 122 Euro. Expected social impact influences investors‚Äô perception but has no influence (for female empowerment, employees and beneficiaries) or limited influence (for turnover) on investors‚Äô funding decisions. When all available loans pay the same interest rates, female borrowers - but not firms with many employees or beneficiaries - are more likely to be chosen, suggesting that variation in financial returns can crowd out salient dimensions of social impact. The study implies that peer-to-peer lending platforms should function as gatekeepers of social impact and cannot outsource the evaluation of social impact to retail impact investors.\n\n\nI‚Äôm excited to share my first-ever publication ü•≥At times doing a PhD feels like filling a pool with water one cup at a time and it's difficult to see progress. I'm glad that my first publishing experience was smooth with great comments from editor & referee.A thread: [1/11] pic.twitter.com/72doP4ziiJ‚Äî Philipp Kollenda (@philippkollenda) July 23, 2021"
  },
  {
    "objectID": "pages/puplications.html#working-papers",
    "href": "pages/puplications.html#working-papers",
    "title": "Publications",
    "section": "Working Papers",
    "text": "Working Papers\n\nBad News Travel Fast ‚Ä¶ and Decrease Credit Supply in Peer-to-Peer Lending\n\nUsing a new dataset of transactions on a Dutch peer-to-peer lending platform, I investigate the stability of credit supply when investors experience repayment delays. I exploiting a natural experiment, where some investors experience repayment delays in impact investment loans and show that the delays cause aÔ¨Äected investors to decrease their credit supply by 206‚Ç¨ on average, resulting in a shortage of credit of almost 400,000‚Ç¨ to borrowers unconnected to the delays. Repayment delays thus have substantial negative externalities on the ability of Ô¨Årms to raise capital through the peer-to-peer lending platform.\n\n\n\nRipe for Contract? Avocado contract farming in Kenya improves agricultural investments, knowledge and prices. Link to working paper and to presentation\n\nWe evaluate the impact of a multi-layered contract farming intervention that connected smallholder farmers in central Kenya to avocado exporting companies, provided training in good agricultural practices and certified farmer organizations according to the global good agricultural practices (GAP) production standard. Using panel data from 2015 and 2017 we show that the intervention was successful at delivering its immediate goals. It increased the share of farmers that sold to companies, were recently trained and received the GAP certification. Contract farming significantly improved the sales prices of farmers, and knowledge of avocado-farming practices and led to increased investments into the Hass avocado variety which is in higher demand in export markets. We find suggestive, albeit not statistically significant, evidence that contract farming improved farmer income and shifted labour from family to hired labour. At the same time, contract farmers produced less of the local avocado variety, leading to a significant decrease in total quantity sold during the transition to the export-oriented Hass avocado variety. We contribute to the literature by quantifying the impact of a multi-layered contract farming intervention. Panel data allows us to estimate a doubly-robust difference-in-differences design, giving us more confidence to interpret our estimates as causal evidence of contract farming than traditional cross-sectional studies allow for."
  },
  {
    "objectID": "pages/about.html#education",
    "href": "pages/about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nStellenbosch University, South Africa| CapeTown\n\nMSc in Biostatistics|January 2020 - April 2022\n\nAddis Ababa University, Ethiopia| Addis Ababa, Ethiopia\n\nMSc in Statistics(Biostatistics)| Sptember 2015 - July 2017\n\nWollo University, Ethiopia| Dessie, Ethiopia\n\nBSc in Statistics | September 2009 - July 2012"
  },
  {
    "objectID": "pages/about.html#experience",
    "href": "pages/about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nSurvey Manager at: Center for Evaluation and Development (C4ED) from ‚Äò2023-05-01‚Äô to present\nSenior Research Officer at: Frontieri consulting Plc from ‚Äò2022-07-04‚Äô to ‚Äò2023-04-30‚Äô\nBiostatistics Experience and Technical Assistance (BETA) internship/consulting at: CDC Ethiopa from ‚Äò2021-07-01‚Äô to ‚Äò2022-07-12‚Äô\nLecturer at: ‚ÄúDebre Markos University‚Äù from ‚Äò2013-01-01‚Äô date_end: ‚Äô‚Äô"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Feel free to get in touch with me using the following contact details:\n\nEmail: yebelay.ma@gmail.com\nPhone: +251 93 245 3203\n\n\n\nYou can also reach me through social media or by filling out the contact form below.\n\n\nYour Name \nYour Email \nYour Message\n\n\nSend"
  },
  {
    "objectID": "contact.html#visit-us",
    "href": "contact.html#visit-us",
    "title": "Yebelay Berehan",
    "section": "Visit Us",
    "text": "Visit Us\nThe AffCom Lab is located in Fraser Hall Room #455 at the University of Kansas.\n\nStreet Address: 1415 Jayhawk Blvd, Lawrence, KS 66044\nVisitor Parking: 1218 Mississippi St, Lawrence, KS 66045"
  },
  {
    "objectID": "contact.html#email-us",
    "href": "contact.html#email-us",
    "title": "Yebelay Berehan",
    "section": "Email Us",
    "text": "Email Us"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "You can also reach me through social media or by filling out the contact form below.\n\n\nYour Name \nYour Email \nYour Message\n\n\nSend"
  },
  {
    "objectID": "pages/presentations.html#workshop-slides",
    "href": "pages/presentations.html#workshop-slides",
    "title": "Presentations",
    "section": "",
    "text": "You can also view the slides in full screen."
  },
  {
    "objectID": "pages/presentations.html#download-workshop-materials",
    "href": "pages/presentations.html#download-workshop-materials",
    "title": "Presentations",
    "section": "",
    "text": "Download the workshop materials here."
  },
  {
    "objectID": "pages/presentations.html#spatial-data-analysis-training",
    "href": "pages/presentations.html#spatial-data-analysis-training",
    "title": "Presentations",
    "section": "",
    "text": "You can also view the slides in full screen."
  },
  {
    "objectID": "slide/index.html#section",
    "href": "slide/index.html#section",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "Outlines\n\nWhat is spatial data and why should we use it?\nTypes of spatial data\nSpatial data in R: how to store, load, and tidy spatial data\nVisualizing spatial data\nExploring spatial data: Spatial authorization and spatial statistics to quantify relationships"
  },
  {
    "objectID": "slide/index.html#what-is-spatial-data-and-why-should-we-use-it",
    "href": "slide/index.html#what-is-spatial-data-and-why-should-we-use-it",
    "title": "Spatial data analaysis Training",
    "section": "What is Spatial data and why should we use it?",
    "text": "What is Spatial data and why should we use it?\n\nSpatial data, also known as geospatial data, refers to information that identifies the geographic location and characteristics of natural or constructed features and boundaries on the Earth.\nThis data is often represented in terms of Cartesian coordinates (x,y) for two-dimensional maps, but may also include altitude (z) for a three-dimensional representation.\nSpatial data can come in various forms including points, lines, and polygons.\nThe concept of spatial data is integral to a variety of applications that require an understanding of how different elements relate to each other within a geographical space.\nThis data can be collected through various means, including but not limited to, satellite imagery, aerial photography, and ground-based surveys."
  },
  {
    "objectID": "slide/index.html#spatial-data-formats",
    "href": "slide/index.html#spatial-data-formats",
    "title": "Spatial data analaysis Training",
    "section": "Spatial Data Formats",
    "text": "Spatial Data Formats\n\nLet us see a basic way to represent the spatial data.\nBut there is a variety of data formats to represent the data to suit different applications.\nIn most cases, spatial data formats are an extension of existing data formats.\n\n\n\n\n\n\n\n\nType\nNon-Spatial Data\nSpatial Data\n\n\n\n\nText\ncsv, json, xml\ncsv, geojson, gml, kml\n\n\nBinary/Compressed\npdf, xls, zip\nshapefile, geopdf, geopackage\n\n\nImages\ntiff, jpg, png\ngeotiff, jpeg2000\n\n\nDatabases\nSQLite, PostgreSQL, Oracle\nSpatialite, PostGIS, Oracle Spatial"
  },
  {
    "objectID": "slide/index.html#spatial-data-types",
    "href": "slide/index.html#spatial-data-types",
    "title": "Spatial data analaysis Training",
    "section": "Spatial Data Types",
    "text": "Spatial Data Types\nSpatial Data can be broadly categorized into 2 types - Vector and Raster.\n\n\n\n\n\n\n\n\nType\nSub Types\nExamples\n\n\n\n\nVector Data\nPoints: Represents features with a single coordinate pair (x, y).\nLocations of ATMs, tree\n\n\n\nLines: Represents linear features as ordered sequences of points.\nRoads, rivers, utility lines\n\n\n\nPolygons: Represents areas enclosed by closed loops of lines.\nBuildings, lakes, zones\n\n\nRaster Data\nGrids: Represents continuous data across a surface.\nSatellite images, digital elevation models (DEMs)\n\n\n\nPixels: Smallest units in a raster dataset, each with a specific value.\nValues representing color, temperature,\n\n\n\n\n\n\n\n?"
  },
  {
    "objectID": "pages/about.html#projects-and-publications",
    "href": "pages/about.html#projects-and-publications",
    "title": "About Me",
    "section": "Projects and Publications",
    "text": "Projects and Publications\nI‚Äôve published some stuff ‚Äì both academic and not ‚Äì which you can find on my publications page."
  },
  {
    "objectID": "pages/about.html#consulting-and-contracted-work",
    "href": "pages/about.html#consulting-and-contracted-work",
    "title": "About Me",
    "section": "Consulting and Contracted Work",
    "text": "Consulting and Contracted Work\nTime permitting, I occasionally do some consulting and contract work. Although I specialize in education research and data analysis, I‚Äôm generally proficient with in statistical modeling, data visualization, dashboard design, and API development, among other things. If you‚Äôre interested in working with me, send me an email."
  },
  {
    "objectID": "posts/tt2022-08-09_ferriswheels/index.html#using-tidy-tuesday-to-combine-creativity-and-data-visualization-skills.",
    "href": "posts/tt2022-08-09_ferriswheels/index.html#using-tidy-tuesday-to-combine-creativity-and-data-visualization-skills.",
    "title": "How far does that ferris wheel roll?",
    "section": "",
    "text": "What I love about data visualization is how it combines creative thinking (what do I want to show?) with technical programming skills (how do I show this?). Nowhere do you combine these two things as elegantly as in the weekly tidy tuesday challenges. Here, data wizards around the word play and practice with fascinating datasets from chocolate bar ratings to the mobility of ERASMUS students to ‚Äì this week ‚Äì data on ferris wheels around the world.1 As someone who goes pale in any kind of rollercoaster, ferris wheels feel just about advantageous enough to start my own tidy tuesday journey.\n\n\n\nBefore opening R, I think about what I find fascinating about the topic and what I want to convey with my visualization. Here I am sticking to some descriptive analysis and won‚Äôt overthink it, so let‚Äôs just go with one of the first thing that came to my mind‚Ä¶ ferris wheels are really big!\n\nIf a ferris wheel would be an actual wheel, how far would it travel during one rotation?2\n\nThe message gives me part of the structure for the visualization. Here, I want to select a few examples from different countries and show the diameter on the x-axis. I‚Äôve created a lot of horizontal bar charts for a consultancy this summer, so naturally this is my starting point.\nA great thing about these just-for-fun challenges is that you can use them to add 1-2 new skills to your visualization toolbox. For this challenge I want to learn two things:\n\nHow do I add icons (ferris wheels üé°, obviously) instead of dots in the chart?\nHow do I add country flags on the y-axis.\n\nI don‚Äôt want to spend the entire evening on this, so I think it is important to limit myself to only these two new features. I will also concentrate on them and not care too much (yet) about other things, like titles, background, themes, etc‚Ä¶\n\n\n\n\n\n\nImportant\n\n\n\nBecause this isn‚Äôt a mystery novel, I am giving away how the visualization will look like in the end. Don‚Äôt click if you want to keep the suspense‚Ä¶\n\n\n\n\n\nWith these initial thoughts out of the way, I can roughly sketch the visualization process and outline of this post.\n\nA first look at the data. Choose which points to show.\nCreate the simplest graph that shows the intended message.\nAdd the ferris wheel icons.\nAdd the country flags.\nClean up the graph.\n\n\n\n\n\n\n\n\n\nLet‚Äôs first install the necessary packages and load and save the data.\n#install.packages(c(\"tidytuesdayR\", \"tidyverse\", \"here\", \"ggimage\", \"countrycode\", \"ggtext\"))\nlibrary(tidyverse)\nlibrary(here)\n#data &lt;- tidytuesdayR::tt_load(\"2022-08-09\")$wheels\n#write_rds(data, here(\"posts\", \"tt2022-08-09_ferriswheels\", \"data\", \"wheels\"))\ndata &lt;- read_rds(here(\"posts\", \"tt2022-08-09_ferriswheels\", \"data\", \"wheels\"))\n\n\n\nThe entire dataset collects 22 variables on 73 ferris wheels in 26 different countries. This is way to much information to display, so let‚Äôs keep only a handful of ferris wheels and only information on their name, country, height and diameter.\n\ndata &lt;- data |&gt; \n  select(country, name, height, diameter) |&gt; \n  group_by(country) |&gt; \n  filter(height == max(height, na.rm = T),\n         country %in% c(\"Austria\", \"Canada\", \"China\", \"Mexico\", \"UK\", \"USA\")) |&gt;\n  ungroup() |&gt; \n  mutate(across(c(height, diameter), round, 0),\n         country = factor(country))\n\nThis is the data that we will use. As a comparison, I will add data from a more typical wheel: my dutch bicycle wheel.\n\ndata &lt;- bind_rows(data, tibble(country = \"Netherlands\", name = \"Philipp's bike wheel\", height = 0.67, diameter = 0.67))\nknitr::kable(data)\n\n\n\n\ncountry\nname\nheight\ndiameter\n\n\n\n\nChina\nBeijing Great Wheel\n693.00\n643.00\n\n\nUSA\nGolden Gate Flyer\n728.00\n700.00\n\n\nUK\nLondon Eye\n443.00\n394.00\n\n\nCanada\nNiagara SkyWheel\n175.00\n167.00\n\n\nMexico\nStar of Puebla\n262.00\n229.00\n\n\nAustria\nWiener Riesenrad\n212.00\n200.00\n\n\nNetherlands\nPhilipp‚Äôs bike wheel\n0.67\n0.67\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data, aes(y = fct_rev(country), x = diameter)) + \n  labs(x = \"\", y = \"\",\n       # x = \"Diameter of the highest ferris wheel in the country (in meter)\"\n       title = \"How far do you travel during one rotation in the largest ferris wheels?\",\n       caption = \"Data from ferriswheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  scale_x_continuous(limits = c(0, 800)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\np +\n  geom_point() +\n  geom_segment(aes(y = country, yend = country, x = 0, xend = diameter), linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis isn‚Äôt pretty, but that‚Äôs not the point. Having a bare-minimum working example helps to think about whether the graph type fits and what can be done to emphasise the key message. It also helps to create a to-do list of things to improve: like a different order of ferris wheels, maybe adding an empty row to separate the bike data from the ferris wheel data, a label or subtitle that puts the ferris wheel in relation to the bike wheel, another background color, adding ‚Äòmeter‚Äô to the x-axis and more). But let us first tackle the two big learnings that I wanted to get out of the challenge: replacing the dots with icons (now I also need a bike, üö≤) and adding the country flags.\n\n\n\nAs always, there are many ways to Rome. Here I am going with the ggimage package maintained by Guangchuang YU. I have not used this before, but a look at the online vignette makes me hopeful that I can use this for the ferris wheel icons and the country flags üòÉ\nThere are two ways we can go. The first is to use the ggimage::geom_emoji and to insert the ferris wheel and bicycle emojis that I have used throughout the post. That seems simple, we just need the unicode to pass it to the function. That‚Äôs easy to look up at, e.g.¬†https://unicode.org/emoji/charts/full-emoji-list.html.\n\ndata$size = if_else(data$name == \"Philipp's bike wheel\", 0.075, data$diameter / max(data$diameter) * 0.25)\np +\n  geom_segment(aes(y = fct_rev(country), yend = fct_rev(country), x = 0, xend = diameter), linetype = \"dashed\", color = \"gray\", position = position_nudge(y = - 0.25)) +\n  ggimage::geom_emoji(aes(image = if_else(name == \"Philipp's bike wheel\", \"1f6b2\", \"1f3a1\")), position = position_nudge(y = (data$size / 0.3) - 0.25), size = data$size)\n\n\n\n\n\n\n\n\nThis looks OK. The most complicated part was actually that I wanted to scale the size of the image by the size of the ferris wheel, which required making an extra size column and experimenting a lot with position_nudge to let the dashed line go to the bottom of the icon, not to the middle. But I am not super happy with the emoji‚Äôs, so let‚Äôs try to download some images of ferris wheels and use those with ggimage::geom_image() instead.\nI‚Äôll first download a simple transparent ferris wheel png file (from here) and use a bike icon (from here).\n\nferris_path &lt;- here(\"posts\", \"tt2022-08-09_ferriswheels\", \"images\", \"ferriswheel.png\")\nbike_path &lt;- here(\"posts\", \"tt2022-08-09_ferriswheels\", \"images\", \"bike.png\")\n\np1 &lt;- p +\n  geom_segment(aes(y = fct_rev(country), yend = fct_rev(country), x = 0, xend = diameter),\n               linetype = \"dashed\", color = \"gray\", \n               position = position_nudge(y = - 0.25)) +\n  ggimage::geom_image(\n    aes(image = if_else(data$name == \"Philipp's bike wheel\", bike_path, ferris_path)), \n    position = position_nudge(y = (data$size / 0.27) - 0.25, \n                              x = if_else(data$name == \"Philipp's bike wheel\", 15, 0)), \n    size = data$size * 1.5)\np1\n\n\n\n\n\n\n\n\nI‚Äôm happy with this. Onwards to do the country flags!\n\n\n\nIn the plots so far, I kept the country names on the y-axis. However, I would prefer to use the actual names of the ferris wheel and replace the country names with flags. We‚Äôll continue with the ggimage package, which contains a geom_flag() function. The package expects country codes (e.g.¬†NL, US), so we need to transform the names by hand using the countrycodes package.\n\n# Create the country labels from the country names\ndata$code &lt;- countrycode::countrycode(data$country, origin = \"country.name\", destination = \"iso2c\") \n\n# Sort the factor levels of the 'name' variable alphabetically *by country* (the combination of arrange and fct_inorder does that, there are surely smarter ways)\ndata &lt;- data |&gt; arrange(country)\ndata$name &lt;- fct_inorder(factor(data$name))\n\n# Create the graph again from scratch because instead of mapping the country to the y axis, I now\n# map the name to the y axis. Again, surely there are smarter ways to do that...\n# I also move the flags a bit to the left of the x axis and expand the limits of the plot accordingly.\np2 &lt;- \n  ggplot(data, aes(y = fct_rev(name), x = diameter)) + \n  geom_segment(aes(y = fct_rev(name), yend = fct_rev(name), x = 0, xend = diameter),\n               linetype = \"dashed\", color = \"gray\", \n               position = position_nudge(y = - 0.25)) +\n  ggimage::geom_image(aes(image = if_else(name == \"Philipp's bike wheel\", bike_path, ferris_path)),\n                      position = position_nudge(y = (data$size / 0.27) - 0.25,\n                                                x = if_else(data$name == \"Philipp's bike wheel\", 15, 0)),\n                      size = data$size * 1.5) +\n  ggimage::geom_flag(x = -70, aes(image = code)) +\n  labs(x = \"\", y = \"\",\n       title = \"How far do you travel during one rotation in the largest ferris wheels?\",\n       caption = \"Data from ferriswheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  scale_x_continuous(limits = c(-50, 800)) +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\np2\n\n\n\n\n\n\n\n\nThis looks fine. The main headache in the code chunk above was replacing the mapping of the y axis to show the name (and not the country) and correcting the ordering of the observations.\n\n\n\nI am happy with how the flags and ferris wheels turned out and am now ready to go back to the to-do list from the beginning with some minor things to clean up the graph. This is a playful challenge and ferris wheels are fun, so let‚Äôs go for a very light pinkish background colour. I am also adding a text box (using the ggtext package) to use my bicycle wheel as an example to show just how big these ferris wheels are.\n\n\nCode\nturns &lt;- pull(round(data[which(data$diameter == max(data$diameter)), \"diameter\"] / data[data$name == \"Philipp's bike wheel\", \"diameter\"]))\ndata$name &lt;- fct_relevel(data$name, \"Philipp's bike wheel\")\n\np3 &lt;- \n  ggplot(data, aes(y = fct_rev(name), x = diameter)) + \n  geom_segment(aes(y = fct_rev(name), yend = fct_rev(name), x = 0, xend = diameter),\n               linetype = \"dashed\", color = \"gray\", alpha = 0.5,\n               position = position_nudge(y = -0.25)) +\n  ggimage::geom_image(aes(image = if_else(name == \"Philipp's bike wheel\", bike_path, ferris_path)),\n                      position = position_nudge(y = (data$size / 0.27) - 0.25,\n                                                x = if_else(data$name == \"Philipp's bike wheel\", 15, 0)),\n                      size = data$size * 1.5) +\n  ggimage::geom_flag(x = -70, aes(image = code)) +\n  labs(x = \"\", y = \"\",\n       title = \"How far do you travel with one rotation of the largest wheels in the world?\",\n       caption = \"Visualization by Philipp Kollenda / @pkollenda . Accompanying blog post on my website. \\n Data from ferriswheel package by Emil Hvitfeldt, #tidytuesday August 2022\") +\n  ggtext::geom_textbox(aes(x = 300, y = 6.75, hjust = 0,\n                           label = glue::glue(\"My bike wheel has to &lt;span style='color: red;'&gt;turn {turns} times&lt;/span&gt; to travel the same distance as one rotation with the largest ferris wheel.\")),\n                       fill = \"papayawhip\", width = unit(6, \"cm\"), size = 3) +\n  scale_x_continuous(limits = c(-50, 800), labels = scales::label_number(suffix = \" meter\")) +\n  theme_minimal() +\n  theme(panel.grid = element_blank(), legend.position = \"none\",\n        plot.title.position = \"plot\", plot.title = ggtext::element_textbox_simple(size = 20, padding = margin(c(10, 10, 20, 10))),\n        axis.text.y = element_text(size = 12)) +\n  theme(plot.background = element_rect(fill = \"#FFFAFA\"))\np3\n\n\n\n\n\n\n\n\n\n\n\n\nI am happy with how this visualization turned out and glad I learned something about how to insert flags, icons and other images with the ggimage package. I will now hop on my bicycle and think of the Golden Gate Flyer‚Ä¶ approximately every 1045 (bike) wheel turn."
  },
  {
    "objectID": "posts/tt2022-08-09_ferriswheels/index.html#footnotes",
    "href": "posts/tt2022-08-09_ferriswheels/index.html#footnotes",
    "title": "How far does that ferris wheel roll?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSidenote: Neal Grantham has a cool Shiny app to browse previous tidy tuesday submissions.‚Ü©Ô∏é\nPerceptive readers may have noticed that this is just the diameter of the wheel. But, doesn‚Äôt that sound much less exciting?‚Ü©Ô∏é"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Yebelay Berehan",
    "section": "",
    "text": "rstudio::conf(2022) 2022-07-28\n    \n    Cultivating Your Own R Ecosystem as a Solo Contributor\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n    Learn how you can begin to build your own R ecosystem, step by step, to increase the efficiency and impact of your work, even as a solo contributor.\n    \n      ¬†¬†Details\n    \n    \n    \n      ¬†¬†Slides\n    \n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "\nBlog Posts\n",
    "section": "",
    "text": "Introduction 2024-08-03 \n    \n            \n                \n                \n                    quarto\n                \n                \n            \n            \n    Tips on quarto website\n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "",
    "text": "One of the perks of making data visualizations in ggplot2 is that nearly everything is easily customizable, meaning your plots can stick to a common style with the fonts and elements specified by your organization‚Äôs or your own personal style guide (hopefully you have a custom theme to make this easier!). When it comes to using colors in data visualizations, if you find yourself typing in the same hex codes often enough that you start to memorize them‚Äîspeaking from experience here‚Äîit‚Äôs probably time to codify those colors and palettes so you can reference them more easily when creating plots. The examples below will show you how to specify colors and palettes and then use those palettes to create your own scale_color and scale_fill functions for use in plots, for both discrete and continuous data.\nThis blog post by Simon Jackson and this one by Garrick Aden-Buie were both incredibly helpful in helping me figure out how to do this. I somewhat combined their approaches to get this method that worked for my needs, and hopefully either this method or one of theirs is helpful for you!"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#why-custom-palettes",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#why-custom-palettes",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "",
    "text": "One of the perks of making data visualizations in ggplot2 is that nearly everything is easily customizable, meaning your plots can stick to a common style with the fonts and elements specified by your organization‚Äôs or your own personal style guide (hopefully you have a custom theme to make this easier!). When it comes to using colors in data visualizations, if you find yourself typing in the same hex codes often enough that you start to memorize them‚Äîspeaking from experience here‚Äîit‚Äôs probably time to codify those colors and palettes so you can reference them more easily when creating plots. The examples below will show you how to specify colors and palettes and then use those palettes to create your own scale_color and scale_fill functions for use in plots, for both discrete and continuous data.\nThis blog post by Simon Jackson and this one by Garrick Aden-Buie were both incredibly helpful in helping me figure out how to do this. I somewhat combined their approaches to get this method that worked for my needs, and hopefully either this method or one of theirs is helpful for you!"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#defining-custom-colors-and-palettes",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#defining-custom-colors-and-palettes",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "Defining custom colors and palettes",
    "text": "Defining custom colors and palettes\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\n\nThese examples use the penguins data set from the wonderful palmerpenguins package. Let‚Äôs imagine that you work for Penguin Corp and regularly create reports and visualizations with various penguin-related data. Penguin Corp has a style guide that specifies the colors and palettes that should be used, and while you can always specify the colors manually in ggplot2 visualizations, it‚Äôs much easier to define the colors and palettes and then reference them through your own scale_fill and scale_color functions. (These are all functions that can be dropped easily into an internal package!)\nFirst, start by defining the various colors you need.\n\npenguin_corp_color &lt;- function(...) {\n\n  penguin_corp_colors &lt;- c(\n    `pink`     = \"#F7B1AB\",\n    `lavender` = \"#807182\",\n    `gray`     = \"#E3DDDD\",\n    `brown`    = \"#A45C3D\",\n    `purple`   = \"#1C0221\")\n\n  cols &lt;- c(...)\n\n  if (is.null(cols))\n    return (penguin_corp_colors)\n\n  penguin_corp_colors[cols]\n}\n\nThat function can be used to show the definition of a specific color:\n\npenguin_corp_color(\"lavender\")\n\n lavender \n\"#807182\" \n\n\nThat penguin_corp_color function then becomes the base of the penguin_corp_palette function below, where those defined colors are combined into palettes. Your organization might have primary and secondary palettes, or palettes designed for specific uses, but here we‚Äôll define a main palette as well as a highlight palette for when we want just two colors.\n\npenguin_corp_palette &lt;- function(palette = \"main\", ...) {\n\n  penguin_corp_palettes &lt;- list(\n    `main` = penguin_corp_color(\"lavender\", \"gray\", \"pink\", \"brown\"),\n    \n    `highlight` = penguin_corp_color(\"purple\", \"gray\")\n  )\n\n  penguin_corp_palettes[[palette]]\n\n}\n\n\npenguin_corp_palette(\"main\")\n\n lavender      gray      pink     brown \n\"#807182\" \"#E3DDDD\" \"#F7B1AB\" \"#A45C3D\" \n\n\nThe show_col function from the scales package is a nifty way to showcase all the colors available in a given palette:\n\nscales::show_col(penguin_corp_palette(\"main\"), cex_label = 2)"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#creating-your-own-scale_color-and-scale_fill-functions",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#creating-your-own-scale_color-and-scale_fill-functions",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "Creating your own scale_color and scale_fill functions",
    "text": "Creating your own scale_color and scale_fill functions\nHaving the colors and palettes defined is a great first step, but you can go even further and apply those into your own scale_fill and scale_color functions. The first step is a helper function, here called palette_gen. This function has two arguments, the name of the palette (‚Äúmain‚Äù will be our default) and the direction (so you can flip the scale if necessary), and essentially creates another function that will be used in the scale_fill and scale_color functions. (The n there refers to the number of colors that would be needed for a particular plot.)\n\npalette_gen &lt;- function(palette = \"main\", direction = 1) {\n\n  function(n) {\n\n    if (n &gt; length(penguin_corp_palette(palette)))\n      warning(\"Not enough colors in this palette!\")\n\n    else {\n\n      all_colors &lt;- penguin_corp_palette(palette)\n\n      all_colors &lt;- unname(unlist(all_colors))\n\n      all_colors &lt;- if (direction &gt;= 0) all_colors else rev(all_colors)\n\n      color_list &lt;- all_colors[1:n]\n\n    }\n  }\n}\n\nThe function above is for discrete color scales. If you also want to use continuous color scales, the function below uses the existing colorRampPalette function to interpolate the necessary colors between the ones you have chosen in your palette.\n\npalette_gen_c &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  pal &lt;- penguin_corp_palette(palette)\n\n  pal &lt;- if (direction &gt;= 0) pal else rev(pal)\n\n  colorRampPalette(pal, ...)\n\n}\n\nWith that helper function created, you can write the actual functions to be used with ggplot2. I‚Äôve called mine scale_fill_penguin, which takes the same two arguments as before: palette and direction.\n\nscale_fill_penguin &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  ggplot2::discrete_scale(\n    \"fill\", \"penguin\",\n    palette_gen(palette, direction),\n    ...\n  )\n}\n\nYou can use the same syntax for scale_color. (Fun fact: I learned from Garrick‚Äôs post that the ggplot2 convention is to create a scale_colour function and then replicate it as scale_color.)\n\nscale_colour_penguin &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  ggplot2::discrete_scale(\n    \"colour\", \"penguin\",\n    palette_gen(palette, direction),\n    ...\n  )\n}\n\nscale_color_penguin &lt;- scale_colour_penguin\n\nAgain, those are for discrete color scales. If you need a continuous scale, use a function like this:\n\nscale_color_penguin_c &lt;- function(palette = \"main\", direction = 1, ...) {\n\n  pal &lt;- palette_gen_c(palette = palette, direction = direction)\n\n  scale_color_gradientn(colors = pal(256), ...)\n\n}"
  },
  {
    "objectID": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#using-these-colors-in-plots",
    "href": "blog/2022-10-11-creating-custom-color-palettes-with-ggplot2/index.html#using-these-colors-in-plots",
    "title": "Creating Custom Color Palettes with ggplot2",
    "section": "Using these colors in plots",
    "text": "Using these colors in plots\nLet‚Äôs look at some examples to see the various ways these functions can be used to customize colors with ggplot2. Below is a very simple bar chart using the palmerpenguins::penguins data set.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nThe new penguin_corp_color function can be used to specifically define a color. In the plot below, penguin_corp_color(\"lavender\") is the fill argument in the geom_bar layer to make all of the bars that specific shade of lavender.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n)) +\n  geom_bar(stat = \"identity\", fill = penguin_corp_color(\"lavender\")) +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nWe could also add fill = species to the aes layer (meaning that each species will have its own color) and then use scale_fill_penguin(palette = \"main\") to automatically apply our ‚Äúmain‚Äù color palette.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n, fill = species)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_penguin(palette = \"main\") +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\",\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nOr if you want to use the custom colors but not a specific palette, add a scale_fill_manual layer and specify the values using the penguin_corp_color function.\n\npenguins %&gt;% \n  count(species) %&gt;% \n  ggplot(aes(x = species, y = n, fill = species)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = \n                      unname(c(penguin_corp_color(\"brown\",\"gray\",\"purple\")))) +\n  labs(title = \"The count of each species in the palmerpenguins data set\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\",\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nLastly, below is an example of a continuous color scale, with our scale_color_penguin_c function specifying the palette to be used.\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = sex, y = body_mass_g, color = bill_depth_mm)) +\n  geom_jitter(size = 3, width = 0.3) +\n  scale_color_penguin_c(palette = \"highlight\", direction = -1) +\n  labs(title = \"Bill depth and body mass by sex\",\n       y = \"Body mass (g)\",\n       color = \"Bill depth (mm)\") +\n  theme_here() +\n  theme(axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html",
    "href": "blog/2024-02-11-moving-to-quarto/index.html",
    "title": "Introduction",
    "section": "",
    "text": "If you use Netlify (which I highly recommed, as the Quarto/GitHub/Netlify integration is so seamless), you can supply a redirects file that lists all the old urls and their new locations. Netlify builds from Quarto‚Äôs _site directory, so just put the _redirects file (here‚Äôs mine) in your _site folder, right? I tried that first, but it doesn‚Äôt work because the _site directory rebuilds when you rerender your site. Instead, I moved my file into the static folder, which gets copied over to _site as described above.\nMy use case here for redirects is pretty simple: I had a small number of urls to switch, and I don‚Äôt expect to frequently add to that list. If your situation is more complicated and/or you want that _redirects file to automatically generate shorter urls when you create new pages, there are options described here and here."
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#site-organization",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#site-organization",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "Site organization",
    "text": "Site organization\nFirst things first: where do all the files go? My Quarto website structure is fairly similar to the Hugo/blogdown structure, only even simpler. Before, I had a content folder that held blog and talks folders; now those folders are right at the root of the website.\nHere‚Äôs what‚Äôs included:\n\n_quarto.yml: YAML options for the overall website\nindex.qmd: file for the homepage\n404.qmd: my own 404 page in case someone finds a broken link\ntheme.scss: CSS for the theme of my website\nblog/ and talks/: described below\nstatic/: described below\n_site/: what Quarto creates when it renders the site and what Netlify builds off of\n_freeze/: what Quarto creates when you set freeze: auto to avoid rerunning code all the time\n_extensions/: what Quarto creates when you use extensions"
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#blog-and-talks-folders",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#blog-and-talks-folders",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "blog and talks folders",
    "text": "blog and talks folders\nBoth of these folders contain an index.qmd file to specify the YAML options of the landing page, which lists all of the blog posts or talks. Here‚Äôs what mine looks like in the talks folder:\ntitle: &lt;p class=\"center-text\"&gt;Talks&lt;/p&gt;\nauthor: \"\"\ncomments: false\nlisting:\n  contents: \"index.qmd\"\n  sort: \"date desc\"\n  type: \"default\"\n  date-format: \"iso\"\n  fields: [image, date, title, subtitle, author, summary, description, slides, extsite, repo]  \n  page-size: 6\n  template: talk-listing.ejs\npage-layout: full\ntitle-block-banner: true\nformat:\n  html:\n    css: talks.css\nThe listing section specifies which files should be included on the listings page: anything titled index.qmd in the folder and subfolders will get a listing. Mine are sorted descending by date, with six items per page, and I specify the layout of the page with the talk-listing.ejs file.\n\nListing templates\nControlling the look of my listing pages was the hardest part of the entire process! I don‚Äôt love the look of a standard listing page, but it was tough figuring out how to make changes. Quarto has some documentation on creating custom listings pages, but you need to create an EJS template, which I wasn‚Äôt familiar with. Per the documentation: EJS templates allow you to generate HTML using plain javascript, making it easy to loop through items and output their values in your custom HTML.\nNormally, I would start with the code of the standard/default option and tinker around until I figured out how to change the elements I wanted. But the Quarto default templates were more complex than I needed and didn‚Äôt serve as a great starting point. So I went to option number two: adapt the work of people who know more than I do. I looked at lots of Quarto websites‚Äîspecifically, lots of Quarto listings pages‚Äîin an attempt to find something custom that I liked well enough to use as a starting point. I liked the publications page of the AffCom lab at KU as well as the talks page on Andrew Heiss‚Äôs site. Looking at those templates (here and here) was enough to get me started.\nThe template for my blog is pretty simple with the title of the post, the publication date floated to the right, the categories, and the summary. The template for my talks page is similar but also includes buttons for any talk-related links I have like slides, a website, or a repo. The template files pull out the correct elements in the appropriate order from the YAML options of the index.qmd files, and then my .css pages create the style that I want for those listings.\n\n\nMetadata files\nIncluding a _metadata.yml file in both my blog and talks folder is useful to specify YAML options that should apply to all of the index.qmd files within those folders. This way I don‚Äôt have to, for example, add my name as the author in each file or specify my css file or customize my table of contents each time I write a blog post.\n# Date format\ndate-format: \"YYYY-MM-DD\"\n\n# Enable banner style title blocks\ntitle-block-banner: \"#1F2041\"\n\n# Add blog-post class to the banner header area\nformat: \n  html:\n    css: blog.css\n    knitr:\n      opts_chunk:\n        dev: png\n        dev.args:\n          bg: transparent\n\n# Default for table of contents\ntoc: true\ntoc-title: CONTENTS\ntoc-location: left\n\nfreeze: true\n\n# Default author\nauthor:\n  - name: Meghan Hall\n\n\nChanging metadata labels\nThis was one of those little things that fell into the ‚Äúnot necessary but would be nice‚Äù category. The standard metadata labels for author and date look like so:\n\nThis worked nicely for my blog posts, but for my talk pages, I wanted it to look more like the following:\n\nI almost gave up more than once on this change since it was such a small detail, but I figured it out! And maybe there‚Äôs a simpler way to achieve this, but I supplied my own title-block.html file in the talks directory to more clearly specify the output of that title block.\nI created my file based on my own website‚Äôs generated html for a blog post (which you can access on Chrome by right-clicking and selecting View Page Source), plus the standard Quarto title-block.html file."
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#static-files",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#static-files",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "Static files",
    "text": "Static files\nMy website has blog posts as well as landing pages for all of the talks I‚Äôve given (the blog and talks folder, respectively, as described above), but it also needs to hold the various slide decks for all of those talks. When using blogdown, it was easy to drop all of that content into a static folder that you could link to from the root of your website.\nBut where should those files live within the Quarto structure? Netlify builds your Quarto site from the _site directory, but that directory rebuilds when you rerender your site. Google came to the rescue, and I followed Garrick Aden-Buie‚Äôs advice to write a little post-render script.\nThe post-render section of my _quarto.yml code instructs Quarto to copy everything in my static folder over to the _site folder after everything has finished rendering.\nproject:\n  type: website\n  render:\n    - \"blog/**/*.qmd\"\n    - \"talks/**/*.qmd\"\n    - \"index.qmd\"\n    - \"blog/index.qmd\"\n    - \"talks/index.qmd\"\n    - \"404.qmd\"\n  post-render:\n    - \"cp -r static/. _site/\"\nThe period after static/ is important because I want those files (most of which are contained in a slides directory) to be copied directly over to the root of my website in the _site folder without the static folder itself. That way the links are formatted like slides/rstudioconf, not static/slides/rstudioconf."
  },
  {
    "objectID": "blog/2024-02-11-moving-to-quarto/index.html#redirects",
    "href": "blog/2024-02-11-moving-to-quarto/index.html#redirects",
    "title": "Moving My Website from blogdown to Quarto",
    "section": "Redirects",
    "text": "Redirects\nWhen migrating your website over to Quarto, chances are some of your links might change. For example, on my old website a sample link for a talk might be /talk/rladiesparis/, but now I‚Äôve changed it to /talks/2022-09-08-rladies-paris/. I have those old links floating around on Twitter and on other websites, so I‚Äôd like to make sure that they still work and point to the new links. Enter: redirects.\nIf you use Netlify (which I highly recommed, as the Quarto/GitHub/Netlify integration is so seamless), you can supply a redirects file that lists all the old urls and their new locations. Netlify builds from Quarto‚Äôs _site directory, so just put the _redirects file (here‚Äôs mine) in your _site folder, right? I tried that first, but it doesn‚Äôt work because the _site directory rebuilds when you rerender your site. Instead, I moved my file into the static folder, which gets copied over to _site as described above.\nMy use case here for redirects is pretty simple: I had a small number of urls to switch, and I don‚Äôt expect to frequently add to that list. If your situation is more complicated and/or you want that _redirects file to automatically generate shorter urls when you create new pages, there are options described here and here."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "",
    "text": "Creating parameterized documents (i.e., documents with outputs‚Äîtext, plots, tables, etc‚Äîthat change based on the values of the parameters that you select) is such a common task for data analysts, and it‚Äôs a task that can be made much easier with Quarto. (Quarto is a new open-source technical publishing system from RStudio. It is similar in many ways to R Markdown, except that it doesn‚Äôt require R, supports more languages, and combines the functionality of many R Markdown packages, e.g., xaringan, bookdown.) Those documents can take many output forms, but this post focuses specifically on PDF reports. For the example report I‚Äôm walking through here, I used data from the wonderful palmerpenguins data package because who doesn‚Äôt like üêß?!\nThe full PDF is available here, and below is a screenshot, along with some annotations as to what elements in the report change automatically based on our parameters.\n\nToday‚Äôs focus is mostly on the Quarto elements: how to customize the look of our parameterized PDF (like those colors and fonts!) and how to render it easily for the parameter values that I want. I won‚Äôt go into too much detail about how I incorporated the parameters into the R code itself, but the full .qmd file is here and I‚Äôm happy to answer any questions about those elements! (Throughout, I am coming from the perspective of using R and RStudio, though many of the customization elements will still apply if you are using a different coding language and/or IDE.)"
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#parameterized-reports-in-r-and-quarto",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#parameterized-reports-in-r-and-quarto",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "",
    "text": "Creating parameterized documents (i.e., documents with outputs‚Äîtext, plots, tables, etc‚Äîthat change based on the values of the parameters that you select) is such a common task for data analysts, and it‚Äôs a task that can be made much easier with Quarto. (Quarto is a new open-source technical publishing system from RStudio. It is similar in many ways to R Markdown, except that it doesn‚Äôt require R, supports more languages, and combines the functionality of many R Markdown packages, e.g., xaringan, bookdown.) Those documents can take many output forms, but this post focuses specifically on PDF reports. For the example report I‚Äôm walking through here, I used data from the wonderful palmerpenguins data package because who doesn‚Äôt like üêß?!\nThe full PDF is available here, and below is a screenshot, along with some annotations as to what elements in the report change automatically based on our parameters.\n\nToday‚Äôs focus is mostly on the Quarto elements: how to customize the look of our parameterized PDF (like those colors and fonts!) and how to render it easily for the parameter values that I want. I won‚Äôt go into too much detail about how I incorporated the parameters into the R code itself, but the full .qmd file is here and I‚Äôm happy to answer any questions about those elements! (Throughout, I am coming from the perspective of using R and RStudio, though many of the customization elements will still apply if you are using a different coding language and/or IDE.)"
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#yes-this-means-latex",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#yes-this-means-latex",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Yes, this means LaTeX",
    "text": "Yes, this means LaTeX\nSimilarly to how Quarto uses reveal.js to make HTML presentations, Quarto uses LaTeX to make PDF documents. Part of the beauty of Quarto is that you don‚Äôt need to learn LaTeX‚Äîyou can create great PDFs without it. But just like I enjoy learning more about CSS to customize my Quarto HTML products, I enjoy learning more about LaTex to customize my PDF products!\nStandard disclaimer: I am not a LaTeX expert! The tips I offer below are certainly not the only solutions, and depending on your particular use cases they may not be the best solutions. But I think they are useful for someone getting their feet wet with LaTeX and learning how to use LaTeX elements to customize the look of their Quarto PDFs.\nLet‚Äôs review how things work structurally: Quarto documents start with a YAML header. Below is the YAML header for this PDF.\n---\nformat: \n  pdf: \n    mainfont: \"Avenir\"\n    sansfont: \"Avenir\"\n    geometry:\n      - top=0.75in\n      - right=0.75in\n      - bottom=0.75in\n      - left=0.75in\n      - heightrounded\n    number-sections: true\n    include-in-header: \n      - \"penguin-header.tex\"\n    toc: false\neditor: source\nexecute:\n  warning: false\n  echo: false\nparams:\n  species: 'Adelie'\n  year: 2009\n---\nYou‚Äôll see that the include-in-header option references a penguin-header.tex file. You can include raw LaTeX directly in the body of your Quarto document, which we‚Äôll incorporate below, and/or you can reference LaTeX files (that‚Äôs the .tex extension) that can be executed before your Quarto document (as part of what‚Äôs known as the LaTeX preamble). Having a file like this is handy whether you want to customize an entire header page (resources on that available below) or to specify a few LaTeX packages and commands before you get into your Quarto document.\nThis is the content of our penguin-header.tex file:\n\\let\\paragraph\\oldparagraph\n\\let\\subparagraph\\oldsubparagraph\n\n\\usepackage{xcolor}\n\\usepackage{titlesec}\n\\usepackage[parfill=0pt]{parskip}\n\\usepackage{fontspec}\n\n\\setsansfont{Prompt}[\n    Path=Prompt/,\n    Scale=0.9,\n    Extension = .ttf,\n    UprightFont=*-Regular,\n    BoldFont=*-Bold,\n    ItalicFont=*-Italic,\n    ]\n\n\\definecolor{Chinstrap}{HTML}{C25BCC}\n\\definecolor{Gentoo}{HTML}{047075}\n\\definecolor{Adelie}{HTML}{FF6600}\n\n\\titleformat{\\section}\n  {\\sffamily\\Large\\bfseries}{\\thesection}{1em}{}[{\\titlerule[0.8pt]}]\nYou specify LaTeX packages like \\usepackage{xcolor}. Here, we‚Äôre adding a package to help us define custom colors, a package for custom fonts, and a package to help make nifty section headers."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-fonts",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-fonts",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Custom fonts",
    "text": "Custom fonts\nIf you want to use a system font in your PDF (meaning it‚Äôs already on your computer), you can adjust the values of the mainfont, sansfont and monofont options in the header YAML.\n---\nformat: \n  pdf: \n    mainfont: \"Avenir\"\n    sansfont: \"Avenir\"\nIf you would rather use a Google font, you can download it from the site and save the files into the same directory as your Quarto document. For the example below, I downloaded the Prompt font and added the following code to my penguin-header.tex file. This is setting my sans serif font to Prompt.\n\\setsansfont{Prompt}[\n    Path=Prompt/,\n    Scale=0.9,\n    Extension = .ttf,\n    UprightFont=*-Regular,\n    BoldFont=*-Bold,\n    ItalicFont=*-Italic,\n    ]\nAnd then the command \\sffamily is the first line (below the YAML) in my Quarto document and states that I want to use the font I specified in the penguin-header.tex file. This is an example of including raw LaTeX in the body of your document."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-colors",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#custom-colors",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Custom colors",
    "text": "Custom colors\nUsing a LaTeX package like xcolor allows us to define custom colors like so: \\definecolor{Chinstrap}{HTML}{C25BCC}. The penguin-header.tex file defines a color for each penguin species‚ÄîChinstrap, Adelie, and Gentoo‚Äîthat we can use to render different colors depending on which parameter value of species we select.\nBut how to connect the custom-defined color to our parameter so that the right one is used?\nIn an R code block, you can refer to your report parameters with params$species. If you want to reference these parameters in the Quarto text, enclose like so: `r params$species`. And what‚Äôs so neat about Quarto is that you can use this inline code as part of the LaTeX code. So the following command will set the text color to the custom color we defined based on the species determined via the parameter: \\color{`r params$species`}. If our parameter selects the Adelie species, then that piece of code will evaluate as \\color{Adelie} and our orange-y #FF6600 will be used. If you want to set the color back to black, use \\color{black}."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#rendering-parameterized-reports",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#rendering-parameterized-reports",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Rendering parameterized reports",
    "text": "Rendering parameterized reports\nThere are three ways to render this type of parameterized report using Quarto. In increasing order of complexity (and usefulness!):\nFirst. Manually change the parameter values in the header YAML of your .qmd file and hit Render in RStudio. This will create a PDF file in the same directory as your .qmd file with the same name as your .qmd file but with a .pdf extension. This is great for one-offs or for testing!\nSecond. If you prefer to work in the terminal, you can use the following command: quarto render penguins.qmd -P species:'Adelie' -P year:'2009'\nThird. For this palmerpenguins data, we have potentially nine separate PDF reports to create: three years for three species. That‚Äôs a little annoying to do via the first two methods, and you can imagine how really annoying it could get if you had even more parameter combinations. Thankfully, we can leverage the quarto::quarto_render() function in R to automatically create a report for each parameter combination we want and name that file in a way that we specify.\nrunpdfs &lt;- function(species, year) {\n  quarto::quarto_render(\n    \"penguins.qmd\",\n    output_format = \"pdf\",\n    execute_params = list(species = species, year = year),\n    output_file = glue::glue(\"{species} {year}.pdf\")\n  )\n}\n\npurrr::map2(unique(penguins$species), unique(penguins$year), \n            runpdfs)\nThe first chunk of code defines a new function called runpdfs that is based on the quarto::quarto_render() function. The two arguments to runpdfs are the two Quarto report parameters we have: species and year. Those arguments are used in the execute_params argument to specify the parameter values to Quarto and in the output_file argument to specify the name of the files.\nThe second chunk of code uses map2() from purrr to iteratively run that new function, runpdfs, for every unique combination of species and year from the penguins data set.\nThe code above will result in nine PDF files. The two available examples are Adelie 2009.pdf and Gentoo 2007.pdf."
  },
  {
    "objectID": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#resources-for-learning-more",
    "href": "blog/2022-09-25-tips-for-custom-parameterized-pdfs-in-quarto/index.html#resources-for-learning-more",
    "title": "Tips for Custom Parameterized PDFs in Quarto",
    "section": "Resources for learning more",
    "text": "Resources for learning more\nThe Quarto documentation for PDFs covers the basics of creating PDFs in Quarto and also lists all the available PDF options. There‚Äôs also the list of LaTeX variables for Pandoc.\nFor more specifics about customizing LaTeX within Quarto, I found the title page resources from NMFS Open Science to be incredibly helpful for inspiration and also for learning what the possibilities are.\nOne of my personal favorite methods for learning is reviewing others‚Äô code. The files for this example, penguins.qmd and penguins-header.tex, are available on my GitHub and were designed to showcase some basic LaTeX structure and commands."
  },
  {
    "objectID": "puplications.html",
    "href": "puplications.html",
    "title": "Publications",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "puplications.html#publications-by-date",
    "href": "puplications.html#publications-by-date",
    "title": "Publications",
    "section": "Publications by date",
    "text": "Publications by date\n*shared first authors\n2022\nMolalign Gualu Gobena, Yebelay Berelie. Modeling the Determinant of Time to Age at First Marriage among Women in Ethiopia using Cox models with mixed effects. eLife 2022; 11: e66695. Link to paper\nKrause S, Gfrerer S, von K√ºgelgen A, Reuse C, Dombrowski N, Villanueva L, et al.¬†The importance of biofilm formation for cultivation of a Micrarchaeon and its interactions with its Thermoplasmatales host. Nat Commun 2022; 13: 1735. Link to paper\nLangwig MV, De Anda V, Dombrowski N, Seitz KW, Rambo IM, Greening C, et al.¬†Large-scale protein level comparison of Deltaproteobacteria reveals cohesive metabolic groups. ISME J 2022; 16: 307‚Äì320. Link to paper\n2021\nDe Anda V, Chen L-X, Dombrowski N, Hua Z-S, Jiang H-C, Banfield JF, et al.¬†Brockarchaeota, a novel archaeal phylum with unique and versatile carbon cycling pathways. Nat Commun 2021; 12: 2404. Link to paper\nMayer T, Mari A, Almario J, Murillo-Roos M, Syed M. Abdullah H, Dombrowski N, et al.¬†Obtaining deeper insights into microbiome diversity using a simple method to block host and nontargets in amplicon sequencing. Molecular Ecology Resources 2021; 21: 1952‚Äì1965. Link to paper"
  },
  {
    "objectID": "pages/about.html#skills",
    "href": "pages/about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\n Statistical Software: R | STATA | SAS | SPSS\nData Collection Apps: Kobo toolbox | SurveyCTO | REDCap\nProject Management: Project Management | Surveys | Budget\nImpact Evaluation: Causal Inference | Impact Evaluation\nDashboard: Power BI | R Markdown (flexdashboard) | Shiny App\nStudy Design: Study Design | Sampling Technique | Statistical Modelling"
  },
  {
    "objectID": "pages/about.html#skills-1",
    "href": "pages/about.html#skills-1",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills"
  },
  {
    "objectID": "pages/about.html#skills-2",
    "href": "pages/about.html#skills-2",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\nSkills\n\n\n\n\n\n\n&lt;div class=\"featurette-feature-icon\"&gt;\n  &lt;i class=\"fab fa-r-project\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-name\"&gt;\n  Statistical Software\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-description\"&gt;\n  R | STATA | SAS | SPSS\n&lt;/div&gt;\n\n\n&lt;div class=\"featurette-feature-icon\"&gt;\n  &lt;i class=\"fas fa-mobile-alt\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-name\"&gt;\n  Data Collection Apps\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-description\"&gt;\n  Kobo toolbox | SurveyCTO | REDCap\n&lt;/div&gt;\n\n\n&lt;div class=\"featurette-feature-icon\"&gt;\n  &lt;i class=\"fas fa-project-diagram\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-name\"&gt;\n  Project Management\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-description\"&gt;\n  Project Management | Surveys | Budget\n&lt;/div&gt;\n\n\n&lt;div class=\"featurette-feature-icon\"&gt;\n  &lt;i class=\"fas fa-chart-line\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-name\"&gt;\n  Impact Evaluation\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-description\"&gt;\n  Causal Inference | Impact Evaluation\n&lt;/div&gt;\n\n\n&lt;div class=\"featurette-feature-icon\"&gt;\n  &lt;i class=\"fas fa-chart-bar\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-name\"&gt;\n  Dashboard\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-description\"&gt;\n  Power BI | R Markdown (flexdashboard) | Shiny App\n&lt;/div&gt;\n\n\n&lt;div class=\"featurette-feature-icon\"&gt;\n  &lt;i class=\"fas fa-clipboard-list\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-name\"&gt;\n  Study Design\n&lt;/div&gt;\n&lt;div class=\"featurette-feature-description\"&gt;\n  Study Design | Sampling Technique | Statistical Modelling\n&lt;/div&gt;"
  },
  {
    "objectID": "puplications.html#publications-cronolgical-order",
    "href": "puplications.html#publications-cronolgical-order",
    "title": "Publications",
    "section": "Publications cronolgical order",
    "text": "Publications cronolgical order\n\nMolalign Gualu Gobena, Yebelay Berelie. Modeling the Determinant of Time to Age at First Marriage among Women in Ethiopia using Cox models with mixed effects. Reproductive Health volume 19, Article number: 32 (2022). Link to paper\nMuluneh Alene, Leltework Yismaw, Yebelay Berelie, Bekalu Kassie, Reta Yeshambe, Moges Agazhe Assemie. Prevalence and determinants of unintended pregnancy in Ethiopia: A systematic review and meta-analysis of observational studies. Link to paper\n\n\n\nGobena, M.G. and Berelie, Y., 2021. Modeling the Determinant of Time to Age at First Marriage among Women in Ethiopia using Cox models with mixed effects. BMC Reproductive Health. DOI: https://doi.org/10.1186/s12978-022-01339\nBerelie, Y., Yeshiwas, D., Yismaw, L., Alene, M. (2020). Determinants of institutional delivery service utilization in Ethiopia: a population-based cross-sectional study. BMC Public Health 20, 1077. doi: https://doi.org/10.1186/s12889-020-09125-2\n\n\n\nAlene M, Yismaw L, Berelie Y, Kassie B, Yeshambel R, Assemie MA (2020). Prevalence and determinants of unintended pregnancy in Ethiopia: A systematic review and meta-analysis of observational studies. PLoS ONE 15(4): e0231012. doi: https://doi.org/10.1371/journal.pone.0231012\n\n\n\nYeshiwas, D., & Berelie, Y. (2020). Forecasting the Covolatility of Coffee Arabica and Crude Oil Prices: A Multivariate GARCH Approach with High-Frequency Data. Journal of Probability and Statistics 2020(2):1-10. Doi: https://doi.org/10.1155/2020/1424020.\n\n\n\nBerelie, Y., Yismaw, L., Tesfa, E., & Alene, M. (2019). Risk factors for under-five mortality in Ethiopia: Evidence from the 2016 Ethiopian Demographic and Health Survey. South African Journal of Child Health, 13(3), 137-140. DOI: 10.7196/SAJCH.2019.v13i3.1645\nAlene, M., Yismaw, L., Berelie, Y., & Kassie, B. (2019). Health care utilization for common childhood illnesses in rural parts of Ethiopia: evidence from the 2016 Ethiopian demographic and health survey. BMC Public Health, 19(1), 57. Doi: https://doi.org/10.1186/s12889-019-6397-x\n\nAbstract summited for the conference :\n\nSisay A., Yebelay Berelie, Biniyam E., Kyle Milligan, Snigdha V., ¬†Helen C., Dereje H., Solomon A., Wondimu T., Minesh Pradyuman. Successful suppression of HIV viral load in PWH with virologic failure in PEPFAR-supported clinics ‚Äì Addis Ababa, Ethiopia, 2015‚Äì2021, ¬†submitted to AIDS 2022.\n\nManuscripts under review\n\nBerelie Y., Jim Todd, Robert Peck, 2023. A mixed model for assessing the impact of DTG-based ART regime on weight change in Tanzanian HIV patients. (Submitted to International AIDS).\n\nUnder preparation Works at CDC, Ethiopia\n\nSisay Alemayehu Abayneh, Yebelay Berelie, Biniyam Eskinder Seid, Dereje Habte, Wondimu Teferi, Minesh Shah Pradyuman. Rates of switching to second-line antiretroviral therapy after documented Virologic failure in Addis Ababa, Ethiopia.\nSisay Alemayehu Abayneh, Yebelay Berelie, Biniyam Eskinder Seid, Dereje Habte, Wondimu Teferi, Minesh Shah Pradyuman. Time to Viral Suppression after documented Virologic failure in Addis Ababa, Ethiopia.\n\nSeminar presentation:\n\nDifference-in-Differences (DID) for Impact Evaluation, October 25, 2021, Stellenbosch University, South Africa.\nInsight to Different Methods for Handling Missing Data: Application with R, July 5, 2021, Debre Markos University, Ethiopia."
  },
  {
    "objectID": "puplications.html#peer-reviewed-publications",
    "href": "puplications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "Peer reviewed Publications",
    "text": "Peer reviewed Publications\n\nGobena, M.G. and Berelie, Y., 2021. Modeling the Determinant of Time to Age at First Marriage among Women in Ethiopia using Cox models with mixed effects. BMC Reproductive Health.volume 19, Article number: 32 (2022). DOI link\nBerelie, Y., Yeshiwas, D., Yismaw, L., Alene, M. (2020). Determinants of institutional delivery service utilization in Ethiopia: a population-based cross-sectional study. BMC Public Health, 20, 1077. doi link\nAlene M, Yismaw L, Berelie Y, Kassie B, Yeshambel R, Assemie MA (2020). Prevalence and determinants of unintended pregnancy in Ethiopia: A systematic review and meta-analysis of observational studies. PLoS ONE 15(4): e0231012. doi link.\nYeshiwas, D., & Berelie, Y. (2020). Forecasting the Covolatility of Coffee Arabica and Crude Oil Prices: A Multivariate GARCH Approach with High-Frequency Data. Journal of Probability and Statistics 2020(2):1-10. Doi link.\nBerelie, Y., Yismaw, L., Tesfa, E., & Alene, M. (2019). Risk factors for under-five mortality in Ethiopia: Evidence from the 2016 Ethiopian Demographic and Health Survey. South African Journal of Child Health, 13(3), 137-140. DOI link.\nAlene, M., Yismaw, L., Berelie, Y., & Kassie, B. (2019). Health care utilization for common childhood illnesses in rural parts of Ethiopia: evidence from the 2016 Ethiopian demographic and health survey. BMC Public Health, 19(1), 57. Doi link"
  },
  {
    "objectID": "puplications.html#manuscripts-under-review",
    "href": "puplications.html#manuscripts-under-review",
    "title": "Publications",
    "section": "Manuscripts under review",
    "text": "Manuscripts under review\n\nBerelie Y., Jim Todd, Robert Peck, 2024. A mixed model for assessing the impact of DTG-based ART regime on weight change in Tanzanian HIV patients. (Submitted to International AIDS).\nMulusew J., Saba Y, Fikreselassie G. , Yebelay B. , et al., 2024. Maternal education and child vaccination in Ethiopia: A natural experiment. Elsevier, journal of Social Science & Medicine."
  },
  {
    "objectID": "puplications.html#under-preparation",
    "href": "puplications.html#under-preparation",
    "title": "Publications",
    "section": "Under preparation",
    "text": "Under preparation\n\nPrevalence of zero-dose children and factors associated before and after COVID-19 among children aged 12‚Äì23 months across 8 African countries.\nSisay Alemayehu Abayneh, Yebelay Berelie, Biniyam Eskinder Seid, Dereje Habte, Wondimu Teferi, Minesh Shah Pradyuman. Time to Viral Suppression after documented Virologic failure in Addis Ababa, Ethiopia."
  },
  {
    "objectID": "puplications.html#seminar-presentation",
    "href": "puplications.html#seminar-presentation",
    "title": "Publications",
    "section": "Seminar presentation:",
    "text": "Seminar presentation:\n\nDifference-in-Differences (DID) for Impact Evaluation, October 25, 2021, Stellenbosch University, South Africa.\nInsight to Different Methods for Handling Missing Data: Application with R, July 5, 2021, Debre Markos University, Ethiopia."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Upcoming\nNone at this time.\n\n\nPast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial data analaysis Training\n\n\n\n\n\n\nR\n\n\nsf\n\n\nSpatial analysis\n\n\n\nA step by step procedure for learning spatial from scrach\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies DC Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Quarto reports improve understanding of soil health\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nagriculture\n\n\nsoil health\n\n\n\nCreating custom soil health reports with Quarto\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShiny optimization of climate benefits from a statewide agricultural grant program\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nagriculture\n\n\nclimate\n\n\n\nDevelopment process of {WaCSE} shiny app\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Soil Health Initative and Climate Smart Estimator\n\n\n\n\n\n\nagriculture\n\n\nsoil health\n\n\nclimate\n\n\n\nOverview of the WA Soil Health Initative & WA Climate Smart Estimator {WaCSE} shiny app\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2024-02-21_rladies-abuja-quarto-params/index.html",
    "href": "talks/2024-02-21_rladies-abuja-quarto-params/index.html",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "Course website  Slides  Code\n\n\nDetails\nüìÜ February 21, 2024 // 4:30 pm - 6:30 pm WAT\nüè® Virtual\nüè° Workshop website\nüîñ Source tag\n\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2024,\n  author = {Berehan, Yebelay},\n  title = {Spatial Data Analaysis {Training}},\n  date = {2024-02-21},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2024. ‚ÄúSpatial Data Analaysis Training.‚Äù\nFebruary 21, 2024."
  },
  {
    "objectID": "talks/2023-09-25_posit_parameterized-quarto/index.html",
    "href": "talks/2023-09-25_posit_parameterized-quarto/index.html",
    "title": "Parameterized Quarto reports improve understanding of soil health",
    "section": "",
    "text": "Slides  Code  Video \n\nDetails\nüìÜ September 25, 2023 // 5:30 pm - 5:40 pm CDT üè® Chicago, IL\nüå† posit::conf(2023)\n\n\nAbstract\nSoil sampling data are notoriously challenging to tidy and effectively communicate to farmers. We used functional programming with the tidyverse to reproducibly streamline data cleaning and summarization. To improve project outreach, we developed a Quarto project to dynamically create interactive HTML reports and printable PDFs. Custom to every farmer, reports include project goals, measured parameter descriptions, summary statistics, maps, tables, and graphs.\nOur case study presents a workflow for data preparation and parameterized reporting, with best practices for effective data visualization, interpretation, and accessibility.\nSee an example HTML report.\nLearn more about the Washington Soil Health Initiative State of the Soils Assessment.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and McIlquham, Molly and Sarpong, Kwabena and\n    Michel, Leslie and Potter, Teal and Griffin LaHue, Deirdre and\n    Gelardi, Dani},\n  title = {Parameterized {Quarto} Reports Improve Understanding of Soil\n    Health},\n  date = {2023-09-25},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Molly McIlquham, Kwabena Sarpong, Leslie Michel, Teal\nPotter, Deirdre Griffin LaHue, and Dani Gelardi. 2023.\n‚ÄúParameterized Quarto Reports Improve Understanding of Soil\nHealth.‚Äù September 25, 2023."
  },
  {
    "objectID": "talks/2023-06-13_wade_washi-wacse/index.html",
    "href": "talks/2023-06-13_wade_washi-wacse/index.html",
    "title": "Washington Soil Health Initative and Climate Smart Estimator",
    "section": "",
    "text": "Slides  Video \n\nDetails\nüìÜ June 13, 2023 // 1:30 pm - 2:20 pm PT\nüè® Leavenworth, WA\nüå† Washington Association of District Employees (WADE) conference\n\n\nAbstract\nWashington Soil Health Initiative overview and updates.\nHow to get the most of the Sustainable Farms and Fields Washington Climate Smart Estimator (WaCSE) tool.\n\n\nSlides\n\n\nOops! Your browser doesn‚Äôt seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Soil} {Health} {Initative} and {Climate} {Smart}\n    {Estimator}},\n  date = {2023-06-13},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. ‚ÄúWashington\nSoil Health Initative and Climate Smart Estimator.‚Äù June 13,\n2023."
  },
  {
    "objectID": "talks/2023-08-19_cascadia_shiny-wacse/index.html",
    "href": "talks/2023-08-19_cascadia_shiny-wacse/index.html",
    "title": "Shiny optimization of climate benefits from a statewide agricultural grant program",
    "section": "",
    "text": "Slides  Code  Video \n\nDetails\nüìÜ August 19, 2023 // 2:05 pm - 2:20 pm PT\nüè® Seattle, WA\nüå† Cascadia R Conf\n\n\nAbstract\nWashington‚Äôs Sustainable Farms and Fields program provides grants to growers to increase soil carbon or reduce greenhouse gas (GHG) emissions on their farms. To optimize the climate benefits of the program, we developed the Washington Climate Smart Estimator {WaCSE} using R and Shiny.\nIntegrating national climate models and datasets, this intuitive, regionally specific user interface allows farmers and policymakers to compare the climate benefits of different agricultural practices across Washington‚Äôs diverse counties and farm sizes. Users can explore GHG estimates in interactive tables and plots, download results in spreadsheets and figures, and generate PDF reports. In this talk, we present the development process of {WaCSE} and discuss the lessons we learned from creating our first ever Shiny app.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Shiny Optimization of Climate Benefits from a Statewide\n    Agricultural Grant Program},\n  date = {2023-08-19},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. ‚ÄúShiny\nOptimization of Climate Benefits from a Statewide Agricultural Grant\nProgram.‚Äù August 19, 2023."
  },
  {
    "objectID": "talks/2024-01-18_rladies-dc_quarto-params/index.html",
    "href": "talks/2024-01-18_rladies-dc_quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies DC Workshop",
    "section": "",
    "text": "Course website  Slides  Code  Video \n\nDetails\nüìÜ January 18, 2024 // 6:30 pm - 8:30 pm EDT\nüè® Virtual\nüÜì FREE with registration\nüè° Workshop website\nüîñ Source tag\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you‚Äôll learn by checking out the slides for my posit::conf(2023) talk.\nEveryone is welcome to attend. If you‚Äôre new to Quarto, we recommend watching Tom Mock‚Äôs excellent 2-hour introduction to Quarto.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2024,\n  author = {Ryan, Jadey},\n  title = {Parameterized {Reports} with {Quarto:} {R-Ladies} {DC}\n    {Workshop}},\n  date = {2024-01-18},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2024. ‚ÄúParameterized Reports with Quarto: R-Ladies DC\nWorkshop.‚Äù January 18, 2024."
  },
  {
    "objectID": "presentations/2024-01-18_rladies-dc_quarto-params/index.html",
    "href": "presentations/2024-01-18_rladies-dc_quarto-params/index.html",
    "title": "Joint Models Workshop",
    "section": "",
    "text": "Slides  Code\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2024,\n  author = {Berehan, Yebelay},\n  title = {Joint {Models} {Workshop}},\n  date = {2024-01-18},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2024. ‚ÄúJoint Models Workshop.‚Äù January\n18, 2024."
  },
  {
    "objectID": "presentations/2023-08-19_cascadia_shiny-wacse/index.html",
    "href": "presentations/2023-08-19_cascadia_shiny-wacse/index.html",
    "title": "Introduction to Rmarkdown",
    "section": "",
    "text": "Slides\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2023,\n  author = {Berehan, Yebelay},\n  title = {Introduction to {Rmarkdown}},\n  date = {2023-08-19},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2023. ‚ÄúIntroduction to Rmarkdown.‚Äù August\n19, 2023."
  },
  {
    "objectID": "presentations/2023-06-13_wade_washi-wacse/index.html",
    "href": "presentations/2023-06-13_wade_washi-wacse/index.html",
    "title": "Washington Soil Health Initative and Climate Smart Estimator",
    "section": "",
    "text": "Slides  Video \n\nDetails\nüìÜ June 13, 2023 // 1:30 pm - 2:20 pm PT\nüè® Leavenworth, WA\nüå† Washington Association of District Employees (WADE) conference\n\n\nAbstract\nWashington Soil Health Initiative overview and updates.\nHow to get the most of the Sustainable Farms and Fields Washington Climate Smart Estimator (WaCSE) tool.\n\n\nSlides\n\n\nOops! Your browser doesn‚Äôt seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Soil} {Health} {Initative} and {Climate} {Smart}\n    {Estimator}},\n  date = {2023-06-13},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. ‚ÄúWashington\nSoil Health Initative and Climate Smart Estimator.‚Äù June 13,\n2023."
  },
  {
    "objectID": "presentations/2023-09-25_posit_parameterized-quarto/index.html",
    "href": "presentations/2023-09-25_posit_parameterized-quarto/index.html",
    "title": "C4ED Presentation them",
    "section": "",
    "text": "Slides\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2023,\n  author = {Berehan, Yebelay},\n  title = {C4ED {Presentation} Them},\n  date = {2023-09-25},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2023. ‚ÄúC4ED Presentation Them.‚Äù September\n25, 2023."
  },
  {
    "objectID": "presentations/2024-02-21_rladies-abuja-quarto-params/index.html",
    "href": "presentations/2024-02-21_rladies-abuja-quarto-params/index.html",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "Course website  Slides  Code\n\n\nDetails\nüìÜ February 21, 2024 // 4:30 pm - 6:30 pm WAT\nüè® Virtual\nüè° Workshop website\nüîñ Source tag\n\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2024,\n  author = {Berehan, Yebelay},\n  title = {Spatial Data Analaysis {Training}},\n  date = {2024-02-21},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2024. ‚ÄúSpatial Data Analaysis Training.‚Äù\nFebruary 21, 2024."
  },
  {
    "objectID": "presentations/2024-07-21_Spatial-data-visualization/index.html",
    "href": "presentations/2024-07-21_Spatial-data-visualization/index.html",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "Course website  Slides  Code\n\n\nDetails\nüìÜ February 21, 2024 // 4:30 pm - 6:30 pm WAT\nüè® Virtual\nüè° Workshop website\nüîñ Source tag\n\n\nSlides"
  },
  {
    "objectID": "presentations/2023-09-25_c4ed-theme/index.html",
    "href": "presentations/2023-09-25_c4ed-theme/index.html",
    "title": "C4ED Presentation them",
    "section": "",
    "text": "Slides\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2023,\n  author = {Berehan, Yebelay},\n  title = {C4ED {Presentation} Them},\n  date = {2023-09-25},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2023. ‚ÄúC4ED Presentation Them.‚Äù September\n25, 2023."
  },
  {
    "objectID": "presentations/2023-08-19_Rmarkdown/index.html",
    "href": "presentations/2023-08-19_Rmarkdown/index.html",
    "title": "Introduction to Rmarkdown",
    "section": "",
    "text": "Slides\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2023,\n  author = {Berehan, Yebelay},\n  title = {Introduction to {Rmarkdown}},\n  date = {2023-08-19},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2023. ‚ÄúIntroduction to Rmarkdown.‚Äù August\n19, 2023."
  },
  {
    "objectID": "presentations/2023-10-18_Joint-Models-EPHI/index.html",
    "href": "presentations/2023-10-18_Joint-Models-EPHI/index.html",
    "title": "Joint Models Workshop",
    "section": "",
    "text": "Slides  Code\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{berehan2024,\n  author = {Berehan, Yebelay},\n  title = {Joint {Models} {Workshop}},\n  date = {2024-01-18},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerehan, Yebelay. 2024. ‚ÄúJoint Models Workshop.‚Äù January\n18, 2024."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "What is spatial data and why should we use it?\nTypes of spatial data\nSpatial data in R: how to store, load, and tidy spatial data\nVisualizing spatial data\nExploring spatial data: Spatial authorization and spatial statistics to quantify relationships"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#section",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#section",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "What is spatial data and why should we use it?\nTypes of spatial data\nSpatial data in R: how to store, load, and tidy spatial data\nVisualizing spatial data\nExploring spatial data: Spatial authorization and spatial statistics to quantify relationships"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#what-is-spatial-data-and-why-should-we-use-it",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#what-is-spatial-data-and-why-should-we-use-it",
    "title": "Spatial data analaysis Training",
    "section": "What is Spatial data and why should we use it?",
    "text": "What is Spatial data and why should we use it?\n\nSpatial data, also known as geospatial data, refers to information that identifies the geographic location and characteristics of natural or constructed features and boundaries on the Earth.\nThis data is often represented in terms of Cartesian coordinates (x,y) for two-dimensional maps, but may also include altitude (z) for a three-dimensional representation.\nSpatial data can come in various forms including points, lines, and polygons.\nThe concept of spatial data is integral to a variety of applications that require an understanding of how different elements relate to each other within a geographical space.\nThis data can be collected through various means, including but not limited to, satellite imagery, aerial photography, and ground-based surveys."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#spatial-data-formats",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#spatial-data-formats",
    "title": "Spatial data analaysis Training",
    "section": "Spatial Data Formats",
    "text": "Spatial Data Formats\n\nLet us see a basic way to represent the spatial data.\nBut there is a variety of data formats to represent the data to suit different applications.\nIn most cases, spatial data formats are an extension of existing data formats.\n\n\n\n\n\n\n\n\nType\nNon-Spatial Data\nSpatial Data\n\n\n\n\nText\ncsv, json, xml\ncsv, geojson, gml, kml\n\n\nBinary/Compressed\npdf, xls, zip\nshapefile, geopdf, geopackage\n\n\nImages\ntiff, jpg, png\ngeotiff, jpeg2000\n\n\nDatabases\nSQLite, PostgreSQL, Oracle\nSpatialite, PostGIS, Oracle Spatial"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#spatial-data-types",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#spatial-data-types",
    "title": "Spatial data analaysis Training",
    "section": "Spatial Data Types",
    "text": "Spatial Data Types\nSpatial Data can be broadly categorized into 2 types - Vector and Raster.\n\n\n\n\n\n\n\n\nType\nSub Types\nExamples\n\n\n\n\nVector Data\nPoints: Represents features with a single coordinate pair (x, y).\nLocations of ATMs, tree\n\n\n\nLines: Represents linear features as ordered sequences of points.\nRoads, rivers, utility lines\n\n\n\nPolygons: Represents areas enclosed by closed loops of lines.\nBuildings, lakes, zones\n\n\nRaster Data\nGrids: Represents continuous data across a surface.\nSatellite images, digital elevation models (DEMs)\n\n\n\nPixels: Smallest units in a raster dataset, each with a specific value.\nValues representing color, temperature,"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#other-data-associated-with-vector-and-raster",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#other-data-associated-with-vector-and-raster",
    "title": "Spatial data analaysis Training",
    "section": "Other data associated with Vector and Raster",
    "text": "Other data associated with Vector and Raster\n\nAttribute data: Additional information describing the characteristics of spatial features.\nFor example in Vector data:\n\nSchools: have school name, number of students, education level.\nRoads:have road type, traffic volume, maintenance status.\nRegions: have population density, average income, and land use type.\n\n\nIn Raster Data: Each pixel can have multiple attributes, such as vegetation type or pollution levels."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#what-are-the-components-of-spatial-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#what-are-the-components-of-spatial-data",
    "title": "Spatial data analaysis Training",
    "section": "What are the components of spatial data",
    "text": "What are the components of spatial data\n\nGeometry: Refers to the coordinates that define the shape of an object.\n\nCoordinates: These can be in different coordinate systems, such as geographic (latitude) and longitude) or projected systems.\nTypes of Geometries: Common types include points, lines, and polygons.\nDimension: Geometry can also have dimensional attributes like 2D (x, y), 3D (x, y, z), or even 4D (x, y, z, time).\n\nTopology: Refers to the spatial relationships between geometric entities.\n\nSpatial Relationships\n\nAdjacency: How entities are next to each other.\nConnectivity: How entities are connected.\nContainment: How one entity is contained within another.\n\nRules and Constraints: Enforces rules such as:\n\nNo overlapping polygons.\nLines meeting at nodes.\nEnsuring polygon boundaries are closed loops."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#section-1",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#section-1",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "Attribute Data: provide detailed information about spatial features.\n\nExample: A point representing a city may have attributes like population, elevation, and city name.\nTypes of Attributes: Attribute data is either,\n\nNumeric (population size),\nCategorical (land use type), or\nTemporal (date of a satellite image).\n\nAttribute data is often stored in tables and linked to geometry through unique identifiers."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#how-does-the-workflow-of-handling-spatial-data-effectively",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#how-does-the-workflow-of-handling-spatial-data-effectively",
    "title": "Spatial data analaysis Training",
    "section": "How does the workflow of handling spatial data effectively",
    "text": "How does the workflow of handling spatial data effectively\nData Collection: Data can be collected in either\n\nSatellite Imagery: Captured by remote sensing satellites, these images can provide data on land cover, vegetation, weather patterns, and more.\nGPS Surveys: Use Global Positioning System technology to collect precise location data for mapping and navigation purposes.\nTraditional Surveying Methods: Involve measuring angles, distances, and elevations to map out areas accurately, often using tools like theodolites and total stations.\n\n\nData Storage:\n\nShapefiles: A popular vector data format that stores geometric locations and associated attribute data.\n\nIt consists of at least three files with extensions .shp (geometry), .shx (shape index), and .dbf (attribute data).\n\nGeoJSON: A format for encoding a variety of geographic data structures using JavaScript Object Notation (JSON).\nGeoTIFF: A raster format that includes spatial metadata embedded within the file, making it suitable for georeferenced raster data like satellite images.\nSpatial Databases: PostgreSQL with PostGIS extensions enables the storage, querying, and manipulation of spatial data within a relational database system, supporting complex spatial operations and large datasets."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#practice-for-spatial-data-in-r",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#practice-for-spatial-data-in-r",
    "title": "Spatial data analaysis Training",
    "section": "Practice for Spatial Data in R",
    "text": "Practice for Spatial Data in R\n\nSpatial Data: Information about the location and shape of geographic features.\nTypes: Vector Data and Raster Data.\n\n\nShapefile\n\nShapefile: A common format for representing vector data.\n\n.shp: Contains geometry data.\n.shx: Positional index of the geometry data.\n.dbf: Stores attributes for each shape.\n\nAdditional Files:\n\n.prj: Describes the projection (plain text).\n.sbn and .sbx: Spatial indices of the geometry data.\n.shp.xml: Contains spatial metadata in XML format."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#the-sf-package-for-spatial-vector-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#the-sf-package-for-spatial-vector-data",
    "title": "Spatial data analaysis Training",
    "section": "The sf package for spatial vector data",
    "text": "The sf package for spatial vector data\n\nThe sf package stores geometric features (termed simple features, hence sf) in a data frame.\nGeographic data is stored in the special geometry column.\nThe sf package is a modern alternative to the traditional sp, rgeos, and rgdal packages.\nSupports a wide range of geometry types: points, lines, polygons, and their ‚Äòmulti‚Äô versions.\nUseful for handling geographic vector data.\n\n\nUnderstanding Shapefiles\n\nShapefiles are a simple non-topological format to store geometric location and attribute information for geographical features.\nThey are commonly used in spatial analyses as a type of map.\nWe can read a shapefile or a sf object with the st_read() function of sf.\nFor example, here we read the Eth_Region_2013.shp shapefile of sf which contains the regions of Ethiopia, with their name, and Region and country code."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#reading-a-shapefile-with-sf",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#reading-a-shapefile-with-sf",
    "title": "Spatial data analaysis Training",
    "section": "Reading a Shapefile with sf",
    "text": "Reading a Shapefile with sf\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nethR_shape &lt;- st_read(\"Ethiopia_All/Eth_Region_2013.shp\", \n                      quiet = TRUE)\nclass(ethR_shape)\n\n[1] \"sf\"         \"data.frame\"\n\nhead(ethR_shape, 2)\n\nSimple feature collection with 2 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 460361.8 ymin: 976360 xmax: 864787.5 ymax: 1602973\nProjected CRS: Adindan / UTM zone 37N\n   REGIONNAME REG_P_CODE REG_Pcode      HRname HRpcode HRparent\n1 Addis Ababa         14      ET14 Addis Ababa    ET14       ET\n2        Afar          2      ET02        Afar    ET02       ET\n                        geometry\n1 MULTIPOLYGON (((475625.4 10...\n2 MULTIPOLYGON (((626409.3 16...\n\n\n\n\nThe sf object ethR_shape is a data.frame containing a collection with\n\n[11 simple features (rows)] {style=‚Äúcolor:#ae01c7;‚Äù} and 6 attributes (columns) [plus a list-column with the geometry of each feature] {style=‚Äúcolor:red;‚Äù}.\n\nA sf object contains the following objects of class sf, sfc and sfg:\n\nsf (simple feature): each row of the data.frame is a single simple feature consisting of attributes and geometry.\nsfc (simple feature geometry list-column): the geometry column of the data.frame is a list-column of class sfc with the geometry of each simple feature.\nsfg (simple feature geometry): each of the rows of the sfc list-column corresponds to the simple feature geometry (sfg) of a single simple feature.\n\nThe sf package stores geometric features in a data frame.\nGeographic data is stored in the special geometry column.\n\n\n\nUsing dim() and names() functions we can see how many rows and columns it contains, as well as the names of the columns.\n\n\ndim(ethR_shape)\n\n[1] 11  7\n\nnames(ethR_shape)\n\n[1] \"REGIONNAME\" \"REG_P_CODE\" \"REG_Pcode\"  \"HRname\"     \"HRpcode\"   \n[6] \"HRparent\"   \"geometry\"  \n\n\n\nEach row represents a geographic element, in this case, one of the 11 regions of Ethiopia.\nColumns contain information about the region, including the name and administrative code.\nThe last column is the geometry column."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#displaying-the-first-few-geometries",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#displaying-the-first-few-geometries",
    "title": "Spatial data analaysis Training",
    "section": "Displaying the First Few Geometries",
    "text": "Displaying the First Few Geometries\n\nhead(ethR_shape$geometry)\n\nGeometry set for 6 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -162404.5 ymin: 695805 xmax: 866537.7 ymax: 1602973\nProjected CRS: Adindan / UTM zone 37N\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((475625.4 1005122, 477066.7 1003...\n\n\nMULTIPOLYGON (((626409.3 1602131, 631762.1 1600...\n\n\nMULTIPOLYGON (((232286 1502975, 232875.5 150268...\n\n\nMULTIPOLYGON (((197369 992969.9, 194511.6 99040...\n\n\nMULTIPOLYGON (((858131.4 1069094, 857695.8 1068...\n\n\nEach region is captured as a multipolygon geometry."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#plotting-the-shapefile",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#plotting-the-shapefile",
    "title": "Spatial data analaysis Training",
    "section": "Plotting the Shapefile",
    "text": "Plotting the Shapefile\n\neth_reg &lt;- ethR_shape %&gt;%\n  dplyr::select(region_name = REGIONNAME)\nplot(eth_reg, main = \"Region name\")\n\n\n\n\n\n\n\n\n\nThe default behavior of plot with an sf object is to create a map for each column of data that describes the geography."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#plotting-with-ggplot2",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#plotting-with-ggplot2",
    "title": "Spatial data analaysis Training",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nlibrary(ggplot2)\nregion_map &lt;- ggplot(ethR_shape) +geom_sf(aes(fill=REGIONNAME))+\n  scale_fill_discrete(\"Region\") + coord_sf(datum = NA) +\n  theme_bw() + labs(x = NULL, y = NULL) + \n  ggtitle(\"Map of regions in Ethiopia\")\nprint(region_map)\n\n\n\n\n\n\n\n\n\nggplot2 is flexible once you learn the syntax.\nThe plot shows each region filled with different colors."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#adding-labels-to-the-map",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#adding-labels-to-the-map",
    "title": "Spatial data analaysis Training",
    "section": "Adding Labels to the Map",
    "text": "Adding Labels to the Map\n\nAdding labels to each region ensures they do not overlap.\n\n\nmap_label &lt;- ggplot() + geom_sf(data = ethR_shape) +\n  ggrepel::geom_label_repel(data = ethR_shape,\n    aes(label = REGIONNAME, geometry = geometry),\n    stat = \"sf_coordinates\",min.segment.length = 0) + \n  labs(x = NULL, y = NULL)+ coord_sf(datum = NA) +  theme_bw()\nprint(map_label)\n\n\n\n\n\n\n\n\n\nThe st_geometry() function can be used to retrieve the simple feature geometry list-column (sfc).\n\n\n\n# Geometries printed in abbreviated form \nst_geometry(ethR_shape) # View complete geometry \n\nGeometry set for 11 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -162404.5 ymin: 375657.9 xmax: 1491092 ymax: 1641360\nProjected CRS: Adindan / UTM zone 37N\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((475625.4 1005122, 477066.7 1003...\n\n\nMULTIPOLYGON (((626409.3 1602131, 631762.1 1600...\n\n\nMULTIPOLYGON (((232286 1502975, 232875.5 150268...\n\n\nMULTIPOLYGON (((197369 992969.9, 194511.6 99040...\n\n\nMULTIPOLYGON (((858131.4 1069094, 857695.8 1068...\n\nst_geometry(ethR_shape)[[1]]\n\nMULTIPOLYGON (((475625.4 1005122, 477066.7 1003356, 478017.5 1003453, 478585 1003869, 479491.6 1003613, 480739.7 1003156, 482699.6 1003456, 483464.2 1003407, 484258.3 1003998, 485518.6 1004151, 486408.7 1003783, 486715 1003114, 487027.6 1002343, 487139.3 1001449, 487135.5 1000524, 487506.8 999939.2, 488278 998878.8, 488642.2 998134.8, 489136.4 996928.2, 489558.3 995813.2, 489673.7 994490.2, 489485.1 993896.1, 489546.6 993007, 489694 992266.7, 489457.5 991396, 489313.4 990607.8, 488593.1 989770.7, 489127.6 988809.2, 488650.2 987801.8, 488063.6 987504, 487036.5 987710.5, 485652.9 988089.4, 484765.3 988427.7, 484120.5 987943.3, 484700.5 987267, 485495.9 986762.8, 486091.8 986335.4, 486746.3 985733.7, 486497.3 985094, 486071 984272.6, 486530.5 983243.5, 486081.7 982415.1, 485687.7 981584.2, 485564.9 980728.7, 485025.6 981813.9, 484429.6 981901.2, 483835.1 981696.4, 483185.8 981501.7, 482483.3 981483.4, 481636.3 981822.1, 480868.5 981521.2, 480970.9 980800.3, 481242.7 980260.3, 480982.5 979649.7, 480897.7 979026.3, 480636.8 978489.2, 480628.2 977791.6, 480689.6 977226, 479930.4 977000.9, 479386.9 977081.1, 478611.4 976775.1, 477741.1 976360, 477129.9 976613.7, 476539.2 976559.2, 475873 976630.2, 475630.2 977281.4, 475530 978455, 475038.4 979012.8, 473781 978587.6, 473220.8 978546.6, 472963 979138.5, 472017.8 979673.2, 471983.4 980343.9, 472214.2 981191.4, 471759.8 981551.4, 471905.6 982844.6, 471808.8 984002.6, 471031 983860, 470432.8 984280.5, 470010.1 984916.8, 469583.1 985330.3, 469122.3 985898.2, 467944.5 986031.7, 467234.5 986553.3, 466670.3 987336.4, 466343.1 988352.6, 465689.6 988114.8, 464891.6 988264.6, 464927.5 989212.5, 464407.8 990090.7, 463218.9 990476.1, 461894.6 990678.4, 460736.1 991006, 460361.8 991910.9, 461362.8 992126.6, 462037.3 992230, 462559.4 993165.4, 462385.1 994206.5, 461645 995055.6, 461743.7 995916.8, 462637.8 995811.1, 463402.8 995886.9, 463348.5 996628.2, 463424.9 997678.3, 463959.6 998442, 464016.9 999053, 464475.2 999549.4, 465220.7 1000308, 465239 1001359, 465239 1002108, 465428.1 1002850, 466433.4 1003369, 467107.3 1003676, 467774.2 1003821, 468861.9 1004474, 470011 1004248, 471211 1004375, 472621.9 1004779, 473549 1005371, 474200.2 1005620, 474863.1 1005625, 475625.4 1005122)))"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#creating-a-sf-object",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#creating-a-sf-object",
    "title": "Spatial data analaysis Training",
    "section": "Creating a sf object",
    "text": "Creating a sf object\n\nWe can use the st_sf() function to create a sf object by providing two elements, namely, a data.frame with the attributes of each feature, and a simple feature geometry list-column sfc containing simple feature geometries sfg.\nIn more detail, we create simple feature geometries sfg and use the st_sfc() function to create a simple feature geometry list-column sfc with them.\nThen, we use st_sf() to put the data.frame with the attributes and the simple feature geometry list-column sfc together.\n\nSimple feature geometries sfg objects can be, for example, of type POINT (single point), MULTIPOINT (set of points) or POLYGON (polygon), and can be created with st_point(), st_multipoint() and st_polygon(), respectively.\n\n\nHere, we create a sf object containing two single points, a set of points, and a polygon, with one attribute.\nFirst, we create the simple feature geometry objects (sfg) of type POINT, MULTIPOINT, and POLYGON.\nThen, we use st_sfc() to create a simple feature geometry list-column sfc with the sfg objects.\nFinally, we use st_sf() to put the data.frame with the attribute and the simple feature geometry list-column sfc together.\n\n\n\n# Single point (point as a vector) \np1_sfg &lt;- st_point(c(2, 2)) \np2_sfg &lt;- st_point(c(2.5, 3))  # Set of points \np &lt;- rbind(c(6, 2), c(6.1, 2.6), c(6.8, 2.5), \n           c(6.2, 1.5), c(6.8, 1.8)) \nmp_sfg &lt;- st_multipoint(p)  \n\n\n\n#|ccolumn: true\np1 &lt;- rbind(c(10, 0), c(11, 0), c(13, 2), c(12, 4), \n            c(11, 4), c(10, 0)) \np2 &lt;- rbind(c(11, 1), c(11, 2), c(12, 2), c(11, 1)) \npol_sfg &lt;- st_polygon(list(p1, p2))  # Create sf object \np_sfc &lt;- st_sfc(p1_sfg, p2_sfg, mp_sfg, pol_sfg) \ndf &lt;- data.frame(v1 = c(\"A\", \"B\", \"C\", \"D\"))\n\n# Plot single points, set of points and polygon\np_sf &lt;- st_sf(df, geometry = p_sfc)  \nlibrary(ggplot2) \nggplot(p_sf) + geom_sf(aes(col = v1), size = 3) + \n  theme_bw()"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#st_-functions",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#st_-functions",
    "title": "Spatial data analaysis Training",
    "section": "st_*() functions",
    "text": "st_*() functions\nCommon functions to manipulate sf objects include the following:\n\nst_read() reads a sf object,\nst_write() writes a sf object,\nst_crs() gets or sets a new coordinate reference system (CRS),\nst_transform() transforms data to a new CRS,\nst_intersection() intersects sf objects,\nst_union() combines several sf objects into one,\nst_simplify() simplifies a sf object,\nst_coordinates() retrieves coordinates of a sf object,\nst_as_sf() converts a foreign object to a sf object.\n\n\n\nWe can delete some of the polygons by taking a subset of the rows of map.\n\n\n# Delete polygon \nmap &lt;- ethR_shape %&gt;%\n  st_filter(ethR_shape[ethR_shape$REG_P_CODE %in% c(\"3\", \"4\"), ])\n\nggplot(map) + geom_sf(aes(fill = HRname))  \n\n\n\n\n\n\n\n\n\n# Combine geometries \nggplot(st_union(map, by_feature=FALSE) %&gt;% st_sf())+geom_sf() \n# Simplify\nggplot(st_simplify(map, dTolerance =10000))+geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsf object obtained by deleting some of its polygons (top), combining polygons (middle), and simplifying polygons (bottom)."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-point-data-to-an-sf-object",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-point-data-to-an-sf-object",
    "title": "Spatial data analaysis Training",
    "section": "Transforming Point Data to an sf Object",
    "text": "Transforming Point Data to an sf Object\n\nThe st_as_sf() function allows us to convert a foreign object to an sf object.\nThis can be specifying in the argument coords the name of the columns that contain the point coordinates.\nExample: Converting a Data Frame to an sf Object\nHere, we use st_as_sf() to turn a data frame containing coordinates long and lat and two variables place and value into an sf object.\nThen, we use st_crs() to set the coordinate reference system given by the EPSG code 4326 to represent longitude and latitude coordinates.\n\n\n\nlibrary(sf)\nlibrary(mapview)\n\nWarning: package 'mapview' was built under R version 4.4.1\n\nd &lt;- data.frame(\n  place = c(\"Ethiopia\", \"Kenya\", \"Somalia\", \"Sudan\"),\n  long = c(39.9559, 37.9062, 45.0792, 30.2176),\n  lat = c(9.145, -1.286389, 2.046934, 19.6133),\n  value = c(200, 150, 100, 300))\nclass(d)\n\n[1] \"data.frame\"\n\ndsf &lt;- st_as_sf(d, coords = c(\"long\", \"lat\"))\nst_crs(dsf) &lt;- 4326\nclass(dsf)\n\n[1] \"sf\"         \"data.frame\"\n\nmapview(dsf)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#counting-the-number-of-points-within-polygons",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#counting-the-number-of-points-within-polygons",
    "title": "Spatial data analaysis Training",
    "section": "Counting the Number of Points within Polygons",
    "text": "Counting the Number of Points within Polygons\n\nUsing st_intersects()\n\nWe can use the st_intersects() function of sf to count the number of points within the polygons of an sf object.\nThe returned object is a list with feature ids intersected in each of the polygons.\nWe can use the lengths() function to calculate the number of points inside each feature.\n\n\n\nExample: Counting Points within Polygons\nIn this example, we create a map with divisions (an sf object) and generate random points over the map.\n\nWe then count the number of points within each polygon using st_intersects() and visualize the results with ggplot2.\n\nlibrary(sf)\nlibrary(ggplot2)\n# Points over map (simple feature geometry list-column sfc)\npoints &lt;- st_sample(map, size = 100)\n# Map of points within polygons\nggplot() + geom_sf(data = map) + geom_sf(data = points)\n\n\n\n\n\n\n\n# Intersection (first argument map, then points)\ninter &lt;- st_intersects(map, points)\n# Add point count to each polygon\nmap$count &lt;- lengths(inter)\n# Map of number of points within polygons\nggplot(map) + geom_sf(aes(fill = count))"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#identifying-polygons-containing-points",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#identifying-polygons-containing-points",
    "title": "Spatial data analaysis Training",
    "section": "Identifying Polygons Containing Points",
    "text": "Identifying Polygons Containing Points\n\nUsing st_intersects()\n\nGiven an sf object with points and an sf object with polygons, we can use the st_intersects() function to obtain the polygon each of the points belongs to.\n\n\n\nExample: Identifying Polygons Containing Points\n\nIn this example, we create a map with divisions (an sf object) and generate three random points over the map.\nWe then identify which polygons contain these points and add the polygon names to the points data.\n\n\nlibrary(sf)\nlibrary(ggplot2)\n\n# Points over map (sf object)\npoints &lt;- st_sample(map, size = 3) %&gt;% st_as_sf()\ninter &lt;- st_intersects(points, map)# first points, then map\npoints$areaname &lt;- map[unlist(inter), \"HRname\", drop = TRUE] # drop geometry\npoints\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 123931.4 ymin: 623208.4 xmax: 268816.9 ymax: 722373.9\nProjected CRS: Adindan / UTM zone 37N\n                          x areaname\n1 POINT (123931.4 708206.9)    SNNPR\n2 POINT (268816.9 722373.9)    SNNPR\n3 POINT (167254.7 623208.4)    SNNPR\n\n# Map\nggplot(map) + geom_sf() + geom_sf(data = points) + \n geom_sf_label(data = map[unlist(inter), ], aes(label = HRname), nudge_y = 0.2)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#joining-map-and-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#joining-map-and-data",
    "title": "Spatial data analaysis Training",
    "section": "Joining Map and Data",
    "text": "Joining Map and Data\n\nUsing left_join()\n\nSometimes, a map and its corresponding data are available separately and we may wish to create an sf object representing the map with the added data that we can manipulate and plot.\nWe can create an sf map with the data attributes by joining the map and the data with the left_join() function of the dplyr package.\n\n\n\nExample: Adding Air Pollution Data to a World Map\nFirst, we use the ne_countries() function of rnaturalearth to download the world map with the country polygons of class sf.\n\nlibrary(rnaturalearth)\n\nWarning: package 'rnaturalearth' was built under R version 4.4.1\n\nmap &lt;- ne_countries(returnclass = \"sf\")\n\n\n\nThen, we use the wbstats package to download a data frame of air pollution data from the World Bank.\n\nSpecifically, we search the pollution indicators with wb_search(), and use wb_data() to download PM2.5 in the year 2016 by specifying the indicator corresponding to PM2.5, and the start and end dates.\n\n\n\nlibrary(wbstats)\n\nWarning: package 'wbstats' was built under R version 4.4.1\n\nindicators &lt;- wb_search(pattern = \"pollution\")\n#d &lt;- wb_data(indicator = \"EN.ATM.PM25.MC.M3\", start_date = 2016, end_date = 2016)\n\n\n\nNext, we use the left_join() function of dplyr to join the map and the data, specifying the argument by with the variables we wish to join by. Here, we use the ISO3 standard code of the countries rather than the country names, since names can be written differently in the map and the data frame.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(viridis)\n\nmap1 &lt;- left_join(map, d, by = c(\"iso_a3\" = \"iso3c\"))\nggplot(map1) + geom_sf(aes(fill = EN.ATM.PM25.MC.M3)) +\n  scale_fill_viridis() + labs(fill = \"PM2.5\") + theme_bw()\n\n\n\n\nNote\nWhen we use left_join(), the class of the resulting object is the same as the class of the first argument.\n\nmap1 &lt;- left_join(map, d, by = c(\"iso_a3\" = \"iso3c\"))\nclass(map1)\n# [1] \"sf\" \"data.frame\"\n\nd1 &lt;- left_join(d, map, by = c(\"iso3c\" = \"iso_a3\"))\nclass(d1)\n# [1] \"data.frame\""
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#raster-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#raster-data",
    "title": "Spatial data analaysis Training",
    "section": "Raster Data",
    "text": "Raster Data\n\nRaster Data: A spatial data structure that divides the study region into equal-sized rectangles called cells or pixels, storing one or more values for each cell.\nUses: Represent spatially continuous phenomena such as elevation, temperature, or air pollution values.\nMain Packages:\n\nterra: Primary package for working with raster data. Also supports vector data.\nraster: Previously used for raster data; terra is faster and has more functionality.\nstars: Used for analyzing raster data and spatial data cubes (arrays with spatial dimensions)."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#geotiff",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#geotiff",
    "title": "Spatial data analaysis Training",
    "section": "GeoTIFF",
    "text": "GeoTIFF\n\nGeoTIFF: A common format for raster data, with the extension .tif.\nExample: Reading the elev.tif file from the terra package, representing elevation in Luxembourg.\n\n\n# Load the terra package \nlibrary(terra)\n\nWarning: package 'terra' was built under R version 4.4.1\n\n\nterra 1.7.78\n\npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\")\nr &lt;- terra::rast(pathraster)\nprint(r)\n\nclass       : SpatRaster \ndimensions  : 90, 95, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : 5.741667, 6.533333, 49.44167, 50.19167  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : elev.tif \nname        : elevation \nmin value   :       141 \nmax value   :       547 \n\n\n\n\n# Plot the raster data\nplot(r)\n\n\n\n\n\n\n\n\n\n\nEPSG Codes and CRS Transformation\n\nEPSG Codes\n\nMost common CRSs can be specified by providing their EPSG (European Petroleum Survey Group) codes or their Proj4 strings.\n\nDetails of a given projection can be inspected using the st_crs() function of the sf package.\n\nExample: EPSG Code 4326\nEPSG code 4326 refers to the WGS84 longitude/latitude projection.\n\n\nst_crs(\"EPSG:4326\")$Name\n\n[1] \"WGS 84\"\n\nst_crs(\"EPSG:4326\")$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nst_crs(\"EPSG:4326\")$epsg\n\n[1] 4326"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-crs-with-sf-and-terra",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-crs-with-sf-and-terra",
    "title": "Spatial data analaysis Training",
    "section": "Transforming CRS with sf and terra",
    "text": "Transforming CRS with sf and terra\n\nFunctions sf::st_crs() and terra::crs() allow us to get the CRS of spatial data.\nThese functions also allow us to set a CRS to spatial data by using st_crs(x) &lt;- value if x is a sf object, and crs(r) &lt;- value if r is a raster.\n\n\nImportant Note\n\nSetting a CRS does not transform the data; it just changes the CRS label.\nWe may want to set a CRS to data that does not come with CRS, and the CRS should be what it is, not what we would like it to be.\nWe use sf::st_transform() and terra::project() to transform the sf or raster data, respectively, to a new CRS."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#working-with-sf-package",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#working-with-sf-package",
    "title": "Spatial data analaysis Training",
    "section": "Working with sf Package",
    "text": "Working with sf Package\n\nReading and Getting the CRS\n\nlibrary(sf)\npathshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\nmap &lt;- st_read(pathshp, quiet = TRUE)\n# Get CRS\nst_crs(map)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-the-crs",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-the-crs",
    "title": "Spatial data analaysis Training",
    "section": "Transforming the CRS",
    "text": "Transforming the CRS\n\n# Transform CRS\nmap2 &lt;- st_transform(map, crs = \"EPSG:4326\")\n\n# Get CRS of transformed data\nst_crs(map2)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#working-with-terra-package",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#working-with-terra-package",
    "title": "Spatial data analaysis Training",
    "section": "Working with terra Package",
    "text": "Working with terra Package\n\nReading and Getting the CRS of a Raster\n\nlibrary(terra)\npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\")\nr &lt;- rast(pathraster)\n# Get CRS\ncrs(r)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\""
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-the-crs-of-a-raster",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#transforming-the-crs-of-a-raster",
    "title": "Spatial data analaysis Training",
    "section": "Transforming the CRS of a Raster",
    "text": "Transforming the CRS of a Raster\n\n# Transform CRS\nr2 &lt;- terra::project(r, \"EPSG:2169\")\n# Get CRS of transformed data\ncrs(r2)\n\n[1] \"PROJCRS[\\\"LUREF / Luxembourg TM\\\",\\n    BASEGEOGCRS[\\\"LUREF\\\",\\n        DATUM[\\\"Luxembourg Reference Frame\\\",\\n            ELLIPSOID[\\\"International 1924\\\",6378388,297,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4181]],\\n    CONVERSION[\\\"Luxembourg TM\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",49.8333333333333,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",6.16666666666667,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",1,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",80000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",100000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (X)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (Y)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Luxembourg.\\\"],\\n        BBOX[49.44,5.73,50.19,6.53]],\\n    ID[\\\"EPSG\\\",2169]]\""
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#the-terra-package-for-raster-and-vector-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#the-terra-package-for-raster-and-vector-data",
    "title": "Spatial data analaysis Training",
    "section": "The terra Package for Raster and Vector Data",
    "text": "The terra Package for Raster and Vector Data\nThe terra package (Hijmans 2022) provides functions to create, read, manipulate, and write raster and vector data. Raster data represents spatially continuous phenomena through a grid of equally sized cells, while vector data includes points, lines, and polygons with associated attributes. This chapter demonstrates how to handle raster and vector data using terra."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#raster-data-1",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#raster-data-1",
    "title": "Spatial data analaysis Training",
    "section": "Raster Data",
    "text": "Raster Data\n\nCreating and Reading Raster Data\nThe rast() function can create and read raster data. The writeRaster() function allows writing raster data. Here, we read elevation data for Luxembourg from a file provided by terra.\n\nlibrary(terra) \npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\") \nr &lt;- rast(pathraster)\nplot(r)\n\n\n\n\n\n\n\n\n\nWe can also create a SpatRaster object by specifying dimensions and extents.\n\n\nRaster Operations\nSeveral functions provide information about raster size and dimensions.\n\nnrow(r) # number of rows\n\n[1] 90\n\nncol(r) # number of columns\n\n[1] 95\n\ndim(r) # dimensions\n\n[1] 90 95  1\n\nncell(r) # number of cells\n\n[1] 8550\n\n\nThe values() function sets and accesses raster values.\n\nvalues(r) &lt;- 1:ncell(r)\n\nCreating multilayer rasters and subsetting layers is straightforward.\n\nr2 &lt;- r * r\ns &lt;- c(r, r2)\nplot(s[[2]]) # layer 2\n\n\n\n\n\n\n\n\n\nGeneric operations on rasters include:\n\nplot(min(s))\n\n\n\n\n\n\n\nplot(r + r + 10)\n\n\n\n\n\n\n\nplot(round(r))\n\n\n\n\n\n\n\nplot(r == 1)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#vector-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#vector-data",
    "title": "Spatial data analaysis Training",
    "section": "Vector Data",
    "text": "Vector Data\nThe SpatVector class handles vector data with attributes. The vect() function reads shapefiles, and writeVector() writes SpatVector objects. Here, we obtain a map of Luxembourg‚Äôs divisions.\n\npathshp &lt;- system.file(\"ex/lux.shp\", package = \"terra\")\nv &lt;- vect(pathshp)\n\n\nCreating a SpatVector with point locations and attributes:\n\nlong &lt;- c(-0.118092, 2.349014, -3.703339, 12.496366)\nlat &lt;- c(51.509865, 48.864716, 40.416729, 41.902782)\nlonglat &lt;- cbind(long, lat)\n\ncrspoints &lt;- \"+proj=longlat +datum=WGS84\"\nd &lt;- data.frame(place = c(\"London\", \"Paris\", \"Madrid\", \"Rome\"), value = c(200, 300, 400, 500))\npts &lt;- vect(longlat, atts = d, crs = crspoints)\nplot(pts)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#cropping-masking-and-aggregating-raster-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#cropping-masking-and-aggregating-raster-data",
    "title": "Spatial data analaysis Training",
    "section": "Cropping, Masking, and Aggregating Raster Data",
    "text": "Cropping, Masking, and Aggregating Raster Data\nThe terra package provides functions to crop, mask, and aggregate raster data. Here, we demonstrate these operations with temperature data for Spain.\n\nDownloading Data\n\nlibrary(terra)\nr &lt;- geodata::worldclim_country(country = \"Spain\", var = \"tavg\", res = 10, path = tempdir())\nplot(r)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#averaging-temperature-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#averaging-temperature-data",
    "title": "Spatial data analaysis Training",
    "section": "Averaging Temperature Data",
    "text": "Averaging Temperature Data\n\nr &lt;- mean(r)\nplot(r)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#cropping-and-masking-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#cropping-and-masking-data",
    "title": "Spatial data analaysis Training",
    "section": "Cropping and Masking Data",
    "text": "Cropping and Masking Data\n\nlibrary(ggplot2)\nlibrary(terra)\nmap &lt;- rnaturalearth::ne_states(\"Spain\", returnclass = \"sf\")\nmap &lt;- map[-which(map$region == \"Canary Is.\"), ] # delete region\nggplot(map) + geom_sf()"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#aggregating-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#aggregating-data",
    "title": "Spatial data analaysis Training",
    "section": "Aggregating Data",
    "text": "Aggregating Data\n\nr &lt;- terra::aggregate(r, fact = 20, fun = \"mean\", na.rm = TRUE)\nplot(r)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#extracting-raster-values-at-points",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#extracting-raster-values-at-points",
    "title": "Spatial data analaysis Training",
    "section": "Extracting Raster Values at Points",
    "text": "Extracting Raster Values at Points\nThe extract() function retrieves raster values at specified points.\n\nExample\n\nlibrary(terra)\nr &lt;- rast(system.file(\"ex/elev.tif\", package = \"terra\"))\nv &lt;- vect(system.file(\"ex/lux.shp\", package = \"terra\"))\n\npoints &lt;- crds(centroids(v))\n\nplot(r)\nplot(v, add = TRUE)\npoints(points)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#extracting-values",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#extracting-values",
    "title": "Spatial data analaysis Training",
    "section": "Extracting Values",
    "text": "Extracting Values\n\npoints &lt;- as.data.frame(points)\nvaluesatpoints &lt;- extract(r, points)\ncbind(points, valuesatpoints)\n\n          x        y ID elevation\n1  6.009082 50.07064  1       444\n2  6.127425 49.86614  2       295\n3  5.886502 49.80014  3       382\n4  6.165081 49.92886  4       404\n5  5.914545 49.93892  5       414\n6  6.378449 49.78511  6       320\n7  6.311601 49.54569  7       193\n8  6.346395 49.68742  8       228\n9  5.963503 49.64159  9       313\n10 6.023816 49.52331 10       282\n11 6.167624 49.61815 11       328\n12 6.113598 49.75744 12       221"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#extracting-and-averaging-raster-values",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#extracting-and-averaging-raster-values",
    "title": "Spatial data analaysis Training",
    "section": "Extracting and Averaging Raster Values",
    "text": "Extracting and Averaging Raster Values\nExtracting raster values within polygons and computing area-weighted averages.\n\nExtracting Values Within Polygons\n\n# Extracted raster cells within each polygon\nhead(extract(r, v, na.rm = TRUE))\n\n  ID elevation\n1  1       547\n2  1       485\n3  1       497\n4  1       515\n5  1       515\n6  1       515\n\n# Extracted raster cells and percentage of area\n# covered within each polygon\nhead(extract(r, v, na.rm = TRUE, weights = TRUE))\n\n  ID elevation     weight\n1  1        NA 0.04545454\n2  1        NA 0.10909091\n3  1       529 0.24545454\n4  1       542 0.46363635\n5  1       547 0.68181816\n6  1       535 0.11818181"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#averaging-values",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#averaging-values",
    "title": "Spatial data analaysis Training",
    "section": "Averaging Values",
    "text": "Averaging Values\n\n# Average raster values by polygon\nv$avg &lt;- extract(r, v, mean, na.rm = TRUE)$elevation\n\n# Area-weighted average raster values by polygon (weights = TRUE)\nv$weightedavg &lt;- extract(r, v, mean, na.rm = TRUE, weights = TRUE)$elevation\n\nlibrary(ggplot2)\nlibrary(tidyterra)\n\nWarning: package 'tidyterra' was built under R version 4.4.1\n\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n# Plot average raster values within polygons\nggplot(data = v) + geom_spatvector(aes(fill = avg)) + scale_fill_terrain_c()\n\n\n\n\n\n\n\n# Plot area-weighted average raster values within polygons\nggplot(data = v) + geom_spatvector(aes(fill = weightedavg)) + scale_fill_terrain_c()"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#introduction-to-mapping-in-r",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#introduction-to-mapping-in-r",
    "title": "Spatial data analaysis Training",
    "section": "Introduction to Mapping in R",
    "text": "Introduction to Mapping in R\n\nImportance of Maps\n\nMaps are essential tools for visualizing spatial data.\nThey help to reveal patterns and trends that are not immediately apparent in tabular data.\nUseful in various fields such as geography, urban planning, epidemiology, and environmental science.\n\n\n\nPackages Covered\n\nggplot2: A powerful package for creating static maps based on the grammar of graphics.\nleaflet: A package for creating interactive maps using the Leaflet JavaScript library.\nmapview: A package for quick and easy interactive map visualization.\ntmap: A versatile package that can create both static and interactive maps.\nflowmapblue: A package for visualizing mobility flows with interactive maps."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#preparing-areal-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#preparing-areal-data",
    "title": "Spatial data analaysis Training",
    "section": "Preparing Areal Data",
    "text": "Preparing Areal Data\n\nData Source\n\nThe data used is from a study on sudden infant deaths in North Carolina, USA, for the years 1974 and 1979.\n\n\n\nLoading Data\n\nUse the sf package to load shapefile data.\n\n\nlibrary(sf)\nnameshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\nd &lt;- st_read(nameshp, quiet = TRUE)\nd$vble &lt;- d$SID74\nd$vble2 &lt;- d$SID79"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#static-maps-with-ggplot2",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#static-maps-with-ggplot2",
    "title": "Spatial data analaysis Training",
    "section": "Static Maps with ggplot2",
    "text": "Static Maps with ggplot2\n\nGrammar of Graphics\n\nggplot2 is based on the ‚Äúgrammar of graphics‚Äù, which provides a structured way to describe and build graphs.\nYou define aesthetics (like colors and shapes) and layers (like points, lines, and polygons).\n\n\n\nCreating a Map\n\nFirst, load the required libraries.\nUse geom_sf() to plot the spatial data.\nCustomize the map with color scales and themes.\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nggplot(d) + geom_sf(aes(fill = vble)) +\n  scale_fill_viridis() + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nSaving Plots\n\nSave your plots to a file using ggsave().\nInteractive Maps with leaflet\nCreating a Leaflet Map\n\nLeaflet maps are interactive and require transforming the coordinate reference system (CRS) to EPSG:4326.\n\n\n\nd &lt;- st_transform(d, 4326)\n\n\n\nDefine Color Palette and Create Map\n\nDefine a color palette using colorNumeric().\nCreate the map and add polygons with addPolygons().\nAdd a legend with addLegend().\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.4.1\n\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = d$vble)\nleaflet(d) %&gt;% addTiles() %&gt;%\n  addPolygons(color = \"white\", fillColor = ~ pal(vble),\n              fillOpacity = 0.8) %&gt;%\n  addLegend(pal = pal, values = ~vble, opacity = 0.8)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#quick-maps-with-mapview",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#quick-maps-with-mapview",
    "title": "Spatial data analaysis Training",
    "section": "Quick Maps with mapview",
    "text": "Quick Maps with mapview\n\nCreating a Map\n\nUse mapview() for quick and simple interactive maps.\n\n\nlibrary(mapview)\nmapview(d, zcol = \"vble\")\n\n\n\n\n\n\n\nCustomizing Maps\n\nCustomize the map by changing the background, color palette, and legend title.\n\n\nlibrary(RColorBrewer)\npalette &lt;- colorRampPalette(brewer.pal(9, \"YlOrRd\"))\n\nmapview(d, zcol = \"vble\", map.types = \"CartoDB.DarkMatter\",\n        col.regions = colorRampPalette(brewer.pal(9, \"YlOrRd\"))(), \n        layer.name = \"SDI\")"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#side-by-side-and-synchronized-maps",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#side-by-side-and-synchronized-maps",
    "title": "Spatial data analaysis Training",
    "section": "Side-by-Side and Synchronized Maps",
    "text": "Side-by-Side and Synchronized Maps\n\nSide-by-Side Maps\n\nDisplay multiple maps side-by-side for comparison.\n\n\nlibrary(leaflet.extras2)\n#m1 | m2\n\n\n\n\nSynchronized Maps\n\nSync multiple maps to pan and zoom together using leafsync.\n\n\nm &lt;- leafsync::sync(m1, m2)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#static-and-interactive-maps-with-tmap",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#static-and-interactive-maps-with-tmap",
    "title": "Spatial data analaysis Training",
    "section": "Static and Interactive Maps with tmap",
    "text": "Static and Interactive Maps with tmap\n\nStatic Map\n\nCreate static maps with tmap using tm_shape() and tm_polygons().\n\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.4.1\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(d) + tm_polygons(\"vble\")\n\n\n\n\n\n\n\n\n\n\n\nInteractive Map\n\nSwitch to interactive mode with tmap_mode(\"view\") and create maps.\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(d) + tm_polygons(\"vble\")"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#mapping-point-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#mapping-point-data",
    "title": "Spatial data analaysis Training",
    "section": "Mapping Point Data",
    "text": "Mapping Point Data\n\nCreating Point Data Map with ggplot2\n\nPlot point data with ggplot2, using geom_sf() to define aesthetics for color and size.\n\n\nggplot(d) + geom_sf(aes(col = vble, size = size)) +\n  scale_color_viridis()"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#mapping-raster-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#mapping-raster-data",
    "title": "Spatial data analaysis Training",
    "section": "Mapping Raster Data",
    "text": "Mapping Raster Data\n\nCreating Raster Data Map with ggplot2\n\nPlot raster data by converting it to a data frame and using geom_raster().\n\n\nggplot(d) + geom_sf() +\n  geom_raster(data = as.data.frame(r, xy = TRUE),\n    aes(x = x, y = y, fill = elevation))"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#mapping-mobility-flows-with-flowmapblue",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#mapping-mobility-flows-with-flowmapblue",
    "title": "Spatial data analaysis Training",
    "section": "Mapping Mobility Flows with flowmapblue",
    "text": "Mapping Mobility Flows with flowmapblue\n\nCreating Interactive Flow Map\n\nCreate interactive flow maps with flowmapblue.\nDefine locations and flows data frames.\nGenerate the map with clustering, dark mode, and animation options.\n\n\nlibrary(flowmapblue)\nlocations &lt;- data.frame(\n  id = c(1, 2, 3),\n  name = c(\"New York\", \"London\", \"Rio de Janeiro\"),\n  lat = c(40.713543, 51.507425, -22.906241),\n  lon = c(-74.011219, -0.127738, -43.180244))\nflows &lt;- data.frame(\n  origin = c(1, 2, 3, 2, 1, 3),\n  dest = c(2, 1, 1, 3, 3 , 2),\n  count = c(42, 51, 50, 40, 22, 42))\nflowmapblue(locations, flows, mapboxAccessToken,\n            clustering = TRUE, darkMode = TRUE, animation = FALSE)"
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#r-packages-to-download-open-spatial-data",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#r-packages-to-download-open-spatial-data",
    "title": "Spatial data analaysis Training",
    "section": "R Packages to Download Open Spatial Data",
    "text": "R Packages to Download Open Spatial Data\nSpatial data are used in a wide range of disciplines including environment, health, agriculture, economy, and society (Moraga and Baker 2022). Several R packages have been recently developed as clients for various databases that can be used for easy access to spatial data including administrative boundaries, climatic, and OpenStreetMap data. Here, we give short reproducible examples on how to download and visualize spatial data that can be useful in different settings. More extended examples and details about the capabilities of each of the packages can be seen at the packages‚Äô websites and the rspatialdata website which provides a collection of tutorials on R packages to download and visualize spatial data using R."
  },
  {
    "objectID": "static/slides/Spatail-Data-Slide/Spatial.html#administrative-boundaries-of-countries",
    "href": "static/slides/Spatail-Data-Slide/Spatial.html#administrative-boundaries-of-countries",
    "title": "Spatial data analaysis Training",
    "section": "Administrative Boundaries of Countries",
    "text": "Administrative Boundaries of Countries\n\nOverview\n\nAdministrative boundaries data can be obtained from multiple packages:\n\nrnaturalearth: Global administrative boundaries.\ntidycensus and tigris: USA-specific data.\nmapSpain: Spain-specific data.\ngeobr: Brazil-specific data.\ngiscoR: Eurostat GISCO data."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "What is spatial data and why should we use it?\nTypes of spatial data\nSpatial data in R: how to store, load, and tidy spatial data\nVisualizing spatial data\nExploring spatial data: Spatial authorization and spatial statistics to quantify relationships"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#section",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#section",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "What is spatial data and why should we use it?\nTypes of spatial data\nSpatial data in R: how to store, load, and tidy spatial data\nVisualizing spatial data\nExploring spatial data: Spatial authorization and spatial statistics to quantify relationships"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#what-is-spatial-data-and-why-should-we-use-it",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#what-is-spatial-data-and-why-should-we-use-it",
    "title": "Spatial data analaysis Training",
    "section": "What is Spatial data and why should we use it?",
    "text": "What is Spatial data and why should we use it?\n\nSpatial data, also known as geospatial data, refers to information that identifies the geographic location and characteristics of natural or constructed features and boundaries on the Earth.\nThis data is often represented in terms of Cartesian coordinates (x,y) for two-dimensional maps, but may also include altitude (z) for a three-dimensional representation.\nSpatial data can come in various forms including points, lines, and polygons.\nThe concept of spatial data is integral to a variety of applications that require an understanding of how different elements relate to each other within a geographical space.\nThis data can be collected through various means, including but not limited to, satellite imagery, aerial photography, and ground-based surveys."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#spatial-data-formats",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#spatial-data-formats",
    "title": "Spatial data analaysis Training",
    "section": "Spatial Data Formats",
    "text": "Spatial Data Formats\n\nLet us see a basic way to represent the spatial data.\nBut there is a variety of data formats to represent the data to suit different applications.\nIn most cases, spatial data formats are an extension of existing data formats.\n\n\n\n\n\n\n\n\nType\nNon-Spatial Data\nSpatial Data\n\n\n\n\nText\ncsv, json, xml\ncsv, geojson, gml, kml\n\n\nBinary/Compressed\npdf, xls, zip\nshapefile, geopdf, geopackage\n\n\nImages\ntiff, jpg, png\ngeotiff, jpeg2000\n\n\nDatabases\nSQLite, PostgreSQL, Oracle\nSpatialite, PostGIS, Oracle Spatial"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#spatial-data-types",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#spatial-data-types",
    "title": "Spatial data analaysis Training",
    "section": "Spatial Data Types",
    "text": "Spatial Data Types\nSpatial Data can be broadly categorized into 2 types - Vector and Raster.\n\n\n\n\n\n\n\n\nType\nSub Types\nExamples\n\n\n\n\nVector Data\nPoints: Represents features with a single coordinate pair (x, y).\nLocations of ATMs, tree\n\n\n\nLines: Represents linear features as ordered sequences of points.\nRoads, rivers, utility lines\n\n\n\nPolygons: Represents areas enclosed by closed loops of lines.\nBuildings, lakes, zones\n\n\nRaster Data\nGrids: Represents continuous data across a surface.\nSatellite images, digital elevation models (DEMs)\n\n\n\nPixels: Smallest units in a raster dataset, each with a specific value.\nValues representing color, temperature,"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#other-data-associated-with-vector-and-raster",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#other-data-associated-with-vector-and-raster",
    "title": "Spatial data analaysis Training",
    "section": "Other data associated with Vector and Raster",
    "text": "Other data associated with Vector and Raster\n\nAttribute data: Additional information describing the characteristics of spatial features.\nFor example in Vector data:\n\nSchools: have school name, number of students, education level.\nRoads:have road type, traffic volume, maintenance status.\nRegions: have population density, average income, and land use type.\n\n\nIn Raster Data: Each pixel can have multiple attributes, such as vegetation type or pollution levels."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#what-are-the-components-of-spatial-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#what-are-the-components-of-spatial-data",
    "title": "Spatial data analaysis Training",
    "section": "What are the components of spatial data",
    "text": "What are the components of spatial data\n\nGeometry: Refers to the coordinates that define the shape of an object.\n\nCoordinates: These can be in different coordinate systems, such as geographic (latitude) and longitude) or projected systems.\nTypes of Geometries: Common types include points, lines, and polygons.\nDimension: Geometry can also have dimensional attributes like 2D (x, y), 3D (x, y, z), or even 4D (x, y, z, time).\n\nTopology: Refers to the spatial relationships between geometric entities.\n\nSpatial Relationships\n\nAdjacency: How entities are next to each other.\nConnectivity: How entities are connected.\nContainment: How one entity is contained within another.\n\nRules and Constraints: Enforces rules such as:\n\nNo overlapping polygons.\nLines meeting at nodes.\nEnsuring polygon boundaries are closed loops."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#section-1",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#section-1",
    "title": "Spatial data analaysis Training",
    "section": "",
    "text": "Attribute Data: provide detailed information about spatial features.\n\nExample: A point representing a city may have attributes like population, elevation, and city name.\nTypes of Attributes: Attribute data is either,\n\nNumeric (population size),\nCategorical (land use type), or\nTemporal (date of a satellite image).\n\nAttribute data is often stored in tables and linked to geometry through unique identifiers."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#how-does-the-workflow-of-handling-spatial-data-effectively",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#how-does-the-workflow-of-handling-spatial-data-effectively",
    "title": "Spatial data analaysis Training",
    "section": "How does the workflow of handling spatial data effectively",
    "text": "How does the workflow of handling spatial data effectively\nData Collection: Data can be collected in either\n\nSatellite Imagery: Captured by remote sensing satellites, these images can provide data on land cover, vegetation, weather patterns, and more.\nGPS Surveys: Use Global Positioning System technology to collect precise location data for mapping and navigation purposes.\nTraditional Surveying Methods: Involve measuring angles, distances, and elevations to map out areas accurately, often using tools like theodolites and total stations.\n\n\nData Storage:\n\nShapefiles: A popular vector data format that stores geometric locations and associated attribute data.\n\nIt consists of at least three files with extensions .shp (geometry), .shx (shape index), and .dbf (attribute data).\n\nGeoJSON: A format for encoding a variety of geographic data structures using JavaScript Object Notation (JSON).\nGeoTIFF: A raster format that includes spatial metadata embedded within the file, making it suitable for georeferenced raster data like satellite images.\nSpatial Databases: PostgreSQL with PostGIS extensions enables the storage, querying, and manipulation of spatial data within a relational database system, supporting complex spatial operations and large datasets."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#practice-for-spatial-data-in-r",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#practice-for-spatial-data-in-r",
    "title": "Spatial data analaysis Training",
    "section": "Practice for Spatial Data in R",
    "text": "Practice for Spatial Data in R\n\nSpatial Data: Information about the location and shape of geographic features.\nTypes: Vector Data and Raster Data.\n\n\nShapefile\n\nShapefile: A common format for representing vector data.\n\n.shp: Contains geometry data.\n.shx: Positional index of the geometry data.\n.dbf: Stores attributes for each shape.\n\nAdditional Files:\n\n.prj: Describes the projection (plain text).\n.sbn and .sbx: Spatial indices of the geometry data.\n.shp.xml: Contains spatial metadata in XML format."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#the-sf-package-for-spatial-vector-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#the-sf-package-for-spatial-vector-data",
    "title": "Spatial data analaysis Training",
    "section": "The sf package for spatial vector data",
    "text": "The sf package for spatial vector data\n\nThe sf package stores geometric features (termed simple features, hence sf) in a data frame.\nGeographic data is stored in the special geometry column.\nThe sf package is a modern alternative to the traditional sp, rgeos, and rgdal packages.\nSupports a wide range of geometry types: points, lines, polygons, and their ‚Äòmulti‚Äô versions.\nUseful for handling geographic vector data.\n\n\nUnderstanding Shapefiles\n\nShapefiles are a simple non-topological format to store geometric location and attribute information for geographical features.\nThey are commonly used in spatial analyses as a type of map.\nWe can read a shapefile or a sf object with the st_read() function of sf.\nFor example, here we read the Eth_Region_2013.shp shapefile of sf which contains the regions of Ethiopia, with their name, and Region and country code."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#reading-a-shapefile-with-sf",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#reading-a-shapefile-with-sf",
    "title": "Spatial data analaysis Training",
    "section": "Reading a Shapefile with sf",
    "text": "Reading a Shapefile with sf\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nethR_shape &lt;- st_read(\"Ethiopia_All/Eth_Region_2013.shp\", \n                      quiet = TRUE)\nclass(ethR_shape)\n\n[1] \"sf\"         \"data.frame\"\n\nhead(ethR_shape, 2)\n\nSimple feature collection with 2 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 460361.8 ymin: 976360 xmax: 864787.5 ymax: 1602973\nProjected CRS: Adindan / UTM zone 37N\n   REGIONNAME REG_P_CODE REG_Pcode      HRname HRpcode HRparent\n1 Addis Ababa         14      ET14 Addis Ababa    ET14       ET\n2        Afar          2      ET02        Afar    ET02       ET\n                        geometry\n1 MULTIPOLYGON (((475625.4 10...\n2 MULTIPOLYGON (((626409.3 16...\n\n\n\n\nThe sf object ethR_shape is a data.frame containing a collection with\n\n[11 simple features (rows)] {style=‚Äúcolor:#ae01c7;‚Äù} and 6 attributes (columns) [plus a list-column with the geometry of each feature] {style=‚Äúcolor:red;‚Äù}.\n\nA sf object contains the following objects of class sf, sfc and sfg:\n\nsf (simple feature): each row of the data.frame is a single simple feature consisting of attributes and geometry.\nsfc (simple feature geometry list-column): the geometry column of the data.frame is a list-column of class sfc with the geometry of each simple feature.\nsfg (simple feature geometry): each of the rows of the sfc list-column corresponds to the simple feature geometry (sfg) of a single simple feature.\n\nThe sf package stores geometric features in a data frame.\nGeographic data is stored in the special geometry column.\n\n\n\nUsing dim() and names() functions we can see how many rows and columns it contains, as well as the names of the columns.\n\n\ndim(ethR_shape)\n\n[1] 11  7\n\nnames(ethR_shape)\n\n[1] \"REGIONNAME\" \"REG_P_CODE\" \"REG_Pcode\"  \"HRname\"     \"HRpcode\"   \n[6] \"HRparent\"   \"geometry\"  \n\n\n\nEach row represents a geographic element, in this case, one of the 11 regions of Ethiopia.\nColumns contain information about the region, including the name and administrative code.\nThe last column is the geometry column."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#displaying-the-first-few-geometries",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#displaying-the-first-few-geometries",
    "title": "Spatial data analaysis Training",
    "section": "Displaying the First Few Geometries",
    "text": "Displaying the First Few Geometries\n\nhead(ethR_shape$geometry)\n\nGeometry set for 6 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -162404.5 ymin: 695805 xmax: 866537.7 ymax: 1602973\nProjected CRS: Adindan / UTM zone 37N\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((475625.4 1005122, 477066.7 1003...\n\n\nMULTIPOLYGON (((626409.3 1602131, 631762.1 1600...\n\n\nMULTIPOLYGON (((232286 1502975, 232875.5 150268...\n\n\nMULTIPOLYGON (((197369 992969.9, 194511.6 99040...\n\n\nMULTIPOLYGON (((858131.4 1069094, 857695.8 1068...\n\n\nEach region is captured as a multipolygon geometry."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#plotting-the-shapefile",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#plotting-the-shapefile",
    "title": "Spatial data analaysis Training",
    "section": "Plotting the Shapefile",
    "text": "Plotting the Shapefile\n\neth_reg &lt;- ethR_shape %&gt;%\n  dplyr::select(region_name = REGIONNAME)\nplot(eth_reg, main = \"Region name\")\n\n\n\n\n\n\n\n\n\nThe default behavior of plot with an sf object is to create a map for each column of data that describes the geography."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#plotting-with-ggplot2",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#plotting-with-ggplot2",
    "title": "Spatial data analaysis Training",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nlibrary(ggplot2)\nregion_map &lt;- ggplot(ethR_shape) +geom_sf(aes(fill=REGIONNAME))+\n  scale_fill_discrete(\"Region\") + coord_sf(datum = NA) +\n  theme_bw() + labs(x = NULL, y = NULL) + \n  ggtitle(\"Map of regions in Ethiopia\")\nprint(region_map)\n\n\n\n\n\n\n\n\n\nggplot2 is flexible once you learn the syntax.\nThe plot shows each region filled with different colors."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#adding-labels-to-the-map",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#adding-labels-to-the-map",
    "title": "Spatial data analaysis Training",
    "section": "Adding Labels to the Map",
    "text": "Adding Labels to the Map\n\nAdding labels to each region ensures they do not overlap.\n\n\nmap_label &lt;- ggplot() + geom_sf(data = ethR_shape) +\n  ggrepel::geom_label_repel(data = ethR_shape,\n    aes(label = REGIONNAME, geometry = geometry),\n    stat = \"sf_coordinates\",min.segment.length = 0) + \n  labs(x = NULL, y = NULL)+ coord_sf(datum = NA) +  theme_bw()\nprint(map_label)\n\n\n\n\n\n\n\n\n\nThe st_geometry() function can be used to retrieve the simple feature geometry list-column (sfc).\n\n\n\n# Geometries printed in abbreviated form \nst_geometry(ethR_shape) # View complete geometry \n\nGeometry set for 11 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -162404.5 ymin: 375657.9 xmax: 1491092 ymax: 1641360\nProjected CRS: Adindan / UTM zone 37N\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((475625.4 1005122, 477066.7 1003...\n\n\nMULTIPOLYGON (((626409.3 1602131, 631762.1 1600...\n\n\nMULTIPOLYGON (((232286 1502975, 232875.5 150268...\n\n\nMULTIPOLYGON (((197369 992969.9, 194511.6 99040...\n\n\nMULTIPOLYGON (((858131.4 1069094, 857695.8 1068...\n\nst_geometry(ethR_shape)[[1]]\n\nMULTIPOLYGON (((475625.4 1005122, 477066.7 1003356, 478017.5 1003453, 478585 1003869, 479491.6 1003613, 480739.7 1003156, 482699.6 1003456, 483464.2 1003407, 484258.3 1003998, 485518.6 1004151, 486408.7 1003783, 486715 1003114, 487027.6 1002343, 487139.3 1001449, 487135.5 1000524, 487506.8 999939.2, 488278 998878.8, 488642.2 998134.8, 489136.4 996928.2, 489558.3 995813.2, 489673.7 994490.2, 489485.1 993896.1, 489546.6 993007, 489694 992266.7, 489457.5 991396, 489313.4 990607.8, 488593.1 989770.7, 489127.6 988809.2, 488650.2 987801.8, 488063.6 987504, 487036.5 987710.5, 485652.9 988089.4, 484765.3 988427.7, 484120.5 987943.3, 484700.5 987267, 485495.9 986762.8, 486091.8 986335.4, 486746.3 985733.7, 486497.3 985094, 486071 984272.6, 486530.5 983243.5, 486081.7 982415.1, 485687.7 981584.2, 485564.9 980728.7, 485025.6 981813.9, 484429.6 981901.2, 483835.1 981696.4, 483185.8 981501.7, 482483.3 981483.4, 481636.3 981822.1, 480868.5 981521.2, 480970.9 980800.3, 481242.7 980260.3, 480982.5 979649.7, 480897.7 979026.3, 480636.8 978489.2, 480628.2 977791.6, 480689.6 977226, 479930.4 977000.9, 479386.9 977081.1, 478611.4 976775.1, 477741.1 976360, 477129.9 976613.7, 476539.2 976559.2, 475873 976630.2, 475630.2 977281.4, 475530 978455, 475038.4 979012.8, 473781 978587.6, 473220.8 978546.6, 472963 979138.5, 472017.8 979673.2, 471983.4 980343.9, 472214.2 981191.4, 471759.8 981551.4, 471905.6 982844.6, 471808.8 984002.6, 471031 983860, 470432.8 984280.5, 470010.1 984916.8, 469583.1 985330.3, 469122.3 985898.2, 467944.5 986031.7, 467234.5 986553.3, 466670.3 987336.4, 466343.1 988352.6, 465689.6 988114.8, 464891.6 988264.6, 464927.5 989212.5, 464407.8 990090.7, 463218.9 990476.1, 461894.6 990678.4, 460736.1 991006, 460361.8 991910.9, 461362.8 992126.6, 462037.3 992230, 462559.4 993165.4, 462385.1 994206.5, 461645 995055.6, 461743.7 995916.8, 462637.8 995811.1, 463402.8 995886.9, 463348.5 996628.2, 463424.9 997678.3, 463959.6 998442, 464016.9 999053, 464475.2 999549.4, 465220.7 1000308, 465239 1001359, 465239 1002108, 465428.1 1002850, 466433.4 1003369, 467107.3 1003676, 467774.2 1003821, 468861.9 1004474, 470011 1004248, 471211 1004375, 472621.9 1004779, 473549 1005371, 474200.2 1005620, 474863.1 1005625, 475625.4 1005122)))"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#creating-a-sf-object",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#creating-a-sf-object",
    "title": "Spatial data analaysis Training",
    "section": "Creating a sf object",
    "text": "Creating a sf object\n\nWe can use the st_sf() function to create a sf object by providing two elements, namely, a data.frame with the attributes of each feature, and a simple feature geometry list-column sfc containing simple feature geometries sfg.\nIn more detail, we create simple feature geometries sfg and use the st_sfc() function to create a simple feature geometry list-column sfc with them.\nThen, we use st_sf() to put the data.frame with the attributes and the simple feature geometry list-column sfc together.\n\nSimple feature geometries sfg objects can be, for example, of type POINT (single point), MULTIPOINT (set of points) or POLYGON (polygon), and can be created with st_point(), st_multipoint() and st_polygon(), respectively.\n\n\nHere, we create a sf object containing two single points, a set of points, and a polygon, with one attribute.\nFirst, we create the simple feature geometry objects (sfg) of type POINT, MULTIPOINT, and POLYGON.\nThen, we use st_sfc() to create a simple feature geometry list-column sfc with the sfg objects.\nFinally, we use st_sf() to put the data.frame with the attribute and the simple feature geometry list-column sfc together.\n\n\n\n# Single point (point as a vector) \np1_sfg &lt;- st_point(c(2, 2)) \np2_sfg &lt;- st_point(c(2.5, 3))  # Set of points \np &lt;- rbind(c(6, 2), c(6.1, 2.6), c(6.8, 2.5), \n           c(6.2, 1.5), c(6.8, 1.8)) \nmp_sfg &lt;- st_multipoint(p)  \n\n\n\n#|ccolumn: true\np1 &lt;- rbind(c(10, 0), c(11, 0), c(13, 2), c(12, 4), \n            c(11, 4), c(10, 0)) \np2 &lt;- rbind(c(11, 1), c(11, 2), c(12, 2), c(11, 1)) \npol_sfg &lt;- st_polygon(list(p1, p2))  # Create sf object \np_sfc &lt;- st_sfc(p1_sfg, p2_sfg, mp_sfg, pol_sfg) \ndf &lt;- data.frame(v1 = c(\"A\", \"B\", \"C\", \"D\"))\n\n# Plot single points, set of points and polygon\np_sf &lt;- st_sf(df, geometry = p_sfc)  \nlibrary(ggplot2) \nggplot(p_sf) + geom_sf(aes(col = v1), size = 3) + \n  theme_bw()"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#st_-functions",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#st_-functions",
    "title": "Spatial data analaysis Training",
    "section": "st_*() functions",
    "text": "st_*() functions\nCommon functions to manipulate sf objects include the following:\n\nst_read() reads a sf object,\nst_write() writes a sf object,\nst_crs() gets or sets a new coordinate reference system (CRS),\nst_transform() transforms data to a new CRS,\nst_intersection() intersects sf objects,\nst_union() combines several sf objects into one,\nst_simplify() simplifies a sf object,\nst_coordinates() retrieves coordinates of a sf object,\nst_as_sf() converts a foreign object to a sf object.\n\n\n\nWe can delete some of the polygons by taking a subset of the rows of map.\n\n\n# Delete polygon \nmap &lt;- ethR_shape %&gt;%\n  st_filter(ethR_shape[ethR_shape$REG_P_CODE %in% c(\"3\", \"4\"), ])\n\nggplot(map) + geom_sf(aes(fill = HRname))  \n\n\n\n\n\n\n\n\n\n# Combine geometries \nggplot(st_union(map, by_feature=FALSE) %&gt;% st_sf())+geom_sf() \n# Simplify\nggplot(st_simplify(map, dTolerance =10000))+geom_sf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsf object obtained by deleting some of its polygons (top), combining polygons (middle), and simplifying polygons (bottom)."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-point-data-to-an-sf-object",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-point-data-to-an-sf-object",
    "title": "Spatial data analaysis Training",
    "section": "Transforming Point Data to an sf Object",
    "text": "Transforming Point Data to an sf Object\n\nThe st_as_sf() function allows us to convert a foreign object to an sf object.\nThis can be specifying in the argument coords the name of the columns that contain the point coordinates.\nExample: Converting a Data Frame to an sf Object\nHere, we use st_as_sf() to turn a data frame containing coordinates long and lat and two variables place and value into an sf object.\nThen, we use st_crs() to set the coordinate reference system given by the EPSG code 4326 to represent longitude and latitude coordinates.\n\n\n\nlibrary(sf)\nlibrary(mapview)\n\nWarning: package 'mapview' was built under R version 4.4.1\n\nd &lt;- data.frame(\n  place = c(\"Ethiopia\", \"Kenya\", \"Somalia\", \"Sudan\"),\n  long = c(39.9559, 37.9062, 45.0792, 30.2176),\n  lat = c(9.145, -1.286389, 2.046934, 19.6133),\n  value = c(200, 150, 100, 300))\nclass(d)\n\n[1] \"data.frame\"\n\ndsf &lt;- st_as_sf(d, coords = c(\"long\", \"lat\"))\nst_crs(dsf) &lt;- 4326\nclass(dsf)\n\n[1] \"sf\"         \"data.frame\"\n\nmapview(dsf)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#counting-the-number-of-points-within-polygons",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#counting-the-number-of-points-within-polygons",
    "title": "Spatial data analaysis Training",
    "section": "Counting the Number of Points within Polygons",
    "text": "Counting the Number of Points within Polygons\n\nUsing st_intersects()\n\nWe can use the st_intersects() function of sf to count the number of points within the polygons of an sf object.\nThe returned object is a list with feature ids intersected in each of the polygons.\nWe can use the lengths() function to calculate the number of points inside each feature.\n\n\n\nExample: Counting Points within Polygons\nIn this example, we create a map with divisions (an sf object) and generate random points over the map.\n\nWe then count the number of points within each polygon using st_intersects() and visualize the results with ggplot2.\n\nlibrary(sf)\nlibrary(ggplot2)\n# Points over map (simple feature geometry list-column sfc)\npoints &lt;- st_sample(map, size = 100)\n# Map of points within polygons\nggplot() + geom_sf(data = map) + geom_sf(data = points)\n\n\n\n\n\n\n\n# Intersection (first argument map, then points)\ninter &lt;- st_intersects(map, points)\n# Add point count to each polygon\nmap$count &lt;- lengths(inter)\n# Map of number of points within polygons\nggplot(map) + geom_sf(aes(fill = count))"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#identifying-polygons-containing-points",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#identifying-polygons-containing-points",
    "title": "Spatial data analaysis Training",
    "section": "Identifying Polygons Containing Points",
    "text": "Identifying Polygons Containing Points\n\nUsing st_intersects()\n\nGiven an sf object with points and an sf object with polygons, we can use the st_intersects() function to obtain the polygon each of the points belongs to.\n\n\n\nExample: Identifying Polygons Containing Points\n\nIn this example, we create a map with divisions (an sf object) and generate three random points over the map.\nWe then identify which polygons contain these points and add the polygon names to the points data.\n\n\nlibrary(sf)\nlibrary(ggplot2)\n\n# Points over map (sf object)\npoints &lt;- st_sample(map, size = 3) %&gt;% st_as_sf()\ninter &lt;- st_intersects(points, map)# first points, then map\npoints$areaname &lt;- map[unlist(inter), \"HRname\", drop = TRUE] # drop geometry\npoints\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 175658 ymin: 1186496 xmax: 912580.8 ymax: 1260013\nProjected CRS: Adindan / UTM zone 37N\n                         x         areaname\n1   POINT (175658 1260013) Beneshangul Gumu\n2 POINT (912580.8 1223935)           Somali\n3 POINT (567991.9 1186496)           Amhara\n\n# Map\nggplot(map) + geom_sf() + geom_sf(data = points) + \n geom_sf_label(data = map[unlist(inter), ], aes(label = HRname), nudge_y = 0.2)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#joining-map-and-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#joining-map-and-data",
    "title": "Spatial data analaysis Training",
    "section": "Joining Map and Data",
    "text": "Joining Map and Data\n\nUsing left_join()\n\nSometimes, a map and its corresponding data are available separately and we may wish to create an sf object representing the map with the added data that we can manipulate and plot.\nWe can create an sf map with the data attributes by joining the map and the data with the left_join() function of the dplyr package.\n\n\n\nExample: Adding Air Pollution Data to a World Map\nFirst, we use the ne_countries() function of rnaturalearth to download the world map with the country polygons of class sf.\n\nlibrary(rnaturalearth)\n\nWarning: package 'rnaturalearth' was built under R version 4.4.1\n\nmap &lt;- ne_countries(returnclass = \"sf\")\n\n\n\nThen, we use the wbstats package to download a data frame of air pollution data from the World Bank.\n\nSpecifically, we search the pollution indicators with wb_search(), and use wb_data() to download PM2.5 in the year 2016 by specifying the indicator corresponding to PM2.5, and the start and end dates.\n\n\n\nlibrary(wbstats)\n\nWarning: package 'wbstats' was built under R version 4.4.1\n\nindicators &lt;- wb_search(pattern = \"pollution\")\n#d &lt;- wb_data(indicator = \"EN.ATM.PM25.MC.M3\", start_date = 2016, end_date = 2016)\n\n\n\nNext, we use the left_join() function of dplyr to join the map and the data, specifying the argument by with the variables we wish to join by. Here, we use the ISO3 standard code of the countries rather than the country names, since names can be written differently in the map and the data frame.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(viridis)\n\nmap1 &lt;- left_join(map, d, by = c(\"iso_a3\" = \"iso3c\"))\nggplot(map1) + geom_sf(aes(fill = EN.ATM.PM25.MC.M3)) +\n  scale_fill_viridis() + labs(fill = \"PM2.5\") + theme_bw()\n\n\n\n\nNote\nWhen we use left_join(), the class of the resulting object is the same as the class of the first argument.\n\nmap1 &lt;- left_join(map, d, by = c(\"iso_a3\" = \"iso3c\"))\nclass(map1)\n# [1] \"sf\" \"data.frame\"\n\nd1 &lt;- left_join(d, map, by = c(\"iso3c\" = \"iso_a3\"))\nclass(d1)\n# [1] \"data.frame\""
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#raster-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#raster-data",
    "title": "Spatial data analaysis Training",
    "section": "Raster Data",
    "text": "Raster Data\n\nRaster Data: A spatial data structure that divides the study region into equal-sized rectangles called cells or pixels, storing one or more values for each cell.\nUses: Represent spatially continuous phenomena such as elevation, temperature, or air pollution values.\nMain Packages:\n\nterra: Primary package for working with raster data. Also supports vector data.\nraster: Previously used for raster data; terra is faster and has more functionality.\nstars: Used for analyzing raster data and spatial data cubes (arrays with spatial dimensions)."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#geotiff",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#geotiff",
    "title": "Spatial data analaysis Training",
    "section": "GeoTIFF",
    "text": "GeoTIFF\n\nGeoTIFF: A common format for raster data, with the extension .tif.\nExample: Reading the elev.tif file from the terra package, representing elevation in Luxembourg.\n\n\n# Load the terra package \nlibrary(terra)\n\nWarning: package 'terra' was built under R version 4.4.1\n\n\nterra 1.7.78\n\npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\")\nr &lt;- terra::rast(pathraster)\nprint(r)\n\nclass       : SpatRaster \ndimensions  : 90, 95, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : 5.741667, 6.533333, 49.44167, 50.19167  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : elev.tif \nname        : elevation \nmin value   :       141 \nmax value   :       547 \n\n\n\n\n# Plot the raster data\nplot(r)\n\n\n\n\n\n\n\n\n\n\nEPSG Codes and CRS Transformation\n\nEPSG Codes\n\nMost common CRSs can be specified by providing their EPSG (European Petroleum Survey Group) codes or their Proj4 strings.\n\nDetails of a given projection can be inspected using the st_crs() function of the sf package.\n\nExample: EPSG Code 4326\nEPSG code 4326 refers to the WGS84 longitude/latitude projection.\n\n\nst_crs(\"EPSG:4326\")$Name\n\n[1] \"WGS 84\"\n\nst_crs(\"EPSG:4326\")$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nst_crs(\"EPSG:4326\")$epsg\n\n[1] 4326"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-crs-with-sf-and-terra",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-crs-with-sf-and-terra",
    "title": "Spatial data analaysis Training",
    "section": "Transforming CRS with sf and terra",
    "text": "Transforming CRS with sf and terra\n\nFunctions sf::st_crs() and terra::crs() allow us to get the CRS of spatial data.\nThese functions also allow us to set a CRS to spatial data by using st_crs(x) &lt;- value if x is a sf object, and crs(r) &lt;- value if r is a raster.\n\n\nImportant Note\n\nSetting a CRS does not transform the data; it just changes the CRS label.\nWe may want to set a CRS to data that does not come with CRS, and the CRS should be what it is, not what we would like it to be.\nWe use sf::st_transform() and terra::project() to transform the sf or raster data, respectively, to a new CRS."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#working-with-sf-package",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#working-with-sf-package",
    "title": "Spatial data analaysis Training",
    "section": "Working with sf Package",
    "text": "Working with sf Package\n\nReading and Getting the CRS\n\nlibrary(sf)\npathshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\nmap &lt;- st_read(pathshp, quiet = TRUE)\n# Get CRS\nst_crs(map)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-the-crs",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-the-crs",
    "title": "Spatial data analaysis Training",
    "section": "Transforming the CRS",
    "text": "Transforming the CRS\n\n# Transform CRS\nmap2 &lt;- st_transform(map, crs = \"EPSG:4326\")\n\n# Get CRS of transformed data\nst_crs(map2)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#working-with-terra-package",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#working-with-terra-package",
    "title": "Spatial data analaysis Training",
    "section": "Working with terra Package",
    "text": "Working with terra Package\n\nReading and Getting the CRS of a Raster\n\nlibrary(terra)\npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\")\nr &lt;- rast(pathraster)\n# Get CRS\ncrs(r)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\""
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-the-crs-of-a-raster",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#transforming-the-crs-of-a-raster",
    "title": "Spatial data analaysis Training",
    "section": "Transforming the CRS of a Raster",
    "text": "Transforming the CRS of a Raster\n\n# Transform CRS\nr2 &lt;- terra::project(r, \"EPSG:2169\")\n# Get CRS of transformed data\ncrs(r2)\n\n[1] \"PROJCRS[\\\"LUREF / Luxembourg TM\\\",\\n    BASEGEOGCRS[\\\"LUREF\\\",\\n        DATUM[\\\"Luxembourg Reference Frame\\\",\\n            ELLIPSOID[\\\"International 1924\\\",6378388,297,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4181]],\\n    CONVERSION[\\\"Luxembourg TM\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",49.8333333333333,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",6.16666666666667,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",1,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",80000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",100000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"northing (X)\\\",north,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        AXIS[\\\"easting (Y)\\\",east,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n    USAGE[\\n        SCOPE[\\\"Engineering survey, topographic mapping.\\\"],\\n        AREA[\\\"Luxembourg.\\\"],\\n        BBOX[49.44,5.73,50.19,6.53]],\\n    ID[\\\"EPSG\\\",2169]]\""
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#the-terra-package-for-raster-and-vector-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#the-terra-package-for-raster-and-vector-data",
    "title": "Spatial data analaysis Training",
    "section": "The terra Package for Raster and Vector Data",
    "text": "The terra Package for Raster and Vector Data\nThe terra package (Hijmans 2022) provides functions to create, read, manipulate, and write raster and vector data. Raster data represents spatially continuous phenomena through a grid of equally sized cells, while vector data includes points, lines, and polygons with associated attributes. This chapter demonstrates how to handle raster and vector data using terra."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#raster-data-1",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#raster-data-1",
    "title": "Spatial data analaysis Training",
    "section": "Raster Data",
    "text": "Raster Data\n\nCreating and Reading Raster Data\nThe rast() function can create and read raster data. The writeRaster() function allows writing raster data. Here, we read elevation data for Luxembourg from a file provided by terra.\n\nlibrary(terra) \npathraster &lt;- system.file(\"ex/elev.tif\", package = \"terra\") \nr &lt;- rast(pathraster)\nplot(r)\n\n\n\n\n\n\n\n\n\nWe can also create a SpatRaster object by specifying dimensions and extents.\n\n\nRaster Operations\nSeveral functions provide information about raster size and dimensions.\n\nnrow(r) # number of rows\n\n[1] 90\n\nncol(r) # number of columns\n\n[1] 95\n\ndim(r) # dimensions\n\n[1] 90 95  1\n\nncell(r) # number of cells\n\n[1] 8550\n\n\nThe values() function sets and accesses raster values.\n\nvalues(r) &lt;- 1:ncell(r)\n\nCreating multilayer rasters and subsetting layers is straightforward.\n\nr2 &lt;- r * r\ns &lt;- c(r, r2)\nplot(s[[2]]) # layer 2\n\n\n\n\n\n\n\n\n\nGeneric operations on rasters include:\n\nplot(min(s))\n\n\n\n\n\n\n\nplot(r + r + 10)\n\n\n\n\n\n\n\nplot(round(r))\n\n\n\n\n\n\n\nplot(r == 1)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#vector-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#vector-data",
    "title": "Spatial data analaysis Training",
    "section": "Vector Data",
    "text": "Vector Data\nThe SpatVector class handles vector data with attributes. The vect() function reads shapefiles, and writeVector() writes SpatVector objects. Here, we obtain a map of Luxembourg‚Äôs divisions.\n\npathshp &lt;- system.file(\"ex/lux.shp\", package = \"terra\")\nv &lt;- vect(pathshp)\n\n\nCreating a SpatVector with point locations and attributes:\n\nlong &lt;- c(-0.118092, 2.349014, -3.703339, 12.496366)\nlat &lt;- c(51.509865, 48.864716, 40.416729, 41.902782)\nlonglat &lt;- cbind(long, lat)\n\ncrspoints &lt;- \"+proj=longlat +datum=WGS84\"\nd &lt;- data.frame(place = c(\"London\", \"Paris\", \"Madrid\", \"Rome\"), value = c(200, 300, 400, 500))\npts &lt;- vect(longlat, atts = d, crs = crspoints)\nplot(pts)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#cropping-masking-and-aggregating-raster-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#cropping-masking-and-aggregating-raster-data",
    "title": "Spatial data analaysis Training",
    "section": "Cropping, Masking, and Aggregating Raster Data",
    "text": "Cropping, Masking, and Aggregating Raster Data\nThe terra package provides functions to crop, mask, and aggregate raster data. Here, we demonstrate these operations with temperature data for Spain.\n\nDownloading Data\n\nlibrary(terra)\nr &lt;- geodata::worldclim_country(country = \"Spain\", var = \"tavg\", res = 10, path = tempdir())\nplot(r)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#averaging-temperature-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#averaging-temperature-data",
    "title": "Spatial data analaysis Training",
    "section": "Averaging Temperature Data",
    "text": "Averaging Temperature Data\n\nr &lt;- mean(r)\nplot(r)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#cropping-and-masking-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#cropping-and-masking-data",
    "title": "Spatial data analaysis Training",
    "section": "Cropping and Masking Data",
    "text": "Cropping and Masking Data\n\nlibrary(ggplot2)\nlibrary(terra)\nmap &lt;- rnaturalearth::ne_states(\"Spain\", returnclass = \"sf\")\nmap &lt;- map[-which(map$region == \"Canary Is.\"), ] # delete region\nggplot(map) + geom_sf()"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#aggregating-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#aggregating-data",
    "title": "Spatial data analaysis Training",
    "section": "Aggregating Data",
    "text": "Aggregating Data\n\nr &lt;- terra::aggregate(r, fact = 20, fun = \"mean\", na.rm = TRUE)\nplot(r)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#extracting-raster-values-at-points",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#extracting-raster-values-at-points",
    "title": "Spatial data analaysis Training",
    "section": "Extracting Raster Values at Points",
    "text": "Extracting Raster Values at Points\nThe extract() function retrieves raster values at specified points.\n\nExample\n\nlibrary(terra)\nr &lt;- rast(system.file(\"ex/elev.tif\", package = \"terra\"))\nv &lt;- vect(system.file(\"ex/lux.shp\", package = \"terra\"))\n\npoints &lt;- crds(centroids(v))\n\nplot(r)\nplot(v, add = TRUE)\npoints(points)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#extracting-values",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#extracting-values",
    "title": "Spatial data analaysis Training",
    "section": "Extracting Values",
    "text": "Extracting Values\n\npoints &lt;- as.data.frame(points)\nvaluesatpoints &lt;- extract(r, points)\ncbind(points, valuesatpoints)\n\n          x        y ID elevation\n1  6.009082 50.07064  1       444\n2  6.127425 49.86614  2       295\n3  5.886502 49.80014  3       382\n4  6.165081 49.92886  4       404\n5  5.914545 49.93892  5       414\n6  6.378449 49.78511  6       320\n7  6.311601 49.54569  7       193\n8  6.346395 49.68742  8       228\n9  5.963503 49.64159  9       313\n10 6.023816 49.52331 10       282\n11 6.167624 49.61815 11       328\n12 6.113598 49.75744 12       221"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#extracting-and-averaging-raster-values",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#extracting-and-averaging-raster-values",
    "title": "Spatial data analaysis Training",
    "section": "Extracting and Averaging Raster Values",
    "text": "Extracting and Averaging Raster Values\nExtracting raster values within polygons and computing area-weighted averages.\n\nExtracting Values Within Polygons\n\n# Extracted raster cells within each polygon\nhead(extract(r, v, na.rm = TRUE))\n\n  ID elevation\n1  1       547\n2  1       485\n3  1       497\n4  1       515\n5  1       515\n6  1       515\n\n# Extracted raster cells and percentage of area\n# covered within each polygon\nhead(extract(r, v, na.rm = TRUE, weights = TRUE))\n\n  ID elevation     weight\n1  1        NA 0.04545454\n2  1        NA 0.10909091\n3  1       529 0.24545454\n4  1       542 0.46363635\n5  1       547 0.68181816\n6  1       535 0.11818181"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#averaging-values",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#averaging-values",
    "title": "Spatial data analaysis Training",
    "section": "Averaging Values",
    "text": "Averaging Values\n\n# Average raster values by polygon\nv$avg &lt;- extract(r, v, mean, na.rm = TRUE)$elevation\n\n# Area-weighted average raster values by polygon (weights = TRUE)\nv$weightedavg &lt;- extract(r, v, mean, na.rm = TRUE, weights = TRUE)$elevation\n\nlibrary(ggplot2)\nlibrary(tidyterra)\n\nWarning: package 'tidyterra' was built under R version 4.4.1\n\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n# Plot average raster values within polygons\nggplot(data = v) + geom_spatvector(aes(fill = avg)) + scale_fill_terrain_c()\n\n\n\n\n\n\n\n# Plot area-weighted average raster values within polygons\nggplot(data = v) + geom_spatvector(aes(fill = weightedavg)) + scale_fill_terrain_c()"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#introduction-to-mapping-in-r",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#introduction-to-mapping-in-r",
    "title": "Spatial data analaysis Training",
    "section": "Introduction to Mapping in R",
    "text": "Introduction to Mapping in R\n\nImportance of Maps\n\nMaps are essential tools for visualizing spatial data.\nThey help to reveal patterns and trends that are not immediately apparent in tabular data.\nUseful in various fields such as geography, urban planning, epidemiology, and environmental science.\n\n\n\nPackages Covered\n\nggplot2: A powerful package for creating static maps based on the grammar of graphics.\nleaflet: A package for creating interactive maps using the Leaflet JavaScript library.\nmapview: A package for quick and easy interactive map visualization.\ntmap: A versatile package that can create both static and interactive maps.\nflowmapblue: A package for visualizing mobility flows with interactive maps."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#preparing-areal-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#preparing-areal-data",
    "title": "Spatial data analaysis Training",
    "section": "Preparing Areal Data",
    "text": "Preparing Areal Data\n\nData Source\n\nThe data used is from a study on sudden infant deaths in North Carolina, USA, for the years 1974 and 1979.\n\n\n\nLoading Data\n\nUse the sf package to load shapefile data.\n\n\nlibrary(sf)\nnameshp &lt;- system.file(\"shape/nc.shp\", package = \"sf\")\nd &lt;- st_read(nameshp, quiet = TRUE)\nd$vble &lt;- d$SID74\nd$vble2 &lt;- d$SID79"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#static-maps-with-ggplot2",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#static-maps-with-ggplot2",
    "title": "Spatial data analaysis Training",
    "section": "Static Maps with ggplot2",
    "text": "Static Maps with ggplot2\n\nGrammar of Graphics\n\nggplot2 is based on the ‚Äúgrammar of graphics‚Äù, which provides a structured way to describe and build graphs.\nYou define aesthetics (like colors and shapes) and layers (like points, lines, and polygons).\n\n\n\nCreating a Map\n\nFirst, load the required libraries.\nUse geom_sf() to plot the spatial data.\nCustomize the map with color scales and themes.\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nggplot(d) + geom_sf(aes(fill = vble)) +\n  scale_fill_viridis() + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nSaving Plots\n\nSave your plots to a file using ggsave().\nInteractive Maps with leaflet\nCreating a Leaflet Map\n\nLeaflet maps are interactive and require transforming the coordinate reference system (CRS) to EPSG:4326.\n\n\n\nd &lt;- st_transform(d, 4326)\n\n\n\nDefine Color Palette and Create Map\n\nDefine a color palette using colorNumeric().\nCreate the map and add polygons with addPolygons().\nAdd a legend with addLegend().\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.4.1\n\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = d$vble)\nleaflet(d) %&gt;% addTiles() %&gt;%\n  addPolygons(color = \"white\", fillColor = ~ pal(vble),\n              fillOpacity = 0.8) %&gt;%\n  addLegend(pal = pal, values = ~vble, opacity = 0.8)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#quick-maps-with-mapview",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#quick-maps-with-mapview",
    "title": "Spatial data analaysis Training",
    "section": "Quick Maps with mapview",
    "text": "Quick Maps with mapview\n\nCreating a Map\n\nUse mapview() for quick and simple interactive maps.\n\n\nlibrary(mapview)\nmapview(d, zcol = \"vble\")\n\n\n\n\n\n\n\nCustomizing Maps\n\nCustomize the map by changing the background, color palette, and legend title.\n\n\nlibrary(RColorBrewer)\npalette &lt;- colorRampPalette(brewer.pal(9, \"YlOrRd\"))\n\nmapview(d, zcol = \"vble\", map.types = \"CartoDB.DarkMatter\",\n        col.regions = colorRampPalette(brewer.pal(9, \"YlOrRd\"))(), \n        layer.name = \"SDI\")"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#side-by-side-and-synchronized-maps",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#side-by-side-and-synchronized-maps",
    "title": "Spatial data analaysis Training",
    "section": "Side-by-Side and Synchronized Maps",
    "text": "Side-by-Side and Synchronized Maps\n\nSide-by-Side Maps\n\nDisplay multiple maps side-by-side for comparison.\n\n\nlibrary(leaflet.extras2)\n#m1 | m2\n\n\n\n\nSynchronized Maps\n\nSync multiple maps to pan and zoom together using leafsync.\n\n\nm &lt;- leafsync::sync(m1, m2)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#static-and-interactive-maps-with-tmap",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#static-and-interactive-maps-with-tmap",
    "title": "Spatial data analaysis Training",
    "section": "Static and Interactive Maps with tmap",
    "text": "Static and Interactive Maps with tmap\n\nStatic Map\n\nCreate static maps with tmap using tm_shape() and tm_polygons().\n\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.4.1\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(d) + tm_polygons(\"vble\")\n\n\n\n\n\n\n\n\n\n\n\nInteractive Map\n\nSwitch to interactive mode with tmap_mode(\"view\") and create maps.\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(d) + tm_polygons(\"vble\")"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#mapping-point-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#mapping-point-data",
    "title": "Spatial data analaysis Training",
    "section": "Mapping Point Data",
    "text": "Mapping Point Data\n\nCreating Point Data Map with ggplot2\n\nPlot point data with ggplot2, using geom_sf() to define aesthetics for color and size.\n\n\nggplot(d) + geom_sf(aes(col = vble, size = size)) +\n  scale_color_viridis()"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#mapping-raster-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#mapping-raster-data",
    "title": "Spatial data analaysis Training",
    "section": "Mapping Raster Data",
    "text": "Mapping Raster Data\n\nCreating Raster Data Map with ggplot2\n\nPlot raster data by converting it to a data frame and using geom_raster().\n\n\nggplot(d) + geom_sf() +\n  geom_raster(data = as.data.frame(r, xy = TRUE),\n    aes(x = x, y = y, fill = elevation))"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#mapping-mobility-flows-with-flowmapblue",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#mapping-mobility-flows-with-flowmapblue",
    "title": "Spatial data analaysis Training",
    "section": "Mapping Mobility Flows with flowmapblue",
    "text": "Mapping Mobility Flows with flowmapblue\n\nCreating Interactive Flow Map\n\nCreate interactive flow maps with flowmapblue.\nDefine locations and flows data frames.\nGenerate the map with clustering, dark mode, and animation options.\n\n\nlibrary(flowmapblue)\nlocations &lt;- data.frame(\n  id = c(1, 2, 3),\n  name = c(\"New York\", \"London\", \"Rio de Janeiro\"),\n  lat = c(40.713543, 51.507425, -22.906241),\n  lon = c(-74.011219, -0.127738, -43.180244))\nflows &lt;- data.frame(\n  origin = c(1, 2, 3, 2, 1, 3),\n  dest = c(2, 1, 1, 3, 3 , 2),\n  count = c(42, 51, 50, 40, 22, 42))\nflowmapblue(locations, flows, mapboxAccessToken,\n            clustering = TRUE, darkMode = TRUE, animation = FALSE)"
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#r-packages-to-download-open-spatial-data",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#r-packages-to-download-open-spatial-data",
    "title": "Spatial data analaysis Training",
    "section": "R Packages to Download Open Spatial Data",
    "text": "R Packages to Download Open Spatial Data\nSpatial data are used in a wide range of disciplines including environment, health, agriculture, economy, and society (Moraga and Baker 2022). Several R packages have been recently developed as clients for various databases that can be used for easy access to spatial data including administrative boundaries, climatic, and OpenStreetMap data. Here, we give short reproducible examples on how to download and visualize spatial data that can be useful in different settings. More extended examples and details about the capabilities of each of the packages can be seen at the packages‚Äô websites and the rspatialdata website which provides a collection of tutorials on R packages to download and visualize spatial data using R."
  },
  {
    "objectID": "static/slides/Spatial-Data-Slide/Spatial.html#administrative-boundaries-of-countries",
    "href": "static/slides/Spatial-Data-Slide/Spatial.html#administrative-boundaries-of-countries",
    "title": "Spatial data analaysis Training",
    "section": "Administrative Boundaries of Countries",
    "text": "Administrative Boundaries of Countries\n\nOverview\n\nAdministrative boundaries data can be obtained from multiple packages:\n\nrnaturalearth: Global administrative boundaries.\ntidycensus and tigris: USA-specific data.\nmapSpain: Spain-specific data.\ngeobr: Brazil-specific data.\ngiscoR: Eurostat GISCO data."
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#housekeeping",
    "href": "static/slides/NEAIR/NEAIR.html#housekeeping",
    "title": "Streamlining with R",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\nIntro üëã\nWorkshop materials ‚¨áÔ∏è\nBreak üïò\nBy the end of today ‚úîÔ∏è\nToday‚Äôs plan üìã"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#todays-plan",
    "href": "static/slides/NEAIR/NEAIR.html#todays-plan",
    "title": "Streamlining with R",
    "section": "Today‚Äôs plan",
    "text": "Today‚Äôs plan\n\n\n\nWhat is R? How can it ease the burden of repeated reporting?\nBasic functions for manipulating data\nUsing R effectively\nMore data manipulation\nVisualizing data\nA peek at advanced topics"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#what-is-r-1",
    "href": "static/slides/NEAIR/NEAIR.html#what-is-r-1",
    "title": "Streamlining with R",
    "section": "What is R?",
    "text": "What is R?\n\n\n1 2 3 4 5 6\n\n\n   \n\nR is an open-source (free!) scripting language for working with data"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#the-benefits-of-r",
    "href": "static/slides/NEAIR/NEAIR.html#the-benefits-of-r",
    "title": "Streamlining with R",
    "section": "The benefits of R",
    "text": "The benefits of R\n\n\n1 2 3 4 5 6\n\n\n \nMy personal Excel nightmare\n\n\nThe magic of R is that it‚Äôs reproducible (by someone else or by yourself in six months)\n\n\n\nKeeps data separate from code (data preparation steps)"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#getting-r",
    "href": "static/slides/NEAIR/NEAIR.html#getting-r",
    "title": "Streamlining with R",
    "section": "Getting R",
    "text": "Getting R\n\n\n1 2 3 4 5 6\n\n\n  \nYou need the R language  \n\nAnd also the software"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#navigating-rstudio",
    "href": "static/slides/NEAIR/NEAIR.html#navigating-rstudio",
    "title": "Streamlining with R",
    "section": "Navigating RStudio",
    "text": "Navigating RStudio\n\n\n1 2 3 4 5 6\n\n\n\n\n\n\n\n\n\nproject files are here\n\n\n\n\n\nimported data shows up here\n\n\n\n\n\n\ncode can go here"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#navigating-rstudio-1",
    "href": "static/slides/NEAIR/NEAIR.html#navigating-rstudio-1",
    "title": "Streamlining with R",
    "section": "Navigating RStudio",
    "text": "Navigating RStudio\n\n\n1 2 3 4 5 6\n\n\n\n\n\n\n\n\n\nproject files are here\n\n\n\n\nimported data shows up here\n\n\n\n\ncode can alsogo here"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#using-r",
    "href": "static/slides/NEAIR/NEAIR.html#using-r",
    "title": "Streamlining with R",
    "section": "Using R",
    "text": "Using R\n\n\n1 2 3 4 5 6\n\n\n\n\n  You use R via packages\n\n ‚Ä¶which contain functions\n\n ‚Ä¶which are just verbs"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#todays-data",
    "href": "static/slides/NEAIR/NEAIR.html#todays-data",
    "title": "Streamlining with R",
    "section": "Today‚Äôs data",
    "text": "Today‚Äôs data\n\n\n1 2 3 4 5 6\n\n\n\nfaculty\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#todays-data-1",
    "href": "static/slides/NEAIR/NEAIR.html#todays-data-1",
    "title": "Streamlining with R",
    "section": "Today‚Äôs data",
    "text": "Today‚Äôs data\n\n\n1 2 3 4 5 6\n\n\n\ncourses\n\n\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nenrollment\nlevel\n\n\n\n\n20212202\n10605\n1772\nPhysics\n7\nUG\n\n\n20212202\n10605\n1772\nPhysics\n32\nGR\n\n\n20212202\n11426\n1820\nPolitical Science\n8\nUG\n\n\n20212202\n12048\n1914\nEnglish\n24\nUG\n\n\n20212202\n13269\n1095\nSociology\n48\nUG\n\n\n20212202\n13517\n1086\nMusic\n17\nUG"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#useful-operators",
    "href": "static/slides/NEAIR/NEAIR.html#useful-operators",
    "title": "Streamlining with R",
    "section": "Useful operators",
    "text": "Useful operators\n\n\n1 2 3 4 5 6\n\n\n \n\n\n\n&lt;-\n\n\n\n‚Äúsave as‚Äù\n\n\n\nopt + -\n\n\n\n\n\n\n\n%&gt;%\n\n\n\n‚Äúand then‚Äù\n\n\n\nCmd + shift + m"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#common-functions",
    "href": "static/slides/NEAIR/NEAIR.html#common-functions",
    "title": "Streamlining with R",
    "section": "Common functions",
    "text": "Common functions\n\n\n1 2 3 4 5 6\n\n\n\n\nfilter keeps or discards rows (aka observations)\nselect keeps or discards columns (aka variables)\narrange sorts data set by certain variable(s)\ncount tallies data set by certain variable(s)\nmutate creates new variables\ngroup_by/summarize aggregates data (pivot tables!)\nstr_* functions work easily with text"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#syntax-of-a-function",
    "href": "static/slides/NEAIR/NEAIR.html#syntax-of-a-function",
    "title": "Streamlining with R",
    "section": "Syntax of a function",
    "text": "Syntax of a function\n\n\n1 2 3 4 5 6\n\n\n\n\nfunction(data, argument(s))\n\n\nis the same as\n\ndata %&gt;%\n¬†¬†¬†¬†function(argument(s))"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#filter",
    "href": "static/slides/NEAIR/NEAIR.html#filter",
    "title": "Streamlining with R",
    "section": "Filter",
    "text": "Filter\n\n\n1 2 3 4 5 6\n\n\n\nfilter keeps or discards rows (aka observations)\nthe == operator tests for equality\n\n\n\n\nfaculty %&gt;% \n  filter(dept1 == \"Sociology\")\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology\n\n\n\n2021-22\n1118\nAssistant Professor\nSociology\n\n\n\n2021-22\n1161\nAssistant Professor\nSociology\n\n\n\n2021-22\n1191\nProfessor\nSociology\n\n\n\n2021-22\n1216\nAssociate Professor\nSociology\nAmerican Studies\n\n\n2021-22\n1273\nAssistant Professor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#filter-1",
    "href": "static/slides/NEAIR/NEAIR.html#filter-1",
    "title": "Streamlining with R",
    "section": "Filter",
    "text": "Filter\n\n\n1 2 3 4 5 6\n\n\n\nthe | operator signifies ‚Äúor‚Äù\n\n\n\n\nfaculty %&gt;% \n  filter(dept1 == \"Sociology\" | \n           dept1 == \"Physics\")\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology\n\n\n\n2021-22\n1118\nAssistant Professor\nSociology\n\n\n\n2021-22\n1161\nAssistant Professor\nSociology\n\n\n\n2021-22\n1191\nProfessor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#filter-2",
    "href": "static/slides/NEAIR/NEAIR.html#filter-2",
    "title": "Streamlining with R",
    "section": "Filter",
    "text": "Filter\n\n\n1 2 3 4 5 6\n\n\n\nthe %in% operator allows for multiple options in a list\n\n\n\n\nfaculty %&gt;% \n  filter(dept1 %in% c(\"Sociology\",\n                      \"Physics\",\n                      \"Music\"))\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology\n\n\n\n2021-22\n1118\nAssistant Professor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#filter-3",
    "href": "static/slides/NEAIR/NEAIR.html#filter-3",
    "title": "Streamlining with R",
    "section": "Filter",
    "text": "Filter\n\n\n1 2 3 4 5 6\n\n\n\nthe & operator combines conditions\n\n\n\n\nfaculty %&gt;% \n  filter(dept1 %in% c(\"Sociology\",\n                      \"Physics\",\n                      \"Music\") &\n         rank == \"Professor\")\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1191\nProfessor\nSociology\n\n\n\n2021-22\n1201\nProfessor\nPhysics\n\n\n\n2021-22\n1209\nProfessor\nMusic\n\n\n\n2021-22\n1421\nProfessor\nPhysics\nEngineering"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#select",
    "href": "static/slides/NEAIR/NEAIR.html#select",
    "title": "Streamlining with R",
    "section": "Select",
    "text": "Select\n\n\n1 2 3 4 5 6\n\n\n\nselect keeps or discards columns (aka variables)\n\n\n\n\nfaculty %&gt;% \n  select(id, dept1, rank)\n\n\n\n\n\n\n\n\n\nid\ndept1\nrank\n\n\n\n\n1005\nChemistry\nLecturer\n\n\n1022\nPhysics\nProfessor\n\n\n1059\nPhysics\nProfessor\n\n\n1079\nMusic\nLecturer\n\n\n1086\nMusic\nAssistant Professor\n\n\n1095\nSociology\nAdjunct Instructor"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#select-1",
    "href": "static/slides/NEAIR/NEAIR.html#select-1",
    "title": "Streamlining with R",
    "section": "Select",
    "text": "Select\n\n\n1 2 3 4 5 6\n\n\n\ncan drop columns with -column\n\n\n\n\nfaculty %&gt;% \n  select(-dept2)\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\n\n2021-22\n1022\nProfessor\nPhysics\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#select-2",
    "href": "static/slides/NEAIR/NEAIR.html#select-2",
    "title": "Streamlining with R",
    "section": "Select",
    "text": "Select\n\n\n1 2 3 4 5 6\n\n\n\nthe pipe %&gt;% chains multiple functions together\n\n\n\n\nfaculty %&gt;% \n  select(id, dept1, rank) %&gt;% \n  filter(rank == \"Professor\")\n\n\n\n\n\n\n\n\n\nid\ndept1\nrank\n\n\n\n\n1022\nPhysics\nProfessor\n\n\n1059\nPhysics\nProfessor\n\n\n1191\nSociology\nProfessor\n\n\n1201\nPhysics\nProfessor\n\n\n1209\nMusic\nProfessor\n\n\n1407\nEnglish\nProfessor"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#arrange",
    "href": "static/slides/NEAIR/NEAIR.html#arrange",
    "title": "Streamlining with R",
    "section": "Arrange",
    "text": "Arrange\n\n\n1 2 3 4 5 6\n\n\n\narrange sorts data set by certain variable(s)\nuse desc() to get descending order\n\n\n\n\ncourses %&gt;% \n  arrange(desc(enrollment))\n\n\n\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nenrollment\nlevel\n\n\n\n\n20212201\n10511\n1005\nChemistry\n50\nUG\n\n\n20212201\n15934\n1421\nPhysics\n50\nUG\n\n\n20192002\n13850\n1105\nChemistry\n50\nUG\n\n\n20181901\n17773\n1942\nMusic\n50\nUG\n\n\n20212202\n13269\n1095\nSociology\n48\nUG\n\n\n20202101\n16202\n1816\nPolitical Science\n48\nUG"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#arrange-1",
    "href": "static/slides/NEAIR/NEAIR.html#arrange-1",
    "title": "Streamlining with R",
    "section": "Arrange",
    "text": "Arrange\n\n\n1 2 3 4 5 6\n\n\n\ncan sort by multiple variables\n\n\n\n\ncourses %&gt;% \n  arrange(dept, desc(enrollment))\n\n\n\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nenrollment\nlevel\n\n\n\n\n20212201\n10511\n1005\nChemistry\n50\nUG\n\n\n20192002\n13850\n1105\nChemistry\n50\nUG\n\n\n20202102\n13850\n1258\nChemistry\n39\nUG\n\n\n20202102\n16606\n1393\nChemistry\n38\nUG\n\n\n20202101\n16540\n1784\nChemistry\n38\nUG\n\n\n20181901\n10511\n1829\nChemistry\n36\nUG"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#count",
    "href": "static/slides/NEAIR/NEAIR.html#count",
    "title": "Streamlining with R",
    "section": "Count",
    "text": "Count\n\n\n1 2 3 4 5 6\n\n\n\ncount tallies data set by certain variable(s) (very useful for familiarizing yourself with data)\n\n\n\n\ncourses %&gt;% \n  count(dept)\n\n\n\n\n\n\n\n\n\ndept\nn\n\n\n\n\nChemistry\n16\n\n\nEnglish\n18\n\n\nMusic\n17\n\n\nPhysics\n19\n\n\nPolitical Science\n17\n\n\nSociology\n17"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#count-1",
    "href": "static/slides/NEAIR/NEAIR.html#count-1",
    "title": "Streamlining with R",
    "section": "Count",
    "text": "Count\n\n\n1 2 3 4 5 6\n\n\n\ncan use sort = TRUE to order results\n\n\n\n\ncourses %&gt;% \n  count(dept, level, sort = TRUE)\n\n\n\n\n\n\n\n\n\ndept\nlevel\nn\n\n\n\n\nChemistry\nUG\n16\n\n\nEnglish\nUG\n16\n\n\nMusic\nUG\n16\n\n\nPhysics\nUG\n16\n\n\nPolitical Science\nUG\n16\n\n\nSociology\nUG\n16\n\n\nPhysics\nGR\n3\n\n\nEnglish\nGR\n2\n\n\nMusic\nGR\n1\n\n\nPolitical Science\nGR\n1\n\n\nSociology\nGR\n1"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#mutate",
    "href": "static/slides/NEAIR/NEAIR.html#mutate",
    "title": "Streamlining with R",
    "section": "Mutate",
    "text": "Mutate\n\n\n1 2 3 4 5 6\n\n\n\nmutate creates new variables (with a single =)\n\n\n\n\nfaculty %&gt;% \n  mutate(new = \"hello!\")\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\nnew\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\nhello!\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\nhello!\n\n\n2021-22\n1059\nProfessor\nPhysics\n\nhello!\n\n\n2021-22\n1079\nLecturer\nMusic\n\nhello!\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\nhello!\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology\n\nhello!"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#mutate-1",
    "href": "static/slides/NEAIR/NEAIR.html#mutate-1",
    "title": "Streamlining with R",
    "section": "Mutate",
    "text": "Mutate\n\n\n1 2 3 4 5 6\n\n\n\nmuch more useful with a conditional such as ifelse(), which has three arguments:\ncondition, value if true, value if false\n\n\n\n\nfaculty %&gt;% \n  mutate(prof = ifelse(rank == \"Professor\",\n                       1, 0)) %&gt;% \n  select(rank, prof)\n\n\n\n\n\n\n\n\n\nrank\nprof\n\n\n\n\nLecturer\n0\n\n\nProfessor\n1\n\n\nProfessor\n1\n\n\nLecturer\n0\n\n\nAssistant Professor\n0\n\n\nAdjunct Instructor\n0"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#mutate-2",
    "href": "static/slides/NEAIR/NEAIR.html#mutate-2",
    "title": "Streamlining with R",
    "section": "Mutate",
    "text": "Mutate\n\n\n1 2 3 4 5 6\n\n\n\nthe ! operator means not\nis.na() identifies null values\n\n\n\n\nfaculty %&gt;% \n  mutate(joint = ifelse(!is.na(dept2),\n                        \"joint\", NA)) %&gt;% \n  select(dept1, dept2, joint)\n\n\n\n\n\n\n\n\n\ndept1\ndept2\njoint\n\n\n\n\nChemistry\n\n\n\n\nPhysics\nEngineering\njoint\n\n\nPhysics\n\n\n\n\nMusic\n\n\n\n\nMusic\n\n\n\n\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#mutate-3",
    "href": "static/slides/NEAIR/NEAIR.html#mutate-3",
    "title": "Streamlining with R",
    "section": "Mutate",
    "text": "Mutate\n\n\n1 2 3 4 5 6\n\n\n\nwith multiple conditions, case_when() is much easier!\n\nfaculty %&gt;% \n  mutate(division = case_when(dept1 %in% c(\"Sociology\",\"Political Science\") ~\n                                \"Social Sciences\",\n                              dept1 %in% c(\"Music\",\"English\") ~\n                                \"Humanities\",\n                              dept1 %in% c(\"Chemistry\",\"Physics\") ~\n                                \"Sciences\")) %&gt;% \n  select(dept1, division)\n\n\n\n\n\n\n\n\ndept1\ndivision\n\n\n\n\nChemistry\nSciences\n\n\nPhysics\nSciences\n\n\nPhysics\nSciences\n\n\nMusic\nHumanities\n\n\nMusic\nHumanities\n\n\nSociology\nSocial Sciences"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#group-by-summarize",
    "href": "static/slides/NEAIR/NEAIR.html#group-by-summarize",
    "title": "Streamlining with R",
    "section": "Group by / summarize",
    "text": "Group by / summarize\n\n\n1 2 3 4 5 6\n\n\n\ngroup_by/summarize aggregates data (pivot tables!)\ngroup_by() identifies the grouping variable(s) and summarize() specifies the aggregation\n\n\n\n\ncourses %&gt;% \n  group_by(dept, semester) %&gt;% \n  summarize(enr = sum(enrollment))\n\n\n\n\n\n\n\n\n\ndept\nsemester\nenr\n\n\n\n\nChemistry\n20181901\n59\n\n\nChemistry\n20181902\n44\n\n\nChemistry\n20192001\n47\n\n\nChemistry\n20192002\n68\n\n\nChemistry\n20202101\n69\n\n\nChemistry\n20202102\n77"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#group-by-summarize-1",
    "href": "static/slides/NEAIR/NEAIR.html#group-by-summarize-1",
    "title": "Streamlining with R",
    "section": "Group by / summarize",
    "text": "Group by / summarize\n\n\n1 2 3 4 5 6\n\n\n\nuseful arguments within summarize:\nmean, median, sd, min, max, n\n\n\n\n\ncourses %&gt;% \n  group_by(dept, semester) %&gt;% \n  summarize(enr = sum(enrollment),\n            count = n_distinct(course_id))\n\n\n\n\n\n\n\n\n\ndept\nsemester\nenr\ncourses\n\n\n\n\nChemistry\n20181901\n59\n2\n\n\nChemistry\n20181902\n44\n2\n\n\nChemistry\n20192001\n47\n2\n\n\nChemistry\n20192002\n68\n2\n\n\nChemistry\n20202101\n69\n2\n\n\nChemistry\n20202102\n77\n2"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#working-in-rstudio",
    "href": "static/slides/NEAIR/NEAIR.html#working-in-rstudio",
    "title": "Streamlining with R",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\n1 2 3 4 5 6\n\n\n\n\n\n\n\n\n\nproject files are here\n\n\n\n\nimported data shows up here\n\n\n\n\ncode can alsogo here"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#working-in-rstudio-1",
    "href": "static/slides/NEAIR/NEAIR.html#working-in-rstudio-1",
    "title": "Streamlining with R",
    "section": "Working in RStudio",
    "text": "Working in RStudio\n\n\n1 2 3 4 5 6\n\n\n\n\nTyping in the console\n\nthink of it like a post-it: useful for quick notes but disposable\nactions are saved but code is not\none chunk of code is run at a time (Return)\n\n\n\nTyping in a code file\n\nscript files have a .R extension\ncode is saved and sections of any size can be run (Cmd + Return)\ndo ~95% of your typing in a code file instead of the console!"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#working-with-packages",
    "href": "static/slides/NEAIR/NEAIR.html#working-with-packages",
    "title": "Streamlining with R",
    "section": "Working with packages",
    "text": "Working with packages\n\n\n1 2 3 4 5 6\n\n\npackages need to be installed on each computer you use\n\n# only need to do this once (per computer)\ninstall.packages(\"tidyverse\")\n\n\n\npackages need to be loaded/attached with library() at the beginning of every session\n\n# always put the necessary packages at the top of a code file\nlibrary(tidyverse)\n\n\n\n\ncan access help files by typing ??tidyverse or ??mutate in the console"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#organizing-with-projects",
    "href": "static/slides/NEAIR/NEAIR.html#organizing-with-projects",
    "title": "Streamlining with R",
    "section": "Organizing with projects",
    "text": "Organizing with projects\n\n\n1 2 3 4 5 6\n\n\n\nhighly recommend using projects to stay organized\n\nkeeps code files and data files together, allowing for easier file path navigation and better reproducible work habits\n\n\nFile -&gt; New Project\n\nmore guidance: here and here"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#organizing-with-projects-1",
    "href": "static/slides/NEAIR/NEAIR.html#organizing-with-projects-1",
    "title": "Streamlining with R",
    "section": "Organizing with projects",
    "text": "Organizing with projects\n\n\n1 2 3 4 5 6\n\n\n\n\n\n\n\n\n\nproject files are here\n\n\n\n\nimported data shows up here\n\n\n\n\ncode can alsogo here"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#accessing-workshop-materials",
    "href": "static/slides/NEAIR/NEAIR.html#accessing-workshop-materials",
    "title": "Streamlining with R",
    "section": "Accessing workshop materials",
    "text": "Accessing workshop materials\n\n\n1 2 3 4 5 6\n\n\n\n\n\n\n\n\nclick big green Code button and select ‚ÄúDownload ZIP‚Äù, then open neair.Rproj"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#accessing-data",
    "href": "static/slides/NEAIR/NEAIR.html#accessing-data",
    "title": "Streamlining with R",
    "section": "Accessing data",
    "text": "Accessing data\n\n\n1 2 3 4 5 6\n\n\nuse read_csv() to import a csv file\n\n# the file path is this simple if you use projects!\n# ?read_csv() in the console will bring up the help file with more options\nfaculty &lt;- read_csv(\"faculty.csv\")\n\n\n\nthe readxl package is helpful for Excel files\n\n# needs to be loaded but not installed as it's part of the tidyverse\nlibrary(readxl)\nfaculty &lt;- read_excel(\"faculty.xlsx\", sheet = 2)\n\n\n\n\nview the data with View(faculty) or by clicking on the data name in the Environment pane"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#stringr-functions",
    "href": "static/slides/NEAIR/NEAIR.html#stringr-functions",
    "title": "Streamlining with R",
    "section": "Stringr functions",
    "text": "Stringr functions\n\n\n1 2 3 4 5 6\n\n\n\nfunctions from stringr (which all start with str_) are useful for working with text data\n\n\n\nfaculty %&gt;% \n  filter(str_detect(rank, \"Professor\"))\n\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n\n\n2021-22\n1118\nAssistant Professor\nSociology\n\n\n\n2021-22\n1158\nAssistant Professor\nPolitical Science\n\n\n\n2021-22\n1161\nAssistant Professor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#stringr-functions-1",
    "href": "static/slides/NEAIR/NEAIR.html#stringr-functions-1",
    "title": "Streamlining with R",
    "section": "Stringr functions",
    "text": "Stringr functions\n\n\n1 2 3 4 5 6\n\n\n\ncheat sheet of functions is here\n\n\n\n\ncourses %&gt;% \n  mutate(year = str_c(str_sub(semester, 1, 4), \n                      \"-\",\n                      str_sub(semester, 5, 6))) %&gt;% \n  select(semester, year) %&gt;% \n  unique()\n\n\n\n\n\n\n\n\n\nsemester\nyear\n\n\n\n\n20212202\n2021-22\n\n\n20212201\n2021-22\n\n\n20202102\n2020-21\n\n\n20202101\n2020-21\n\n\n20192002\n2019-20\n\n\n20192001\n2019-20\n\n\n20181902\n2018-19\n\n\n20181901\n2018-19"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#pivoting-data",
    "href": "static/slides/NEAIR/NEAIR.html#pivoting-data",
    "title": "Streamlining with R",
    "section": "Pivoting data",
    "text": "Pivoting data\n\n\n1 2 3 4 5 6\n\n\nexisting faculty data has one row per faculty, some with multiple departments (sometimes known as wide data)\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#pivoting-data-1",
    "href": "static/slides/NEAIR/NEAIR.html#pivoting-data-1",
    "title": "Streamlining with R",
    "section": "Pivoting data",
    "text": "Pivoting data\n\n\n1 2 3 4 5 6\n\n\nwhat if you instead want one row per faculty per department? (sometimes known as long data)\n\n\n\n\n\n\n\nyear\nid\nrank\ndept_no\ndept\n\n\n\n\n2021-22\n1005\nLecturer\ndept1\nChemistry\n\n\n2021-22\n1022\nProfessor\ndept1\nPhysics\n\n\n2021-22\n1022\nProfessor\ndept2\nEngineering\n\n\n2021-22\n1059\nProfessor\ndept1\nPhysics\n\n\n2021-22\n1079\nLecturer\ndept1\nMusic\n\n\n2021-22\n1086\nAssistant Professor\ndept1\nMusic"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#pivoting-data-2",
    "href": "static/slides/NEAIR/NEAIR.html#pivoting-data-2",
    "title": "Streamlining with R",
    "section": "Pivoting data",
    "text": "Pivoting data\n\n\n1 2 3 4 5 6\n\n\n\nthe pivot_longer function lengthens data\n\n\n\n\nfaculty %&gt;% \n  pivot_longer(dept1:dept2,\n               names_to = \"dept_no\",\n               values_to = \"dept\",\n               values_drop_na = TRUE) %&gt;% \n  select(-year, -rank)\n\n\n\n\n\n\n\n\n\nid\ndept_no\ndept\n\n\n\n\n1005\ndept1\nChemistry\n\n\n1022\ndept1\nPhysics\n\n\n1022\ndept2\nEngineering\n\n\n1059\ndept1\nPhysics\n\n\n1079\ndept1\nMusic\n\n\n1086\ndept1\nMusic"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#pivoting-data-3",
    "href": "static/slides/NEAIR/NEAIR.html#pivoting-data-3",
    "title": "Streamlining with R",
    "section": "Pivoting data",
    "text": "Pivoting data\n\n\n1 2 3 4 5 6\n\n\n\nand pivot_wider does the opposite!\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nenrollment\nlevel\n\n\n\n\n20212202\n10605\n1772\nPhysics\n7\nUG\n\n\n20212202\n10605\n1772\nPhysics\n32\nGR\n\n\n\n\n\n\n\n\n\ncourses %&gt;% \n  pivot_wider(names_from = \"level\",\n              values_from = \"enrollment\")\n\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nUG\nGR\n\n\n\n\n20212202\n10605\n1772\nPhysics\n7\n32\n\n\n20212202\n11426\n1820\nPolitical Science\n8\n\n\n\n20212202\n12048\n1914\nEnglish\n24\n\n\n\n20212202\n13269\n1095\nSociology\n48"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\nR has many useful functions for handling relational data\n\nall you need is at least one key variable that connects data sets\n\nleft_join is most common, but there are more"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-1",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-1",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\nwhat‚Äôs the average UG enrollment per year, per faculty rank?\n\n\nfaculty\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n\n\n\n\n\n\ncourses\n\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nenrollment\nlevel\n\n\n\n\n20212202\n10605\n1772\nPhysics\n7\nUG\n\n\n20212202\n10605\n1772\nPhysics\n32\nGR\n\n\n20212202\n11426\n1820\nPolitical Science\n8\nUG\n\n\n20212202\n12048\n1914\nEnglish\n24\nUG\n\n\n\n\n\n\n\n\n\nfaculty$id is the same as courses$faculty_id"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-2",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-2",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\nwhat‚Äôs the average UG enrollment per year, per faculty rank?\n\n\n\n\n\n\n\nsemester\ncourse_id\nfaculty_id\ndept\nenrollment\nlevel\n\n\n\n\n20212202\n10605\n1772\nPhysics\n7\nUG\n\n\n20212202\n10605\n1772\nPhysics\n32\nGR\n\n\n20212202\n11426\n1820\nPolitical Science\n8\nUG\n\n\n20212202\n12048\n1914\nEnglish\n24\nUG\n\n\n20212202\n13269\n1095\nSociology\n48\nUG\n\n\n\n\n\n\n\n\nfilter to UG courses only\ncreate our year variable again\nsummarize enrollment by year and faculty_id"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-3",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-3",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\nuse the &lt;- operator to create a new data frame courses_UG\n\ncourses_UG &lt;- courses %&gt;% \n  filter(level == \"UG\") %&gt;% \n  mutate(year = str_c(str_sub(semester, 1, 4), \n                      \"-\",\n                      str_sub(semester, 5, 6)))"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-4",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-4",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\nfilter to undergraduate courses only and mutate a new academic year variable\n\ncourses_UG &lt;- courses %&gt;% \n  filter(level == \"UG\") %&gt;% \n  mutate(year = str_c(str_sub(semester, 1, 4), \n                      \"-\",\n                      str_sub(semester, 5, 6)))"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-5",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-5",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\ngroup_by year and faculty member; summarize enrollment\n\ncourses_UG &lt;- courses %&gt;% \n  filter(level == \"UG\") %&gt;% \n  mutate(year = str_c(str_sub(semester, 1, 4), \n                      \"-\",\n                      str_sub(semester, 5, 6))) %&gt;% \n  group_by(year, faculty_id) %&gt;% \n  summarize(enr = sum(enrollment))\n\n\n\n\n\n\n\n\n\nyear\nfaculty_id\nenr\n\n\n\n\n2018-19\n1059\n35\n\n\n2018-19\n1086\n14\n\n\n2018-19\n1102\n37\n\n\n2018-19\n1203\n25"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-6",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-6",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\nwhat‚Äôs the average UG enrollment per year, per faculty rank?\n\n\n\nfaculty\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology\n\n\n\n\n\n\n\n\n\ncourses_UG\n\n\n\n\n\n\n\nyear\nfaculty_id\nenr\n\n\n\n\n2021-22\n1005\n50\n\n\n2021-22\n1086\n17\n\n\n2021-22\n1095\n48\n\n\n2021-22\n1128\n32\n\n\n2021-22\n1147\n32\n\n\n2021-22\n1191\n7"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-7",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-7",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\n\nfac_enr &lt;- faculty %&gt;% \n  left_join(courses_UG, by = c(\"id\" = \"faculty_id\",\n                               \"year\" = \"year\"))\n\n\n1\n\n\n2\n\n\n3\n\n\nnew data frame\ndata frame you‚Äôre adding data to\ndata frame where the new data is coming from\n\n\n\n\n\n\n\n\nyear\nid\nrank\ndept1\ndept2\nenr\n\n\n\n\n2021-22\n1005\nLecturer\nChemistry\n\n50\n\n\n2021-22\n1022\nProfessor\nPhysics\nEngineering\n\n\n\n2021-22\n1059\nProfessor\nPhysics\n\n\n\n\n2021-22\n1079\nLecturer\nMusic\n\n\n\n\n2021-22\n1086\nAssistant Professor\nMusic\n\n17\n\n\n2021-22\n1095\nAdjunct Instructor\nSociology\n\n48"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#joining-data-8",
    "href": "static/slides/NEAIR/NEAIR.html#joining-data-8",
    "title": "Streamlining with R",
    "section": "Joining data",
    "text": "Joining data\n\n\n1 2 3 4 5 6\n\n\n\nwhat‚Äôs the average UG enrollment per year, per faculty rank?\n\nfac_enr &lt;- faculty %&gt;% \n  left_join(courses_UG, by = c(\"id\" = \"faculty_id\",\n                               \"year\" = \"year\")) %&gt;% \n  group_by(year, rank) %&gt;% \n  summarize(avg_enr = mean(enr, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\nyear\nrank\navg_enr\n\n\n\n\n2021-22\nAdjunct Instructor\n34.66667\n\n\n2021-22\nAssistant Professor\n23.60000\n\n\n2021-22\nAssociate Professor\n17.25000\n\n\n2021-22\nLecturer\n31.83333\n\n\n2021-22\nProfessor\n32.16667\n\n\n2021-22\nVisiting Researcher"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#ggplot2",
    "href": "static/slides/NEAIR/NEAIR.html#ggplot2",
    "title": "Streamlining with R",
    "section": "ggplot2",
    "text": "ggplot2\n\n\n1 2 3 4 5 6\n\n\n\nggplot2 is the data visualization package that is loaded with the tidyverse\n\nthe grammar of graphics maps data to the aesthetic attributes of geometric points\n\n\nencoding data into visual cues (e.g., length, color, position, size) is how we signify changes and comparisons"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#bar-chart",
    "href": "static/slides/NEAIR/NEAIR.html#bar-chart",
    "title": "Streamlining with R",
    "section": "Bar chart",
    "text": "Bar chart\n\n\n1 2 3 4 5 6\n\n\n\nfaculty %&gt;%\n  count(rank) %&gt;%\n  ggplot(aes(x = rank, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\nto combine lines into one code chunk, use + instead of %&gt;%"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#bar-chart-1",
    "href": "static/slides/NEAIR/NEAIR.html#bar-chart-1",
    "title": "Streamlining with R",
    "section": "Bar chart",
    "text": "Bar chart\n\n\n1 2 3 4 5 6\n\n\ncan create a prettier plot pretty easily\n\n\nexpand for full code\nfaculty %&gt;%\n  count(rank) %&gt;%\n  ggplot(aes(x = reorder(rank, -n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"#cc0000\") +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  geom_text(aes(label = n), vjust = -0.5) +\n  labs(x = NULL, y = NULL,\n       title = \"Count of faculty by rank, 2018-2021\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.ticks = element_blank())"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#line-graph",
    "href": "static/slides/NEAIR/NEAIR.html#line-graph",
    "title": "Streamlining with R",
    "section": "Line graph",
    "text": "Line graph\n\n\n1 2 3 4 5 6\n\n\n\n\nfac_enr %&gt;% \n  filter(!is.na(avg_enr)) %&gt;% \n  ggplot(aes(x = year, y = avg_enr, group = rank, color = rank)) +\n  geom_line()"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#line-graph-1",
    "href": "static/slides/NEAIR/NEAIR.html#line-graph-1",
    "title": "Streamlining with R",
    "section": "Line graph",
    "text": "Line graph\n\n\n1 2 3 4 5 6\n\n\n\n\nexpand for full code\nfac_enr %&gt;% \n  filter(!is.na(avg_enr)) %&gt;% \n  ggplot(aes(x = year, y = avg_enr, group = rank, color = rank)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(x = NULL, y = \"Average enrollment\",\n       title = \"Average undergraduate enrollment per rank over time\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.ticks = element_blank(),\n        legend.title = element_blank(),\n        legend.background = element_rect(fill = NA),\n        legend.key = element_rect(fill = NA),\n        legend.position = c(0.85, 0.82))"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#ggplot2-resources",
    "href": "static/slides/NEAIR/NEAIR.html#ggplot2-resources",
    "title": "Streamlining with R",
    "section": "ggplot2 resources",
    "text": "ggplot2 resources\n\n\n1 2 3 4 5 6\n\n\nfrom R for Data Science\n\nData Visualization: a practical introduction\n\ncreating custom themes\n\nthe ggplot2 book\n\nthe R graph gallery"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#putting-it-all-together",
    "href": "static/slides/NEAIR/NEAIR.html#putting-it-all-together",
    "title": "Streamlining with R",
    "section": "Putting it all together",
    "text": "Putting it all together\n\n\n1 2 3 4 5 6\n\n\n\nwith what we‚Äôve done so far, your .R file could:\n\n\nimport your data files\ndocument all data cleaning and preparation steps and decisions\nproduce a PPT-ready graphic summarizing your results\n\n\n\n\nand that file would make it extremely easy for you or someone else to reproduce this analysis with new data in six months"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#r-markdown",
    "href": "static/slides/NEAIR/NEAIR.html#r-markdown",
    "title": "Streamlining with R",
    "section": "R Markdown",
    "text": "R Markdown\n\n\n1 2 3 4 5 6\n\n\n\nusing RStudio, create .Rmd documents that combine text, code, and graphics\n\n\nmany output formats: html, pdf, Word, slides\n\n\n\nexceedingly useful for parameterized reporting: can create an R-based PDF report and generate it automatically for, say, each department"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#internal-packages",
    "href": "static/slides/NEAIR/NEAIR.html#internal-packages",
    "title": "Streamlining with R",
    "section": "Internal packages",
    "text": "Internal packages\n\n\n1 2 3 4 5 6\n\n\n\nyou can also create your own packages!\n\n\nyour package can hold:\n\ncommon data sets that are used across projects\ncustom ggplot2 themes\ncommon functions and calculations (and their definitions!)\n\n\n\n\ncan be stored on a shared drive to facilitate collaboration"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#r-markdown-and-package-resources",
    "href": "static/slides/NEAIR/NEAIR.html#r-markdown-and-package-resources",
    "title": "Streamlining with R",
    "section": "R Markdown and package resources",
    "text": "R Markdown and package resources\n\n\n1 2 3 4 5 6\n\n\n\nR Markdown\nthe official R Markdown website\nR Markdown: The Definitive Guide\n\ninternal packages\na comprehensive theoretical explainer\na talk I gave earlier this year on the topic"
  },
  {
    "objectID": "static/slides/NEAIR/NEAIR.html#resources",
    "href": "static/slides/NEAIR/NEAIR.html#resources",
    "title": "Streamlining with R",
    "section": "Resources",
    "text": "Resources\nR for Data Science: the ultimate guide\n\nR for Excel users: a very useful workshop\n\nSTAT 545: an online book on reproducible data analysis in R\n\nthe RStudio Education site\n\nthe Learn tidyverse site\n\n\n\ngithub.com/meghall06/neair"
  },
  {
    "objectID": "pres/index.html",
    "href": "pres/index.html",
    "title": "Yebelay Berehan",
    "section": "",
    "text": "2024-02-21\n    \n    Spatial data analaysis Training\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    sf\n                \n                \n                \n                    Spatial analysis\n                \n                \n            \n            \n    \n    \n      ¬†¬†Details\n    \n    \n    \n    \n  \n\n  \n    \n       2024-01-18\n    \n    Joint Models Workshop\n       \n            \n                \n                \n                    Xaringan\n                \n                \n                \n                    Joint Model\n                \n                \n                \n                    workshop\n                \n                \n            \n            \n    \n    \n      ¬†¬†Details\n    \n    \n    \n    \n  \n\n  \n    \n       2023-09-25\n    \n    C4ED Presentation them\n       \n            \n                \n                \n                    R\n                \n                \n                \n                    Xaringan\n                \n                \n                \n                    C4ED theme\n                \n                \n            \n            \n    \n    \n      ¬†¬†Details\n    \n    \n    \n    \n  \n\n  \n    \n       2023-08-19\n    \n    Introduction to Rmarkdown\n       \n            \n                \n                \n                    R Markdown\n                \n                \n                \n                    xaringan\n                \n                \n            \n            \n    \n    \n      ¬†¬†Details\n    \n    \n    \n    \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#introductions",
    "href": "static/slides/Rintro/Day 1.html#introductions",
    "title": "",
    "section": "Introductions",
    "text": "Introductions\nTake few minutes to introduce ourselves.\n\nPlease share ‚Ä¶\n\nYour name\nYour experience in R\nWhat you expect at end of the training\n\n\n\n\n\n\n‚àí&plus;\n\n02:00"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section",
    "href": "static/slides/Rintro/Day 1.html#section",
    "title": "",
    "section": "",
    "text": "3. Contributed packages: Due to the open nature of R, anyone can contribute new packages at any time.\n\nCurrently, the CRAN package repository features 20394 available packages.\n\n\nCodenrow(available.packages())\n\n\nInstalling Packages\n\n\nOption 1: Menu"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#why-r",
    "href": "static/slides/Rintro/Day 1.html#why-r",
    "title": "",
    "section": "Why R?",
    "text": "Why R?\n\nIt is free, versatile, fast, and modern.\nIt has a large and friendly community of users that help answer questions and develop new R tools.\nWith more than 20394 add-on packages available, R offers more functions for data analysis than any other statistical software.\nR makes it easy to construct reproducible analyses and workflows that allow you to easily repeat the same analysis more than once.\nIt is flexible enough to be used to create interactive web pages (eg. my draft website) and automated reports.\nSimply currently the best tool there is for data analysis."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#what-are-advantages-or-disadvantages-of-r",
    "href": "static/slides/Rintro/Day 1.html#what-are-advantages-or-disadvantages-of-r",
    "title": "",
    "section": "What are Advantages or Disadvantages of R",
    "text": "What are Advantages or Disadvantages of R\n\n\nAdvantages\n\nAvailability and compatibility\nState of the art graphics capabilities\nCan import files from other (statistical) programs\nNew version every x months\nInteractive development environments (IDEs) available\nLarge users community\nreproducible research\n\n\nDrawbacks of R\n\nExpert friendly\nLearn by example\nNot very (easily) interactive\nCommand based\nDocumentation sometimes cryptic\n(Too) large amount of resources\nConstantly evolving\nMemory intensive and slow at times"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#rstudio",
    "href": "static/slides/Rintro/Day 1.html#rstudio",
    "title": "",
    "section": "2. RStudio",
    "text": "2. RStudio\nWhat is RStudio? Why use it?\n\nBest Integrated Development Environment (IDE) for R.\nPowerful and makes using R easier\n\nRStudio can:\n\nOrganize your code, output, and plots.\nAuto-complete code and highlight syntax.\nHelp view data and objects.\nEnable easy integration of R code into documents.\n\n\nUser-friendly interfaces"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#set-up-on-windows",
    "href": "static/slides/Rintro/Day 1.html#set-up-on-windows",
    "title": "",
    "section": "Set up on Windows",
    "text": "Set up on Windows\n\nFollow the steps below to download and install R:\n\n\nGo to cran.rstudio.com to access the R installation page. Then click the download link for Windows:\n\n\n\n\n\n\nChoose the ‚Äúbase‚Äù sub-directory."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#rstudio-overview",
    "href": "static/slides/Rintro/Day 1.html#rstudio-overview",
    "title": "",
    "section": "RStudio Overview",
    "text": "RStudio Overview"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#getting-started",
    "href": "static/slides/Rintro/Day 1.html#getting-started",
    "title": "",
    "section": "Getting started",
    "text": "Getting started\n\nSetting up and utilizing R & RStudio\nNavigating R & Rmarkdown scripts and RStudio projects\nImporting and inspecting data sets in R\nManipulating data through filtering, summarizing, transforming, and joining\nVisualizing data using the renowned ggplot2 package"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#customization",
    "href": "static/slides/Rintro/Day 1.html#customization",
    "title": "",
    "section": "Customization",
    "text": "Customization\nPanes\n\nThe size and position of the panes can be customized.\nOn the top right of each pane, there are buttons to adjust the pane size.\nAlso, place your mouse pointer/cursor on the borderline between panes and when the pointer changes its shape, click and drag to adjust the pane size.\nFor more options, go to View &gt; Panes on the menu bar.\nAlternatively, try Tools &gt; Global Options &gt; Pane Layout.\n\nAppearances\n\nThe overall appearance can be customized as well.\nGo to Tools &gt; Global Options&gt; Appearance on the menu bar to change themes, fonts, and more."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#the-rstudio-panes",
    "href": "static/slides/Rintro/Day 1.html#the-rstudio-panes",
    "title": "",
    "section": "The RStudio panes",
    "text": "The RStudio panes\nBy default, RStudio is arranged into four window panes.\nIf you only see three panes, open a new script with File &gt; New File &gt; R Script . This should reveal one more pane.\n\n\n\n\nBefore we go any further, we will rearrange these panes to improve the usability of the interface.\n\nThen under Pane Layout, adjust the pane arrangement. The arrangement we recommend is shown below."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#installing-and-loading-packages",
    "href": "static/slides/Rintro/Day 1.html#installing-and-loading-packages",
    "title": "",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\n\nPackages are collections of R functions, data, and compiled code in a well-defined format.\nThere are three categories of packages.\n\n1. Base Packages: Providing the basic functionality, maintained by the R Core Development group. Currently, there are 14 packages, these are\n\nCoderownames(installed.packages(priority=\"base\"))\n\n [1] \"base\"      \"compiler\"  \"datasets\"  \"graphics\"  \"grDevices\" \"grid\"     \n [7] \"methods\"   \"parallel\"  \"splines\"   \"stats\"     \"stats4\"    \"tcltk\"    \n[13] \"tools\"     \"utils\"    \n\n\n2. Recommended Packages: also a default package, mainly including additional more complex statistical procedures. These are 15 packages\n\nCoderownames(installed.packages(priority=\"recommended\"))\n\n [1] \"boot\"       \"class\"      \"cluster\"    \"codetools\"  \"foreign\"   \n [6] \"KernSmooth\" \"lattice\"    \"MASS\"       \"Matrix\"     \"mgcv\"      \n[11] \"nlme\"       \"nnet\"       \"rpart\"      \"spatial\"    \"survival\""
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#updating-r-and-rstudio",
    "href": "static/slides/Rintro/Day 1.html#updating-r-and-rstudio",
    "title": "",
    "section": "Updating R and RStudio",
    "text": "Updating R and RStudio\nUpdating R\n\nGo to CRAN and download new version\n\nMore efficient: install installr package, load it, and run updateR()\n\nUpdates R and Optionally updates all packages\nMay be better to do this in basic Rgui\n\n\n\nVersion should update automatically in RStudio\n\nCheck/change R version under Tools&gt;Global Options&gt;R version\n\n\n\nThen update the R packages with the code:\n\n\nCodeupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\n\n\nTo updating RStudio: Go to RStudio and download new version\nClick on Help&gt;Check for Updates, follow menu prompts"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#rstudio-options",
    "href": "static/slides/Rintro/Day 1.html#rstudio-options",
    "title": "",
    "section": "RStudio options",
    "text": "RStudio options\n\nRStudio has a number of useful options for changing it‚Äôs look and functionality. Let‚Äôs try these.\nYou may not understand all the changes made for now. That‚Äôs fine.\nIn the RStudio menu at the top of the screen, select Tools &gt; Global Options to bring up RStudio‚Äôs options.\nNow, under Appearance, choose your ideal theme. (We like the ‚ÄúCrimson Editor‚Äù and ‚ÄúTomorrow Night‚Äù themes.)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#command-palette",
    "href": "static/slides/Rintro/Day 1.html#command-palette",
    "title": "",
    "section": "Command palette",
    "text": "Command palette\n\nThe Rstudio command palette gives instant, searchable access to many of the RStudio menu options and settings that we have seen so far.\nThe palette can be invoked with the keyboard shortcut Ctrl + Shift + P.\nIt‚Äôs also available on the Tools menu (Tools -&gt; Show Command Palette).\n\n\nTry using it to:\n\nCreate a new script (Search ‚Äúnew script‚Äù and click on the relevant option)\nRename a script (Search ‚Äúrename‚Äù and click on the relevant option)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#wrapping-up",
    "href": "static/slides/Rintro/Day 1.html#wrapping-up",
    "title": "",
    "section": "Wrapping up",
    "text": "Wrapping up\n\nOf course, you have only scratched the surface of RStudio functionality and you can find more on the cheatsheet below:"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#working-with-r-objects",
    "href": "static/slides/Rintro/Day 1.html#working-with-r-objects",
    "title": "",
    "section": "3. Working with R Objects",
    "text": "3. Working with R Objects\nOrganize with an RStudio project\n\nIt is a good habit to immediately create a project for handling the analysis of new data and keep everything together.\nThe workspace is a working environment where R will store and remember user-defined objects: vectors, matrices, data frames, lists, variables, etc.\nTo Create an R project, go to\nFile &gt; New Project and then choose: New Directory&gt; Name for the directory &gt; Click on Create Project\nFor more complex projects it may be useful to create sub-directories to contain data, scripts, and other documents separately.\nYou can also type the below function into the Console, but we won‚Äôt do that in this session.\n\nprodigenr::setup_project(\"C:/Users/yebel/Desktop/LearningR\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#some-useful-functions-in-vector",
    "href": "static/slides/Rintro/Day 1.html#some-useful-functions-in-vector",
    "title": "",
    "section": "Some useful functions in vector",
    "text": "Some useful functions in vector\n\n\nclass(x): returns class/type of vector x\n\nlength(x): returns the total number of elements\n\nx[length(x)]: returns last value of vector x\n\nrev(x): returns reversed vector\n\nsort(x): returns sorted vector\n\nunique(x): returns vector without multiple elements\n\nrange(x): Range of x\n\nquantile(x): Quantiles of x for the given probabilities\n\nwhich.max(x): index of maximum\n\nwhich.min(x): index of minimum"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#factors",
    "href": "static/slides/Rintro/Day 1.html#factors",
    "title": "",
    "section": "Factors",
    "text": "Factors\n\nA factor is used to store predefined categorical data\nCan be ordered and unordered\n\ne.g.¬†:(‚Äúyes‚Äù, ‚Äúno‚Äù, ‚Äúno‚Äù, ‚Äúyes‚Äù, ‚Äúyes‚Äù), (‚Äúmale‚Äù, ‚Äúfemale‚Äù, ‚Äúfemale‚Äù, ‚Äúmale‚Äù)\n\n\nFactors can be created using factor()\n\n\n\nCodesize &lt;- factor(c(\"small\", \"large\", \"small\", \"medium\"))\n\n\n\nThe levels of a factor can be displayed using levels()."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#matrix",
    "href": "static/slides/Rintro/Day 1.html#matrix",
    "title": "",
    "section": "Matrix",
    "text": "Matrix\n\nMatrix is a rectangular array arranged in rows and columns.\nThe individual items in a matrix are called its elements or entries.\nMatrices can be created by:\n\n\nmatrix()\nconverting a vector into a matrix\nbinding together vectors\n\n\nMatrices can be created using the functions:\n\n\nmatrix() creates a matrix by specifying rows and columns\n\ndim() sets dimensions to a vector\n\ncbind combines columns\n\nrbind combines rows"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#data-frames",
    "href": "static/slides/Rintro/Day 1.html#data-frames",
    "title": "",
    "section": "Data frames",
    "text": "Data frames\n\na data set in R is stored a data frame.\nTwo-dimensional, arranged in rows and columns created using the function: data.frame()\ne.g.\n\n\nCodedf &lt;- data.frame(ID = 1:3, Sex = c(\"F\", \"F\", \"M\"), Age = c(17, 18,18))\n\n\n\nWe can enter data directly by access the editor using either the edit() or fix()\n\n\n\nCode new.data&lt;-data.frame()  # creates an \"empty\" data frame\n new.data&lt;-edit(new.data) # request the changes or  `fix(new.data)`\n\n\n\nWe‚Äôll use the data set called iris data to do this exploration.\n\n\nCodelibrary(readr)\ndata &lt;- read_csv(\"iris.csv\")\n#View(data)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#some-functions-for-inspecting-the-data",
    "href": "static/slides/Rintro/Day 1.html#some-functions-for-inspecting-the-data",
    "title": "",
    "section": "Some functions for inspecting the data",
    "text": "Some functions for inspecting the data\n\nUse head() and tail() to view the first (and last) five rows\nUse View() to view an entire data.table object\nUse str() to view the structure of data.table object\nUse tables() to show all loaded data.table objects\nUse colnames() or names() to look variable names\nUse colSums(is.na()) to sum missing data\nUse subset() to subset data.\nUse attributes() to look attributes of the dataframe\nUse dim() or ncol() and nrow() to see dimensions of the dataframe\nUse summary() to see basic statistics for each variables"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#subsetting",
    "href": "static/slides/Rintro/Day 1.html#subsetting",
    "title": "",
    "section": "Subsetting",
    "text": "Subsetting\n\nCodeiris[] # the whole data frame \niris[1, 1] # 1st element in 1st column \niris[1, 6] # 1st element in the 6th column \niris[, 1] # first column in the data frame \niris[1] # first column in the data frame \niris[1:3, 3] \niris[3, ] # the 3rd row \niris[1:6, ] # the 1st to 6th rows\niris[c(1,4), ] # rows 1 and 4 only \niris[c(1,4), c(1,3) ] \niris[, -1] # the whole except first column\niris$Variable1 # Also extracts the first column\niris[,c(\"col3\", \"col4\")]# extract by name of column"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#reading-and-writing-data",
    "href": "static/slides/Rintro/Day 1.html#reading-and-writing-data",
    "title": "",
    "section": "4. Reading and Writing data",
    "text": "4. Reading and Writing data\n\nImporting data is rather easy in R but that may also depend on the nature of the data to be imported and from what format.\nMost data are in tabular form such as a spreadsheet or a comma-separated file (.csv).\nBase R has a series of read functions to import tabular data from plain text files with columns delimited by: space, tab, and comma, with or without a header containing the column names.\nWith an added package it is also possible to import directly from a Microsoft Excel spreadsheet format or other foreign formats from various sources."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#importing-from-local-files",
    "href": "static/slides/Rintro/Day 1.html#importing-from-local-files",
    "title": "",
    "section": "Importing from local files",
    "text": "Importing from local files\n\nIn base R the standard commands to read text files are based on the read.table()function.\nThe following table lists the collection of the base R read functions.\nFor more details use the help command help(read.table) that will display help for all.\n\n\nDetails of dataset readings\n\nFunction name\nAssumes header\nSeparator\nDecimal\nFile type\n\n\n\nread.table()\nNo\n‚Äù ‚Äù\n.\n.text\n\n\nread.csv()\nYes\n‚Äú,‚Äù\n.\n.csv\n\n\nread.csv2()\nYes\n‚Äú;‚Äù\n,\n.csv\n\n\nread.delim()\nYes\n‚Äútab‚Äù\n.\n.text\n\n\nread.delim2()\nYes\n‚Äútab‚Äù\n,\n.text"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#reading-raw-data-from-other-sources",
    "href": "static/slides/Rintro/Day 1.html#reading-raw-data-from-other-sources",
    "title": "",
    "section": "Reading raw data from other sources",
    "text": "Reading raw data from other sources\nImport data\n\nThere are many ways to get data into R and out of R.\nImport text file data using read.table() and comma separated files using read.csv() functions.\n\n\nCode# syntax: \nread.table(\"file name with full path\", arguments)#&lt;&lt;\n\n\n\nCode# Examples:# Creates a data frame named myData\n  mydata&lt;- read.table(file = \"datafile.txt\",sep=\" \", header=TRUE)\n  mydata&lt;- read.csv(file = \"datafile.csv\")\n\n\n\nFile names are specified in the same way as file.choose() function can be used to select the file interactively. i.e.\n\n\nCodemydata &lt;-read.csv(file.choose(),sep=\",\",header=T)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#read-data",
    "href": "static/slides/Rintro/Day 1.html#read-data",
    "title": "",
    "section": "Read data",
    "text": "Read data\nStata to R: Different packages for stata version &gt;=13 vs. &lt;13\n\nCodelibrary(foreign) # Versions before stata 13\ndata &lt;-  read.dta(file=\"XXX.dta\") # Other options\n\n\n\nCodelibrary(readstata13) # Versions from stata 13\ndata &lt;-  read.dta13(file=\"XXX.dta\") # Other options\n\n\nExcel to R: There are several packages\n\nCodelibrary(readxl)\ndata &lt;-  read_xlsx(path=\"XXX.xlsx\", sheet = 1, col_names = TRUE)\n\n\n\nCodelibrary(readr)\ndata &lt;- read_csv(\"path=XXX.csv\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#read-data-1",
    "href": "static/slides/Rintro/Day 1.html#read-data-1",
    "title": "",
    "section": "Read data",
    "text": "Read data\nCSV to R: There are several packages\n\nCodelibrary(readr)\ndata &lt;- read_csv(file=\"XXX.csv\")\n\n\nText file to R: Available in R base, used for text and csv files\n\nCodedata &lt;- read.table(file=\"XXX.txt\",header=TRUE, sep=\"\")\n\n\n\nCodedata &lt;- read.table(file=\"XXX.csv\",header=TRUE, sep=\",\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#data-import-wizard",
    "href": "static/slides/Rintro/Day 1.html#data-import-wizard",
    "title": "",
    "section": "Data import wizard",
    "text": "Data import wizard\nThe data import wizard is a quick and easy way to import your data\n\nIt‚Äôs actually way better to follow the reproducible steps ‚Äì and hardly any more effort ‚Äì below‚Ä¶\n\nInside the data wizard, you can copy the code int he code-preview window, then paste the code into the code chunk of your quarto document or r script.\n\n\nCode# library(readr)\n#brodhead_center &lt;- read_csv(\"data/brodhead_center.csv\")\n# view(brodheadCenter)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#excel-spss-sas-etc.",
    "href": "static/slides/Rintro/Day 1.html#excel-spss-sas-etc.",
    "title": "",
    "section": "Excel, SPSS, SAS, etc.",
    "text": "Excel, SPSS, SAS, etc.\nThe data import wizard will help you find the proper package for importing your data. For example, use‚Ä¶\n\n\nlibrary(readxl) for Excel data\n\nlibrary(haven) for SPSS, SAS, Stata\n\nlirary(readr) for CSV or other delimeters\n\nJust start with File &gt; Import Dataset to get started composing that code, then paste your code into a script."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#exporting-data",
    "href": "static/slides/Rintro/Day 1.html#exporting-data",
    "title": "",
    "section": "Exporting Data",
    "text": "Exporting Data\nR to Stata: Use the libraries haven or readstata13\n\nCodewrite.dta(data,file=\"XXX.dta\")# Other options\nsave.dta13(data, file=\"XXX.dta\") # Other options\n\n\nR to Excel: Note the package readxl does not work here\n\nCodelibrary(xlsReadWrite)\nwrite.xls(data,\"data.xls\")# Other options\n\n\nR to csv: Use readr package\n\nCodewrite_csv(data, \"data.csv\", na = \"\")\n\n\nR to a text file:\n\nCodewrite.table(data, \"data.txt\", sep=\"\\t\")"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html",
    "href": "static/slides/Rintro/img/Day 1.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Take few minutes to introduce ourselves.\n\nPlease share ‚Ä¶\n\nYour name\nYour experience in R\nWhat you expect at end of the training\n\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#introductions",
    "href": "static/slides/Rintro/img/Day 1.html#introductions",
    "title": "Introduction to R",
    "section": "",
    "text": "Take few minutes to introduce ourselves.\n\nPlease share ‚Ä¶\n\nYour name\nYour experience in R\nWhat you expect at end of the training\n\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#section",
    "href": "static/slides/Rintro/img/Day 1.html#section",
    "title": "Introduction to R",
    "section": "",
    "text": "Outlines\nPart 1: Introduction/Overview of R and R Studio\n\nWorking with R: Objects and Workspace\n\nR Objects & Project Management\nGood Coding Practice\n\n\nPart 2: Data management using tidyverse package\nPart 3: Data visualization\nPart 4: Statistical Analysis\n\n\nSetting up and utilizing R & RStudio\nNavigating R & Rmarkdown scripts and RStudio projects\nImporting and inspecting data sets in R\nManipulating data through filtering, summarizing, transforming, and joining\nVisualizing data using the renowned ggplot2 package"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#why-r",
    "href": "static/slides/Rintro/img/Day 1.html#why-r",
    "title": "Introduction to R",
    "section": "Why R?",
    "text": "Why R?\n\nYour callout content goes here.\n\n\nIt is free, versatile, fast, and modern.\nIt has a large and friendly community of users that help answer questions and develop new R tools.\nWith more than 20394 add-on packages available, R offers more functions for data analysis than any other statistical software.\nR makes it easy to construct reproducible analyses and workflows that allow you to easily repeat the same analysis more than once.\nIt is flexible enough to be used to create interactive web pages (eg. my draft website) and automated reports.\nSimply currently the best tool there is for data analysis."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#what-are-advantages-or-disadvantages-of-r",
    "href": "static/slides/Rintro/img/Day 1.html#what-are-advantages-or-disadvantages-of-r",
    "title": "Introduction to R",
    "section": "What are Advantages or Disadvantages of R",
    "text": "What are Advantages or Disadvantages of R\n\n\nAdvantages\n\nAvailability and compatibility\nState of the art graphics capabilities\nCan import files from other (statistical) programs\nNew version every x months\nInteractive development environments (IDEs) available\nLarge users community\nreproducible research\n\n\nDrawbacks of R\n\nExpert friendly\nLearn by example\nNot very (easily) interactive\nCommand based\nDocumentation sometimes cryptic\n(Too) large amount of resources\nConstantly evolving\nMemory intensive and slow at times"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#rstudio",
    "href": "static/slides/Rintro/img/Day 1.html#rstudio",
    "title": "Introduction to R",
    "section": "2. RStudio",
    "text": "2. RStudio\n\nWhat is RStudio? Why use it?\n\nBest Integrated Development Environment (IDE) for R.\nPowerful and makes using R easier\nRStudio can:\n\nOrganize your code, output, and plots.\nAuto-complete code and highlight syntax.\nHelp view data and objects.\nEnable easy integration of R code into documents.\n\nUser-friendly interfaces"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#set-up-on-windows",
    "href": "static/slides/Rintro/img/Day 1.html#set-up-on-windows",
    "title": "Introduction to R",
    "section": "Set up on Windows",
    "text": "Set up on Windows\nDownload and install R\n\nFollow the steps below to download and install R:\n\n\nGo to cran.rstudio.com to access the R installation page. Then click the download link for Windows:\n\nChoose the ‚Äúbase‚Äù sub-directory.\n\n\n\n\nThen click on the download link at the top of the page to download the latest version of R:\n\n\n\n\n\nNote that the screenshot above may not show the latest version.\nAfter the download is finished, click on the downloaded file, then follow the instructions on the installation pop-up window. During installation, you should not have to change any of the defaults; just keep clicking ‚ÄúNext‚Äù until the installation is done.\nWell done! You should now have R on your computer. But you likely won‚Äôt ever need to interact with R directly. Instead you‚Äôll use the RStudio IDE to work with R. Follow the instructions in the next section to get RStudio.\n\n\n\nDownload, install & run RStudio\nTo download RStudio, go to rstudio.com/products/rstudio/download/#download and download the Windows version.\n\n\nThen click on the downloaded file and follow the installation instructions.\nNow Click to to open the app from the start menu:"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#rstudio-overview",
    "href": "static/slides/Rintro/img/Day 1.html#rstudio-overview",
    "title": "Introduction to R",
    "section": "RStudio Overview",
    "text": "RStudio Overview"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#getting-started",
    "href": "static/slides/Rintro/img/Day 1.html#getting-started",
    "title": "Introduction to R",
    "section": "Getting Started",
    "text": "Getting Started\n\nRStudio will open with 4 sections (called panes):\n\n\n1. Source editor pane**\n\nYou will write your R code/script here and it will be run in the console.\n\nTo create a new R script you can either go to File -&gt; New -&gt; R Script, or click on the icon with the + sign and select R Script, or simply press Ctrl+Shift+N.\nMake sure to save the script.\n\n\n\n\n\n2. Console pane\n\nInteractively run R commands\n\n\n\n3. Environment/history pane**\n\nEnvironment: view objects in the global environment\nHistory: search and view command history\n\n\n\n\n4. Files/Plots/Packages/Help pane**\n\nFiles: navigate directories and\nPlots: view generated plots.\nPackages: manage packages (install or update)\nHelp: View help documentations for any package/function"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#customization",
    "href": "static/slides/Rintro/img/Day 1.html#customization",
    "title": "Introduction to R",
    "section": "Customization",
    "text": "Customization\n\nPanes\n\nThe size and position of the panes can be customized.\nOn the top right of each pane, there are buttons to adjust the pane size.\nAlso, place your mouse pointer/cursor on the borderline between panes and when the pointer changes its shape, click and drag to adjust the pane size.\nFor more options, go to View &gt; Panes on the menu bar.\nAlternatively, try Tools &gt; Global Options &gt; Pane Layout.\n\n\n\nAppearances\n\nThe overall appearance can be customized as well.\nGo to Tools &gt; Global Options&gt; Appearance on the menu bar to change themes, fonts, and more."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#the-rstudio-panes",
    "href": "static/slides/Rintro/img/Day 1.html#the-rstudio-panes",
    "title": "Introduction to R",
    "section": "The RStudio panes",
    "text": "The RStudio panes\nBy default, RStudio is arranged into four window panes.\nIf you only see three panes, open a new script with File &gt; New File &gt; R Script . This should reveal one more pane.\n\nBefore we go any further, we will rearrange these panes to improve the usability of the interface.\n\n\nThen under Pane Layout, adjust the pane arrangement. The arrangement we recommend is shown below.\n\n\n\nAt the top left pane is the Source tab, and at the top right pane, you should have the Console tab.\nThen at the bottom left pane, no tab options should checked‚Äîthis section should be left empty, with the drop-down saying just ‚ÄúTabSet‚Äù.\nFinally, at the bottom right pane, you should check the following tabs: Environment, History, Files, Plots, Packages, Help and Viewer.\n\n\nFirst, open a new script under the File menu if one is not yet open: File &gt; New File &gt; R Script. In the script, type the following:\n\nprint(\"excited for R!\")\n\n\nTo run code, place your cursor anywhere in the code, then hit Control + Enter on Windows.\nThis should send the code to the Console and run it.\n\n\nYou can also run multiple lines at once.\n\nprint(\"excited for R!\")\nprint(\"and RStudio!\")\n\n\nNow drag your cursor to highlight both lines and press Control + Enter.\nTo run the entire script, you can use Control + A to select all code, then press `Control + Enter.\n\n\n\nTo open the script in a new window, click on the third icon in the toolbar directly above the script.\n\n\n\nTo put the window back, click on the same button on the now-external window.\nNext, save the script. Hit Control + S to bring up the Save dialog box.\n\n\n\nConsole\n\nThe console, at the bottom left, is where code is executed. You can type code directly here, but it will not be saved.\nType a random piece of code (maybe a calculation like 3 + 3) and press ‚ÄòEnter‚Äô.\n\n\n\nIf you place your cursor on the last line of the console, and you press the up arrow, you can go back to the last code that was run. Keep pressing it to cycle to the previous lines.\nTo run any of these previous lines, press Enter.\n\n\n\n\nEnvironment\n\n\nAt the top right of the RStudio Window, you should see the Environment tab.\nThe Environment tab shows datasets and other objects that are loaded into R‚Äôs working memory, or ‚Äúworkspace‚Äù.\nTo explore this tab, let‚Äôs import a dataset into your environment from R.\nType the code below into your script and run it:\n\n\ndata &lt;- iris\n\nYou have now imported the dataset and stored it in an object named data. (You could have named the object anything you want.)\n\nNow that the dataset is stored by R, you should be able to see it in the Environment pane.\nIf you click on the blue drop-down icon beside the object‚Äôs name in the Environment tab to reveal a summary.\n\n\n\nTry clicking directly on the data dataset from the Environment tab. This opens it in a ‚ÄòView‚Äô tab.\n\n\n\nThe broom icon, at the top of the Environment pane is used to clear your workspace.\n\n\n\nYou can also remove an object from the workspace with the rm() function.\nType and run the following in a new line on your R script.\n\n\nrm(data)\n\nNotice that the data object no longer shows up in your environment after having run that code.\n\n\n\nHistory\n\nNext, the History tab shows previous commands you have run.\n\n\n\nYou can click a line to highlight it, then send it to the console or to your script with the ‚ÄúTo Console‚Äù and ‚ÄúTo Source‚Äù icons at the top of this tab.\nTo select multiple lines, use the ‚ÄúShift-click‚Äù method: click the first item you want to select, then hold down the ‚ÄúShift‚Äù key and click the last item you want to select.\nFinally, notice that there is a search bar at the top right of the History pane where you can search for past commands that you have run.\n\n\n\n\nFiles\n\nNext, the Files tab. This shows the files and folders in the folder you are working in.\n\n\n\nThe tab allows you to interact with your computer‚Äôs file system.\nTry playing with some of the buttons here, to see what they do. You should try at least the following:\n\nMake a new folder\nDelete that folder\nMake a new R Script\nRename that script\n\n\n\n\n\nPlots\n\nNext, the Plots tab. This is where figures that are generated by R will show up.\nTry creating a simple plot with the following code:\n\n\nplot(women)\n\n\n\nThat code creates a plot of the two variables in the women dataset.\nYou should see this figure in the Plots tab.\nNow, test out the buttons at the top of this tab to explore what they do.\nIn particular, try to export a plot to your computer.\n\n\n\n\nPackages\n\nNext, let‚Äôs look at the Packages tab.\n\n\n\nPackages are collections of R code that extend the functionality of R.\nit is important to know that to use a package, you need to install then load it.\nPackages need to be installed only once, but must be loaded in each new R session.\nAll the package names you see (in blue font) are packages that are installed on your system. And packages with a checkmark are packages which are loaded in the current session."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#installing-and-loading-packages",
    "href": "static/slides/Rintro/img/Day 1.html#installing-and-loading-packages",
    "title": "Introduction to R",
    "section": "Installing and Loading Packages",
    "text": "Installing and Loading Packages\n\nPackages are collections of R functions, data, and compiled code in a well-defined format.\nThere are three categories of packages.\n\n1. Base Packages: Providing the basic functionality, maintained by the R Core Development group. Currently, there are 14 packages, these are\n\nrownames(installed.packages(priority=\"base\"))\n\n [1] \"base\"      \"compiler\"  \"datasets\"  \"graphics\"  \"grDevices\" \"grid\"     \n [7] \"methods\"   \"parallel\"  \"splines\"   \"stats\"     \"stats4\"    \"tcltk\"    \n[13] \"tools\"     \"utils\"    \n\n\n2. Recommended Packages: also a default package, mainly including additional more complex statistical procedures. These are 15 packages\n\nrownames(installed.packages(priority=\"recommended\"))\n\n [1] \"boot\"       \"class\"      \"cluster\"    \"codetools\"  \"foreign\"   \n [6] \"KernSmooth\" \"lattice\"    \"MASS\"       \"Matrix\"     \"mgcv\"      \n[11] \"nlme\"       \"nnet\"       \"rpart\"      \"spatial\"    \"survival\"  \n\n\n\n3. Contributed packages: Due to the open nature of R, anyone can contribute new packages at any time.\n\nCurrently, the CRAN package repository features 20394 available packages.\n\n\nnrow(available.packages())\n\n\nInstalling Packages\n\nOption 1: Menu\n\n\n\n\n\n\n\n\nOption 2: Packages Window\n\n\n\n\n\n\n\nOption 3: Code\n\n\ninstall.packages(\"readxl\") \n\n\n\nLoading Packages\n\nlibrary()   # see all packages installed\nsearch()    # see packages currently loaded"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#updating-r-and-rstudio",
    "href": "static/slides/Rintro/img/Day 1.html#updating-r-and-rstudio",
    "title": "Introduction to R",
    "section": "Updating R and RStudio",
    "text": "Updating R and RStudio\n\nUpdating R\n\nGo to CRAN and download new version\nMore efficient: install installr package, load it, and run updateR()\n\nUpdates R and Optionally updates all packages\nMay be better to do this in basic Rgui\n\nVersion should update automatically in RStudio\n\nCheck/change R version under Tools&gt;Global Options&gt;R version\n\nThen update the R packages with the code:\n\n\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\n\nTo updating RStudio: Go to RStudio and download new version\nClick on Help&gt;Check for Updates, follow menu prompts\n\n\n\n\nViewer\nNotice that the histogram above shows up in a Viewer tab. This tab allows you to preview HTML files and interactive objects.\n\n\nHelp\nLastly, the Help tab shows the documentation for different R objects. Try typing out and running each line below to see what this documentation looks like.\n\n?hchart\n?women\n?read.csv\n\n\n\nHelp files are not always very easy to understand for beginners, but with time they will become more useful."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#rstudio-options",
    "href": "static/slides/Rintro/img/Day 1.html#rstudio-options",
    "title": "Introduction to R",
    "section": "RStudio options",
    "text": "RStudio options\n\nRStudio has a number of useful options for changing it‚Äôs look and functionality. Let‚Äôs try these.\nYou may not understand all the changes made for now. That‚Äôs fine.\nIn the RStudio menu at the top of the screen, select Tools &gt; Global Options to bring up RStudio‚Äôs options.\nNow, under Appearance, choose your ideal theme. (We like the ‚ÄúCrimson Editor‚Äù and ‚ÄúTomorrow Night‚Äù themes.)\n\nUnder Code &gt; Display, check ‚ÄúHighlight R function calls‚Äù.\nWhat this does is give your R functions a unique color, improving readability.\nAlso under Code &gt; Display, check ‚ÄúRainbow parentheses‚Äù.\nWhat this does is make your ‚Äúnested parentheses‚Äù easier to read by giving each pair a unique color.\n\n\n\nFinally under General &gt; Basic, uncheck the box that says ‚ÄúRestore .RData into workspace at startup‚Äù.\nYou don‚Äôt want to restore any data to your workspace (or environment) when you start RStudio.\nStarting with a clean workspace each time is less likely to lead to errors.\nThis also means that you never want to ‚Äúsave your workspace to .RData on exit‚Äù, so set this to Never."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#command-palette",
    "href": "static/slides/Rintro/img/Day 1.html#command-palette",
    "title": "Introduction to R",
    "section": "Command palette",
    "text": "Command palette\n\nThe Rstudio command palette gives instant, searchable access to many of the RStudio menu options and settings that we have seen so far.\nThe palette can be invoked with the keyboard shortcut Ctrl + Shift + P.\nIt‚Äôs also available on the Tools menu (Tools -&gt; Show Command Palette).\n\n\nTry using it to:\n\nCreate a new script (Search ‚Äúnew script‚Äù and click on the relevant option)\nRename a script (Search ‚Äúrename‚Äù and click on the relevant option)"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#wrapping-up",
    "href": "static/slides/Rintro/img/Day 1.html#wrapping-up",
    "title": "Introduction to R",
    "section": "Wrapping up",
    "text": "Wrapping up\n\nOf course, you have only scratched the surface of RStudio functionality and you can find more on the cheatsheet below:"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#working-with-r-objects",
    "href": "static/slides/Rintro/img/Day 1.html#working-with-r-objects",
    "title": "Introduction to R",
    "section": "3. Working with R Objects",
    "text": "3. Working with R Objects\nOrganize with an RStudio project\n\nIt is a good habit to immediately create a project for handling the analysis of new data and keep everything together.\nThe workspace is a working environment where R will store and remember user-defined objects: vectors, matrices, data frames, lists, variables, etc.\nTo Create an R project, go to\nFile &gt; New Project and then choose: New Directory&gt; Name for the directory &gt; Click on Create Project\nFor more complex projects it may be useful to create sub-directories to contain data, scripts, and other documents separately.\nYou can also type the below function into the Console, but we won‚Äôt do that in this session.\n\nprodigenr::setup_project(\"C:/Users/yebel/Desktop/LearningR\")\n\n\nObjects in R\n\nR is an object-oriented programming platform.\nMeaning the entities that R creates and manipulates are known as objects.\nDuring an R session, objects are created and stored by name.\n\n\nobjects() # lists all objects\nls() # lists all objects alternatively\nrm(x) #removes an object  \n\n\nObjects can be created in the form of\n\nvariable &lt;- value or variable = value or variable -&gt; value.\nVariable names can be letters, numbers, and the dot or underline characters but not dot followed by numbers .4youis illegal).\n\n\n\n\nthe symbol &lt;- (Alt + -) that could be read as assign or place into or read in etc.\n\n\n# need to placed in quotes as diabetic is string.\nA &lt;-\"Diabetic\"#&lt;&lt; \n\n\nThe standard data objects in R are: scalars, vectors, factors, matrices and arrays, lists, and data frames.\nData types assigned to each objects are: logical, numeric, integer, character, complex.\n\n\n\nVector\n\nA set of scalars arranged in a one-dimensional array.\nData values are all the same mode(data type), but can hold any mode.\n\ne.g:(-2, 3.4, 3), (TRUE, FALSE, TRUE), (‚Äúblue‚Äù, ‚Äúgray‚Äù, ‚Äúred‚Äù)\n\nVectors can be created using the following functions:\nc() function to combine individual values\n\nx &lt;- c(10.4, 5.6, 3.1, 6.4, 21.7)\n\nseq() to create more complex sequences\n\nseq(from=1, to=10, by=2) or seq(1,10 )\n\nrep() to create replicates of values: rep(1:4, times=2, each=2)"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#some-useful-functions-in-vector",
    "href": "static/slides/Rintro/img/Day 1.html#some-useful-functions-in-vector",
    "title": "Introduction to R",
    "section": "Some useful functions in vector",
    "text": "Some useful functions in vector\n\nclass(x): returns class/type of vector x\nlength(x): returns the total number of elements\nx[length(x)]: returns last value of vector x\nrev(x): returns reversed vector\nsort(x): returns sorted vector\nunique(x): returns vector without multiple elements\nrange(x): Range of x\nquantile(x): Quantiles of x for the given probabilities\nwhich.max(x): index of maximum\nwhich.min(x): index of minimum"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#factors",
    "href": "static/slides/Rintro/img/Day 1.html#factors",
    "title": "Introduction to R",
    "section": "Factors",
    "text": "Factors\n\nA factor is used to store predefined categorical data\nCan be ordered and unordered\n\ne.g.¬†:(‚Äúyes‚Äù, ‚Äúno‚Äù, ‚Äúno‚Äù, ‚Äúyes‚Äù, ‚Äúyes‚Äù), (‚Äúmale‚Äù, ‚Äúfemale‚Äù, ‚Äúfemale‚Äù, ‚Äúmale‚Äù)\n\nFactors can be created using factor()\n\n\nsize &lt;- factor(c(\"small\", \"large\", \"small\", \"medium\"))\n\n\nThe levels of a factor can be displayed using levels()."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#matrix",
    "href": "static/slides/Rintro/img/Day 1.html#matrix",
    "title": "Introduction to R",
    "section": "Matrix",
    "text": "Matrix\n\nMatrix is a rectangular array arranged in rows and columns.\nThe individual items in a matrix are called its elements or entries.\nMatrices can be created by:\n\n\nmatrix()\nconverting a vector into a matrix\nbinding together vectors\n\n\nMatrices can be created using the functions:\n\nmatrix() creates a matrix by specifying rows and columns\ndim() sets dimensions to a vector\ncbind combines columns\nrbind combines rows\n\n\n\ne.g.\n\nm1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nrownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#data-frames",
    "href": "static/slides/Rintro/img/Day 1.html#data-frames",
    "title": "Introduction to R",
    "section": "Data frames",
    "text": "Data frames\n\na data set in R is stored a data frame.\nTwo-dimensional, arranged in rows and columns created using the function: data.frame()\ne.g.\n\n\ndf &lt;- data.frame(ID = 1:3, Sex = c(\"F\", \"F\", \"M\"), Age = c(17, 18,18))\n\n\nWe can enter data directly by access the editor using either the edit() or fix()\n\n\n new.data&lt;-data.frame()  # creates an \"empty\" data frame\n new.data&lt;-edit(new.data) # request the changes or  `fix(new.data)`\n\n\nWe‚Äôll use the data set called iris data to do this exploration.\n\n\nlibrary(readr)\ndata &lt;- read_csv(\"iris.csv\")\n#View(data)"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#some-functions-for-inspecting-the-data",
    "href": "static/slides/Rintro/img/Day 1.html#some-functions-for-inspecting-the-data",
    "title": "Introduction to R",
    "section": "Some functions for inspecting the data",
    "text": "Some functions for inspecting the data\n\nUse head() and tail() to view the first (and last) five rows\nUse View() to view an entire data.table object\nUse str() to view the structure of data.table object\nUse tables() to show all loaded data.table objects\nUse colnames() or names() to look variable names\nUse colSums(is.na()) to sum missing data\nUse subset() to subset data.\nUse attributes() to look attributes of the dataframe\nUse dim() or ncol() and nrow() to see dimensions of the dataframe\nUse summary() to see basic statistics for each variables"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#subsetting",
    "href": "static/slides/Rintro/img/Day 1.html#subsetting",
    "title": "Introduction to R",
    "section": "Subsetting",
    "text": "Subsetting\n\niris[] # the whole data frame \niris[1, 1] # 1st element in 1st column \niris[1, 6] # 1st element in the 6th column \niris[, 1] # first column in the data frame \niris[1] # first column in the data frame \niris[1:3, 3] \niris[3, ] # the 3rd row \niris[1:6, ] # the 1st to 6th rows\niris[c(1,4), ] # rows 1 and 4 only \niris[c(1,4), c(1,3) ] \niris[, -1] # the whole except first column\niris$Variable1 # Also extracts the first column\niris[,c(\"col3\", \"col4\")]# extract by name of column"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#reading-and-writing-data",
    "href": "static/slides/Rintro/img/Day 1.html#reading-and-writing-data",
    "title": "Introduction to R",
    "section": "4. Reading and Writing data",
    "text": "4. Reading and Writing data\n\nImporting data is rather easy in R but that may also depend on the nature of the data to be imported and from what format.\nMost data are in tabular form such as a spreadsheet or a comma-separated file (.csv).\nBase R has a series of read functions to import tabular data from plain text files with columns delimited by: space, tab, and comma, with or without a header containing the column names.\nWith an added package it is also possible to import directly from a Microsoft Excel spreadsheet format or other foreign formats from various sources."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#importing-from-local-files",
    "href": "static/slides/Rintro/img/Day 1.html#importing-from-local-files",
    "title": "Introduction to R",
    "section": "Importing from local files",
    "text": "Importing from local files\n\nIn base R the standard commands to read text files are based on the read.table()function.\nThe following table lists the collection of the base R read functions.\nFor more details use the help command help(read.table) that will display help for all.\n\n\nDetails of dataset readings\n\n\nFunction name\nAssumes header\nSeparator\nDecimal\nFile type\n\n\n\n\nread.table()\nNo\n‚Äù ‚Äù\n.\n.text\n\n\nread.csv()\nYes\n‚Äú,‚Äù\n.\n.csv\n\n\nread.csv2()\nYes\n‚Äú;‚Äù\n,\n.csv\n\n\nread.delim()\nYes\n‚Äútab‚Äù\n.\n.text\n\n\nread.delim2()\nYes\n‚Äútab‚Äù\n,\n.text"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#reading-raw-data-from-other-sources",
    "href": "static/slides/Rintro/img/Day 1.html#reading-raw-data-from-other-sources",
    "title": "Introduction to R",
    "section": "Reading raw data from other sources",
    "text": "Reading raw data from other sources\n\nImport data\n\nThere are many ways to get data into R and out of R.\nImport text file data using read.table() and comma separated files using read.csv() functions.\n\n\n# syntax: \nread.table(\"file name with full path\", arguments)#&lt;&lt;\n\n\n# Examples:# Creates a data frame named myData\n  mydata&lt;- read.table(file = \"datafile.txt\",sep=\" \", header=TRUE)\n  mydata&lt;- read.csv(file = \"datafile.csv\")\n\n\nFile names are specified in the same way as file.choose() function can be used to select the file interactively. i.e.\n\n\nmydata &lt;-read.csv(file.choose(),sep=\",\",header=T)\n\n\nUseful arguments - Check these arguments carefully when you load your data\n\nmyData&lt;-read.csv(file=\"datafile.csv\",header= TRUE, sep=\",\", strip.white =TRUE,\n                 na.strings= \" \")\n\n\nYou can reduce possible errors when loading a data file\nThe header = TRUE argument tells R that the first row of your file contains the variable names\nThe sep =  \",\" argument tells R that fields are separated by comma\nThe strip.white = TRUE argument removes white space before or after factors that has been mistakenly inserted during data entry.\nThe na.strings = \" \" argument replaces empty cells by NA (missing data in R)"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#read-data",
    "href": "static/slides/Rintro/img/Day 1.html#read-data",
    "title": "Introduction to R",
    "section": "Read data",
    "text": "Read data\nStata to R: Different packages for stata version &gt;=13 vs. &lt;13\n\nlibrary(foreign) # Versions before stata 13\ndata &lt;-  read.dta(file=\"XXX.dta\") # Other options\n\n\nlibrary(readstata13) # Versions from stata 13\ndata &lt;-  read.dta13(file=\"XXX.dta\") # Other options\n\nExcel to R: There are several packages\n\nlibrary(readxl)\ndata &lt;-  read_xlsx(path=\"XXX.xlsx\", sheet = 1, col_names = TRUE)\n\n\nlibrary(readr)\ndata &lt;- read_csv(\"path=XXX.csv\")"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#read-data-1",
    "href": "static/slides/Rintro/img/Day 1.html#read-data-1",
    "title": "Introduction to R",
    "section": "Read data",
    "text": "Read data\nCSV to R: There are several packages\n\nlibrary(readr)\ndata &lt;- read_csv(file=\"XXX.csv\")\n\nText file to R: Available in R base, used for text and csv files\n\ndata &lt;- read.table(file=\"XXX.txt\",header=TRUE, sep=\"\")\n\n\ndata &lt;- read.table(file=\"XXX.csv\",header=TRUE, sep=\",\")"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#data-import-wizard",
    "href": "static/slides/Rintro/img/Day 1.html#data-import-wizard",
    "title": "Introduction to R",
    "section": "Data import wizard",
    "text": "Data import wizard\nThe data import wizard is a quick and easy way to import your data\n\nIt‚Äôs actually way better to follow the reproducible steps ‚Äì and hardly any more effort ‚Äì below‚Ä¶\n\nInside the data wizard, you can copy the code int he code-preview window, then paste the code into the code chunk of your quarto document or r script.\n\n\n# library(readr)\n#brodhead_center &lt;- read_csv(\"data/brodhead_center.csv\")\n# view(brodheadCenter)\n\n\n\n\n\n\n\n\nComposing the data import code‚Ä¶\n\n\n\nWriting the import data function can be tricky. Try the import wizard pictured above. THEN, paste the code from the Code Preview section into your script.\n\n\n\nEasily write import data function"
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#excel-spss-sas-etc.",
    "href": "static/slides/Rintro/img/Day 1.html#excel-spss-sas-etc.",
    "title": "Introduction to R",
    "section": "Excel, SPSS, SAS, etc.",
    "text": "Excel, SPSS, SAS, etc.\nThe data import wizard will help you find the proper package for importing your data. For example, use‚Ä¶\n\nlibrary(readxl) for Excel data\nlibrary(haven) for SPSS, SAS, Stata\nlirary(readr) for CSV or other delimeters\n\nJust start with File &gt; Import Dataset to get started composing that code, then paste your code into a script."
  },
  {
    "objectID": "static/slides/Rintro/img/Day 1.html#exporting-data",
    "href": "static/slides/Rintro/img/Day 1.html#exporting-data",
    "title": "Introduction to R",
    "section": "Exporting Data",
    "text": "Exporting Data\nR to Stata: Use the libraries haven or readstata13\n\nwrite.dta(data,file=\"XXX.dta\")# Other options\nsave.dta13(data, file=\"XXX.dta\") # Other options\n\nR to Excel: Note the package readxl does not work here\n\nlibrary(xlsReadWrite)\nwrite.xls(data,\"data.xls\")# Other options\n\nR to csv: Use readr package\n\nwrite_csv(data, \"data.csv\", na = \"\")\n\nR to a text file:\n\nwrite.table(data, \"data.txt\", sep=\"\\t\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-1",
    "href": "static/slides/Rintro/Day 1.html#section-1",
    "title": "",
    "section": "",
    "text": "Option 2: Packages Window\n\n\n\n\n\nOption 3: Code\n\n\n\nCodeinstall.packages(\"readxl\") \n\n\nLoading Packages\n\nCodelibrary()   # see all packages installed\nsearch()    # see packages currently loaded"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-2",
    "href": "static/slides/Rintro/Day 1.html#section-2",
    "title": "",
    "section": "",
    "text": "the symbol &lt;- (Alt + -) that could be read as assign or place into or read in etc.\n\n\nCode# need to placed in quotes as diabetic is string.\nA &lt;-\"Diabetic\"#&lt;&lt; \n\n\n\nThe standard data objects in R are: scalars, vectors, factors, matrices and arrays, lists, and data frames.\nData types assigned to each objects are: logical, numeric, integer, character, complex."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#download-install-run-rstudio",
    "href": "static/slides/Rintro/Day 1.html#download-install-run-rstudio",
    "title": "",
    "section": "Download, install & run RStudio",
    "text": "Download, install & run RStudio\nTo download RStudio, go to posit.co/download/rstudio-desktop and download the Windows version.\n\n\n\n\n\nThen click on the downloaded file and follow the installation instructions."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-3",
    "href": "static/slides/Rintro/Day 1.html#section-3",
    "title": "",
    "section": "",
    "text": "e.g.\n\nCodem1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nCoderownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-4",
    "href": "static/slides/Rintro/Day 1.html#section-4",
    "title": "",
    "section": "",
    "text": "the symbol &lt;- (Alt + -) that could be read as assign or place into or read in etc.\n\n\nCode# need to placed in quotes as diabetic is string.\nA &lt;-\"Diabetic\"#&lt;&lt; \n\n\n\nThe standard data objects in R are: scalars, vectors, factors, matrices and arrays, lists, and data frames.\nData types assigned to each objects are: logical, numeric, integer, character, complex."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-5",
    "href": "static/slides/Rintro/Day 1.html#section-5",
    "title": "",
    "section": "",
    "text": "e.g.\n\nCodem1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nCoderownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-6",
    "href": "static/slides/Rintro/Day 1.html#section-6",
    "title": "",
    "section": "",
    "text": "the symbol &lt;- (Alt + -) that could be read as assign or place into or read in etc.\n\n\nCode# need to placed in quotes as diabetic is string.\nA &lt;-\"Diabetic\"#&lt;&lt; \n\n\n\nThe standard data objects in R are: scalars, vectors, factors, matrices and arrays, lists, and data frames.\nData types assigned to each objects are: logical, numeric, integer, character, complex."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#console",
    "href": "static/slides/Rintro/Day 1.html#console",
    "title": "",
    "section": "Console",
    "text": "Console\n\nThe console, at the bottom left, is where code is executed. You can type code directly here, but it will not be saved.\nType a random piece of code (maybe a calculation like 3 + 3) and press ‚ÄòEnter‚Äô.\n\n\n\nIf you place your cursor on the last line of the console, and you press the up arrow, you can go back to the last code that was run. Keep pressing it to cycle to the previous lines.\nTo run any of these previous lines, press Enter."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#environment",
    "href": "static/slides/Rintro/Day 1.html#environment",
    "title": "",
    "section": "Environment",
    "text": "Environment\n\n\nAt the top right of the RStudio Window, you should see the Environment tab.\nThe Environment tab shows datasets and other objects that are loaded into R‚Äôs working memory, or ‚Äúworkspace‚Äù.\nTo explore this tab, let‚Äôs import a dataset into your environment from R.\nType the code below into your script and run it:\n\n\nCodedata &lt;- iris\n\n\nYou have now imported the dataset and stored it in an object named data. (You could have named the object anything you want.)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-7",
    "href": "static/slides/Rintro/Day 1.html#section-7",
    "title": "",
    "section": "",
    "text": "e.g.\n\nCodem1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nCoderownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-8",
    "href": "static/slides/Rintro/Day 1.html#section-8",
    "title": "",
    "section": "",
    "text": "Useful arguments - Check these arguments carefully when you load your data\n\nCodemyData&lt;-read.csv(file=\"datafile.csv\",header= TRUE, sep=\",\", strip.white =TRUE,\n                 na.strings= \" \")\n\n\n\nYou can reduce possible errors when loading a data file\nThe header = TRUE argument tells R that the first row of your file contains the variable names\nThe sep =  \",\" argument tells R that fields are separated by comma\nThe strip.white = TRUE argument removes white space before or after factors that has been mistakenly inserted during data entry.\nThe na.strings = \" \" argument replaces empty cells by NA (missing data in R)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#history",
    "href": "static/slides/Rintro/Day 1.html#history",
    "title": "",
    "section": "History",
    "text": "History\n\nNext, the History tab shows previous commands you have run.\n\n\n\nYou can click a line to highlight it, then send it to the console or to your script with the ‚ÄúTo Console‚Äù and ‚ÄúTo Source‚Äù icons at the top of this tab.\nTo select multiple lines, use the ‚ÄúShift-click‚Äù method: click the first item you want to select, then hold down the ‚ÄúShift‚Äù key and click the last item you want to select.\nFinally, notice that there is a search bar at the top right of the History pane where you can search for past commands that you have run."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#files",
    "href": "static/slides/Rintro/Day 1.html#files",
    "title": "",
    "section": "Files",
    "text": "Files\n\nNext, the Files tab. This shows the files and folders in the folder you are working in.\n\n\n\nThe tab allows you to interact with your computer‚Äôs file system.\n\nTry playing with some of the buttons here, to see what they do. You should try at least the following:\n\nMake a new folder\nDelete that folder\nMake a new R Script\nRename that script"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#plots",
    "href": "static/slides/Rintro/Day 1.html#plots",
    "title": "",
    "section": "Plots",
    "text": "Plots\n\nNext, the Plots tab. This is where figures that are generated by R will show up.\nTry creating a simple plot with the following code:\n\n\nCodeplot(women)\n\n\n\n\nThat code creates a plot of the two variables in the women dataset.\nYou should see this figure in the Plots tab.\nNow, test out the buttons at the top of this tab to explore what they do.\nIn particular, try to export a plot to your computer."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#packages",
    "href": "static/slides/Rintro/Day 1.html#packages",
    "title": "",
    "section": "Packages",
    "text": "Packages\n\nNext, let‚Äôs look at the Packages tab.\n\n\n\n\n\n\nPackages are collections of R code that extend the functionality of R.\nit is important to know that to use a package, you need to install then load it.\nPackages need to be installed only once, but must be loaded in each new R session.\nAll the package names you see (in blue font) are packages that are installed on your system. And packages with a checkmark are packages which are loaded in the current session."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-9",
    "href": "static/slides/Rintro/Day 1.html#section-9",
    "title": "",
    "section": "",
    "text": "Useful arguments - Check these arguments carefully when you load your data\n\nCodemyData&lt;-read.csv(file=\"datafile.csv\",header= TRUE, sep=\",\", strip.white =TRUE,\n                 na.strings= \" \")\n\n\n\nYou can reduce possible errors when loading a data file\nThe header = TRUE argument tells R that the first row of your file contains the variable names\nThe sep =  \",\" argument tells R that fields are separated by comma\nThe strip.white = TRUE argument removes white space before or after factors that has been mistakenly inserted during data entry.\nThe na.strings = \" \" argument replaces empty cells by NA (missing data in R)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-10",
    "href": "static/slides/Rintro/Day 1.html#section-10",
    "title": "",
    "section": "",
    "text": "e.g.\n\nCodem1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nCoderownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-11",
    "href": "static/slides/Rintro/Day 1.html#section-11",
    "title": "",
    "section": "",
    "text": "Useful arguments - Check these arguments carefully when you load your data\n\nCodemyData&lt;-read.csv(file=\"datafile.csv\",header= TRUE, sep=\",\", strip.white =TRUE,\n                 na.strings= \" \")\n\n\n\nYou can reduce possible errors when loading a data file\nThe header = TRUE argument tells R that the first row of your file contains the variable names\nThe sep =  \",\" argument tells R that fields are separated by comma\nThe strip.white = TRUE argument removes white space before or after factors that has been mistakenly inserted during data entry.\nThe na.strings = \" \" argument replaces empty cells by NA (missing data in R)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-12",
    "href": "static/slides/Rintro/Day 1.html#section-12",
    "title": "",
    "section": "",
    "text": "Under Code &gt; Display, check ‚ÄúHighlight R function calls‚Äù.\nWhat this does is give your R functions a unique color, improving readability.\nAlso under Code &gt; Display, check ‚ÄúRainbow parentheses‚Äù.\n\nWhat this does is make your ‚Äúnested parentheses‚Äù easier to read by giving each pair a unique color."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#objects-in-r",
    "href": "static/slides/Rintro/Day 1.html#objects-in-r",
    "title": "",
    "section": "Objects in R",
    "text": "Objects in R\n\nR is an object-oriented programming platform.\nMeaning the entities that R creates and manipulates are known as objects.\nDuring an R session, objects are created and stored by name.\n\n\nCodeobjects() # lists all objects\nls() # lists all objects alternatively\nrm(x) #removes an object  \n\n\n\nObjects can be created in the form of\n\n\nvariable &lt;- value or variable = value or variable -&gt; value.\nVariable names can be letters, numbers, and the dot or underline characters but not dot followed by numbers .4youis illegal)."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-13",
    "href": "static/slides/Rintro/Day 1.html#section-13",
    "title": "",
    "section": "",
    "text": "Finally under General &gt; Basic, uncheck the box that says ‚ÄúRestore .RData into workspace at startup‚Äù.\nYou don‚Äôt want to restore any data to your workspace (or environment) when you start RStudio.\nStarting with a clean workspace each time is less likely to lead to errors.\nThis also means that you never want to ‚Äúsave your workspace to .RData on exit‚Äù, so set this to Never."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#vector",
    "href": "static/slides/Rintro/Day 1.html#vector",
    "title": "",
    "section": "Vector",
    "text": "Vector\n\nA set of scalars arranged in a one-dimensional array.\n\nData values are all the same mode(data type), but can hold any mode.\n\ne.g:(-2, 3.4, 3), (TRUE, FALSE, TRUE), (‚Äúblue‚Äù, ‚Äúgray‚Äù, ‚Äúred‚Äù)\n\n\nVectors can be created using the following functions:\n\nc() function to combine individual values\n\nx &lt;- c(10.4, 5.6, 3.1, 6.4, 21.7)\n\n\n\nseq() to create more complex sequences\n\nseq(from=1, to=10, by=2) or seq(1,10 )\n\n\nrep() to create replicates of values: rep(1:4, times=2, each=2)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-14",
    "href": "static/slides/Rintro/Day 1.html#section-14",
    "title": "",
    "section": "",
    "text": "the symbol &lt;- (Alt + -) that could be read as assign or place into or read in etc.\n\n\nCode# need to placed in quotes as diabetic is string.\nA &lt;-\"Diabetic\"#&lt;&lt; \n\n\n\nThe standard data objects in R are: scalars, vectors, factors, matrices and arrays, lists, and data frames.\nData types assigned to each objects are: logical, numeric, integer, character, complex."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-15",
    "href": "static/slides/Rintro/Day 1.html#section-15",
    "title": "",
    "section": "",
    "text": "e.g.\n\nCodem1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nCoderownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#section-16",
    "href": "static/slides/Rintro/Day 1.html#section-16",
    "title": "",
    "section": "",
    "text": "Useful arguments - Check these arguments carefully when you load your data\n\nCodemyData&lt;-read.csv(file=\"datafile.csv\",header= TRUE, sep=\",\", strip.white =TRUE,\n                 na.strings= \" \")\n\n\n\nYou can reduce possible errors when loading a data file\nThe header = TRUE argument tells R that the first row of your file contains the variable names\nThe sep =  \",\" argument tells R that fields are separated by comma\nThe strip.white = TRUE argument removes white space before or after factors that has been mistakenly inserted during data entry.\nThe na.strings = \" \" argument replaces empty cells by NA (missing data in R)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#download-install-run-rstudio-1",
    "href": "static/slides/Rintro/Day 1.html#download-install-run-rstudio-1",
    "title": "",
    "section": "Download, install & run RStudio",
    "text": "Download, install & run RStudio\n\nNow Click to to open the app from the start menu:"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#the-rstudio-panes-1",
    "href": "static/slides/Rintro/Day 1.html#the-rstudio-panes-1",
    "title": "",
    "section": "The RStudio panes",
    "text": "The RStudio panes\n\nAt the top left pane is the Source tab, and at the top right pane, you should have the Console tab.\nThen at the bottom left pane, no tab options should checked‚Äîthis section should be left empty, with the drop-down saying just ‚ÄúTabSet‚Äù.\nFinally, at the bottom right pane, you should check the following tabs: Environment, History, Files, Plots, Packages, Help and Viewer."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#the-rstudio-panes-2",
    "href": "static/slides/Rintro/Day 1.html#the-rstudio-panes-2",
    "title": "",
    "section": "The RStudio panes",
    "text": "The RStudio panes\nFirst, open a new script under the File menu if one is not yet open: File &gt; New File &gt; R Script. In the script, type the following:\n\nCodeprint(\"excited for R!\")\n\n\n\nTo run code, place your cursor anywhere in the code, then hit Control + Enter on Windows.\nThis should send the code to the Console and run it.\n\nYou can also run multiple lines at once.\n\nCodeprint(\"excited for R!\")\nprint(\"and RStudio!\")\n\n\n\nNow drag your cursor to highlight both lines and press Control + Enter.\nTo run the entire script, you can use Control + A to select all code, then press `Control + Enter."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#the-rstudio-panes-3",
    "href": "static/slides/Rintro/Day 1.html#the-rstudio-panes-3",
    "title": "",
    "section": "The RStudio panes",
    "text": "The RStudio panes\n\nTo open the script in a new window, click on the third icon in the toolbar directly above the script.\n\n\n\nTo put the window back, click on the same button on the now-external window.\nNext, save the script. Hit Control + S to bring up the Save dialog box."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#sec-outlines",
    "href": "static/slides/Rintro/Day 1.html#sec-outlines",
    "title": "",
    "section": "Outlines",
    "text": "Outlines\n\nPart 1: Introduction/Overview of R and R Studio\n\n\nWorking with R: Objects and Workspace\n\nR Objects & Project Management\nGood Coding Practice\n\n\n\nPart 2: Data management using tidyverse package\nPart 3: Data visualization\nPart 4: Statistical Analysis"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#getting-started-1",
    "href": "static/slides/Rintro/Day 1.html#getting-started-1",
    "title": "",
    "section": "Getting Started",
    "text": "Getting Started\n\nRStudio will open with 4 sections (called panes):\n\n1. Source editor pane\n\nYou will write your R code/script here and it will be run in the console.\n\nTo create a new R script you can either go to File -&gt; New -&gt; R Script, or click on the icon with the + sign and select R Script, or simply press Ctrl+Shift+N.\nMake sure to save the script.\n\n\n\n2. Console pane\n\nInteractively run R commands\n\n3. Environment/history pane\n\n\nEnvironment: view objects in the global environment\n\nHistory: search and view command history\n\n\n4. Files/Plots/Packages/Help pane\n\n\nFiles: navigate directories and\n\nPlots: view generated plots.\n\nPackages: manage packages (install or update)\n\nHelp: View help documentations for any package/function"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#set-up-on-windows-1",
    "href": "static/slides/Rintro/Day 1.html#set-up-on-windows-1",
    "title": "",
    "section": "Set up on Windows",
    "text": "Set up on Windows\n\nThen click on the download link at the top of the page to download the latest version of R:\n\n\nNote that the screenshot above may not show the latest version.\n\nAfter the download is finished, click on the downloaded file, then follow the instructions on the installation pop-up window.\n\n\nDuring installation, you should not have to change any of the defaults; just keep clicking ‚ÄúNext‚Äù until the installation is done."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#environment-1",
    "href": "static/slides/Rintro/Day 1.html#environment-1",
    "title": "",
    "section": "Environment",
    "text": "Environment\n\nNow that the dataset is stored by R, you should be able to see it in the Environment pane.\nIf you click on the blue drop-down icon beside the object‚Äôs name in the Environment tab to reveal a summary.\n\n\n\nTry clicking directly on the data dataset from the Environment tab. This opens it in a ‚ÄòView‚Äô tab."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#environment-2",
    "href": "static/slides/Rintro/Day 1.html#environment-2",
    "title": "",
    "section": "Environment",
    "text": "Environment\n\nThe broom icon, at the top of the Environment pane is used to clear your workspace.\n\n\n\nYou can also remove an object from the workspace with the rm() function.\nType and run the following in a new line on your R script.\n\n\nCoderm(data)\n\n\nNotice that the data object no longer shows up in your environment after having run that code."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#reading-raw-data-from-other-sources-1",
    "href": "static/slides/Rintro/Day 1.html#reading-raw-data-from-other-sources-1",
    "title": "",
    "section": "Reading raw data from other sources",
    "text": "Reading raw data from other sources\nUseful arguments - Check these arguments carefully when you load your data\n\nCodemyData&lt;-read.csv(file=\"datafile.csv\",header= TRUE, sep=\",\", strip.white =TRUE,\n                 na.strings= \" \")\n\n\n\nYou can reduce possible errors when loading a data file\nThe header = TRUE argument tells R that the first row of your file contains the variable names\nThe sep =  \",\" argument tells R that fields are separated by comma\nThe strip.white = TRUE argument removes white space before or after factors that has been mistakenly inserted during data entry.\nThe na.strings = \" \" argument replaces empty cells by NA (missing data in R)"
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#rstudio-options-1",
    "href": "static/slides/Rintro/Day 1.html#rstudio-options-1",
    "title": "",
    "section": "RStudio options",
    "text": "RStudio options\n\nUnder Code &gt; Display, check ‚ÄúHighlight R function calls‚Äù.\nWhat this does is give your R functions a unique color, improving readability.\nAlso under Code &gt; Display, check ‚ÄúRainbow parentheses‚Äù.\nWhat this does is make your ‚Äúnested parentheses‚Äù easier to read by giving each pair a unique color."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#rstudio-options-2",
    "href": "static/slides/Rintro/Day 1.html#rstudio-options-2",
    "title": "",
    "section": "RStudio options",
    "text": "RStudio options\n\n\nFinally under General &gt; Basic, uncheck the box that says ‚ÄúRestore .RData into workspace at startup‚Äù.\nYou don‚Äôt want to restore any data to your workspace (or environment) when you start RStudio.\nStarting with a clean workspace each time is less likely to lead to errors.\nThis also means that you never want to ‚Äúsave your workspace to .RData on exit‚Äù, so set this to Never."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#objects-in-r-1",
    "href": "static/slides/Rintro/Day 1.html#objects-in-r-1",
    "title": "",
    "section": "Objects in R",
    "text": "Objects in R\n\nthe symbol &lt;- (Alt + -) that could be read as assign or place into or read in etc.\n\n\nCode# need to placed in quotes as diabetic is string.\nA &lt;-\"Diabetic\"#&lt;&lt; \n\n\n\nThe standard data objects in R are: scalars, vectors, factors, matrices and arrays, lists, and data frames.\nData types assigned to each objects are: logical, numeric, integer, character, complex."
  },
  {
    "objectID": "static/slides/Rintro/Day 1.html#matrix-1",
    "href": "static/slides/Rintro/Day 1.html#matrix-1",
    "title": "",
    "section": "Matrix",
    "text": "Matrix\ne.g.\n\nCodem1&lt;-matrix(data = 1:6, nrow = 3, ncol = 2)\nm2&lt;-cbind(1:3,5:7,10:12)\nx=1:6\ndim(x) &lt;- c(2, 3)\n\n\n\nNote: dim() can also be used to retrieve dimensions of an object!\n\nAssign names to rows and columns of a matrix\n\nCoderownames(m1) &lt;- c(\"A\", \"B\", \"C\") \ncolnames(m1)&lt;- c(\"a\",\"b\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#what-is-data-visualization",
    "href": "static/slides/Rplot/graph.html#what-is-data-visualization",
    "title": "",
    "section": "What is data visualization?",
    "text": "What is data visualization?\n\nData visualization is the presentation of data in a pictorial or graphical format, and\nA data visualization tool is the software that generates this presentation.\nEffective data visualization provides users with intuitive means to\n\ninteractively explore and analyze data,\nenabling them to effectively identify interesting patterns,\ninfer correlations and causalities, and\nsupports sense-making activities.\n\n\nGood visual presentations tend to enhance the message of the visualization."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#what-is-data-visualization-1",
    "href": "static/slides/Rplot/graph.html#what-is-data-visualization-1",
    "title": "",
    "section": "What is data visualization?",
    "text": "What is data visualization?\n\nWhat are the key principles, methods, and concepts required to visualize data for publications, reports, or presentations?\nThe effectiveness of data visualization depends on several factors\nWhat would you like to communicate?\nWho is your audience? Researchers? Journalists? General public? Grant reviewers?\n\nWhat is the best way to represent your data and your message?\n\nIs it through a box plot?\nShould you use blue or red?\nWhat scale should you use?\nShould you add or should you remove information?"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#impoerant-packages-to-create-figures",
    "href": "static/slides/Rplot/graph.html#impoerant-packages-to-create-figures",
    "title": "",
    "section": "Impoerant packages to create figures",
    "text": "Impoerant packages to create figures\nA few packages to create figures in R are\n\n\n\n\nggplot2grammer of graphics\n\ncowplot for composing ggplots\n\nggforce visual data investigations\n\nggrepel for nice text labeling\n\nggridges for ridge plots\n\nggsci for nice color palettes\n\nggtext for advanced text rendering\n\nggthemes for additional themes\n\ngrid for creating graphical objects\n\ngridExtra additional functions grid\n\n\n\n\n\npatchwork for multi-panel plots\n\nprismatic for manipulating colors\n\nrcartocolor for great color palettes\n\nscico perceptional uniform palettes\n\nshowtext for custom fonts\n\ncharter interactive visualizations\n\necharts4rinteractive visualizations\n\nggiraph interactive visualizations\n\nhighcharterinteractive visualizations\n\nplotly interactive visualizations"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#install-packages-to-create-figures",
    "href": "static/slides/Rplot/graph.html#install-packages-to-create-figures",
    "title": "",
    "section": "install packages to create figures",
    "text": "install packages to create figures\n\nCode# install CRAN packages\ninstall.packages(\n  c(\"ggplot2\", \"tibble\", \"tidyr\", \"forcats\", \"purrr\", \"prismatic\", \"corrr\", \n    \"cowplot\", \"ggforce\", \"ggrepel\", \"ggridges\", \"ggsci\", \"ggtext\", \"ggthemes\", \n    \"grid\", \"gridExtra\", \"patchwork\", \"rcartocolor\", \"scico\", \"showtext\",\n    \"shiny\", \"plotly\", \"highcharter\", \"echarts4r\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#the-basic-components-of-plot-using-ggplot2-package",
    "href": "static/slides/Rplot/graph.html#the-basic-components-of-plot-using-ggplot2-package",
    "title": "",
    "section": "The basic components of plot using ggplot2 Package",
    "text": "The basic components of plot using ggplot2 Package\n\nggplot2 is a system for declaratively creating graphics, based on the Grammar of Graphics.\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nWhy ggplot2?\n\nA grammar of graphics is a grammar used to describe and create a wide range of statistical graphics.\nThe promise of a grammar for graphics.\nEasy to manage, save, etc.\nGraphs are composed of layers.\nEasy to add stuff to existing graphs.\nggplot2 graphics take less work to make beautiful and eye-catching graphics.\nEnables the creation of reproducible visualization patterns.\nPublication quality & beyond"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#ggplot2-mechanics-the-basics",
    "href": "static/slides/Rplot/graph.html#ggplot2-mechanics-the-basics",
    "title": "",
    "section": "ggplot2 mechanics: the basics",
    "text": "ggplot2 mechanics: the basics\nA ggplot is built up from a few basic elements:\n\n\nData: The raw data that you want to plot.\n\nGeometries geom_: The geometric shapes that will represent the data.\n\nAesthetics aes(): Aesthetics of the geometric and statistical objects, such as position, color, size, shape, and transparency\n\nScales scale_: Maps between the data and the aesthetic dimensions, such as data range to plot width or factor values to colors.\n\nStatistical transformations stat_: Statistical summaries of the data, such as quantiles, fitted curves, and sums.\n\nCoordinate system coord_: The transformation used for mapping data coordinates into the plane of the data rectangle.\n\nFacets facet_: The arrangement of the data into a grid of plots.\n\nVisual themes theme(): The overall visual defaults of a plot, such as background, grids, axes, default typeface, sizes and colors."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#components-of-the-layered-grammar",
    "href": "static/slides/Rplot/graph.html#components-of-the-layered-grammar",
    "title": "",
    "section": "Components of the layered grammar",
    "text": "Components of the layered grammar\n\n\n\nLayer\n\nData\nMapping\nStatistical transformation (stat)\nGeometric object (geom)\nPosition adjustment (position)\n\n\nScale\nCoordinate system (coord)\nFaceting (facet)\n\n\n\n\n‚ÄúSource: BloggoType‚Äù"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#data",
    "href": "static/slides/Rplot/graph.html#data",
    "title": "",
    "section": "Data",
    "text": "Data\n\nData defines the source of the information to be visualized.\nMust be a data.frame\nGets pulled into the ggplot() object\n\nAesthetics (aes()) (a.k.a. mapping)\n\n\nx, y: variables\n\ncolour: colours the lines of geometries\n\nfill: fill geometries or fill color\n\ngroup: groups based on the data\n\nshape: shape of point, an integer value 0 to 24, or NA\n\nlinetype: type of line, a integer value 0 to 6 or a string\n\nsize: sizes of elements, a non-negative numeric value\n\nalpha: changes the transparency,a numeric value 0 to 1"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#data-1",
    "href": "static/slides/Rplot/graph.html#data-1",
    "title": "",
    "section": "Data",
    "text": "Data\n\nCode# data and aesthetics\nggplot(data, mapping = aes(x, y, ...))\n\n\n\n\n\nshape values\n\n\n\n‚Äúshape: shape value‚Äù\n\n\n\nline type value\n\n\n\n‚Äúshape: shape value‚Äù"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#geometries-geom_-function",
    "href": "static/slides/Rplot/graph.html#geometries-geom_-function",
    "title": "",
    "section": "Geometries (geom_*()) function",
    "text": "Geometries (geom_*()) function\nThe general syntax is:\n\nggplot(data = data, mapping = aes(mapings))+ geom_function()\n\nGeom Components\n\n\n\n\n\n\n\nGeom\nDescription\nInput\n\n\n\ngeom_histogram\nHistograms\nContinous x\n\n\ngeom_bar\nBar plot with frequncies\nDiscrete x\n\n\ngeom_point\nPoints/scattorplots\nDiscrete/continuous x and y\n\n\ngeom_boxplot\nBox plot\nDisc. x and cont. y\n\n\ngeom_smooth\nfunction line based on data\n\n\n\ngeom_line\nLine plots\nDiscrete/continuous x and y\n\n\ngeom_abline\nReference\nline intercept and slope value\n\n\ngeom_hline\ngeom_vline\nReference lines xintercept or yintercept"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#geom_-functions",
    "href": "static/slides/Rplot/graph.html#geom_-functions",
    "title": "",
    "section": "geom_*() functions",
    "text": "geom_*() functions\nPositions\n\ngeom_bar(position = \"&lt;position &gt;\")\nWhen we have aesthetics mapped, how are they positioned?\nbar: dodge, fill, stacked (default)\npoint: jitter"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#facets",
    "href": "static/slides/Rplot/graph.html#facets",
    "title": "",
    "section": "Facets",
    "text": "Facets\nfacet_grid vs facet_wrap\n\n\nfacet_grid() facets the plot with a variable in a single direction (horizontal or vertical)\n\nfacet_wrap() simply places the facets next to each other and wraps them accoridng to the provided number of columns and/or rows.\n\nThe following table describes how facet formulas work in facet_grid() and facet_wrap():\n\n\n\n\n\n\n\nType\nFormula\nDescription\n\n\n\nGrid\nfacet_grid(. ~ x)\nFacet horizontally across x values\n\n\nGrid\nfacet_grid(y ~ .)\nFacet vertically across y values\n\n\nGrid\nfacet_grid(y ~ x)\nFacet 2-dimensionally\n\n\nWrap\nfacet_wrap(~ x)\nFacet across x values\n\n\nWrap\nfacet_wrap(~ x + y)\nFacet across x and y values"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#facets-1",
    "href": "static/slides/Rplot/graph.html#facets-1",
    "title": "",
    "section": "Facets",
    "text": "Facets\n\n\nStatistics (stat_*()) computed on the data.\n\n\nstat_*()-like functions perform computations such as means, counts, linear models, and other statistical summaries of data.\n\n\n\nCoordinates (coord_*()) establish representation rules to print the data\n\n\ncoord_cartesian() for the Cartesian plane;\n\ncoord_polar() for circular plots;\n\ncoord_map() for different map projections."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#themes",
    "href": "static/slides/Rplot/graph.html#themes",
    "title": "",
    "section": "Themes",
    "text": "Themes\n\nCodeplot + theme_gray(base_size = 11, base_family = \"\")\n\n\n\nTheme is what controls the overall appearance of the ggplot visualiation.\nggplot2 offers several predefined themes that can be quickly applied to the ggplot object. see the details in section Themes"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#practice-with-ggplot2",
    "href": "static/slides/Rplot/graph.html#practice-with-ggplot2",
    "title": "",
    "section": "Practice with ggplot2",
    "text": "Practice with ggplot2\n\nCreate a simple plot object: plot.object &lt;- ggplot()\n\nAdd geometric layers: plot.object &lt;- plot.object + geom_*()\n\nAdd appearance layers: plot.object &lt;- plot.object + coord_*() + theme()\n\nRepeat steps 2 and 3 until satisfied, then print: plot.object or print(plot.object)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#practice-with-ggplot2-1",
    "href": "static/slides/Rplot/graph.html#practice-with-ggplot2-1",
    "title": "",
    "section": "Practice with ggplot2",
    "text": "Practice with ggplot2\n\ndataset to practice: palmerpenguins\n\n\nWe will use the palmerpenguins data set:\nThis data set contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\n\nThis dataset is often used to replace the iris dataset, which has some problems for teaching data science, including its ties to eugenics.\n\nLet us take a look at the variables in the penguins data set:\n\nCodelibrary(palmerpenguins)\ndata(penguins)\n#str(penguins)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#practice-with-ggplot2-2",
    "href": "static/slides/Rplot/graph.html#practice-with-ggplot2-2",
    "title": "",
    "section": "Practice with ggplot2",
    "text": "Practice with ggplot2\n\nspecies, island, and sex are factor variables,\nbill measurements depicted in the image are numeric variables,\ntwo integer variables (flipper length and body mass).\nPrepare data for ggplot2\n\n\nggplot2 requires you to prepare the data as an object of class data.frame or tibble (common in the tidyverse).\n\n\nCodelibrary(tibble)\nclass(penguins) # all set!\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nCodepeng &lt;- as_tibble(penguins) # acceptable\nclass(peng)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\""
  },
  {
    "objectID": "static/slides/Rplot/graph.html#practice-with-ggplot2-3",
    "href": "static/slides/Rplot/graph.html#practice-with-ggplot2-3",
    "title": "",
    "section": "Practice with ggplot2",
    "text": "Practice with ggplot2\nMore complex plots in ggplot2 require the long data frame format.\n\nScientific questions about penguins\n\n\n\nScientific questions\nIs there a relationship between the length & the depth of bills?\nDoes the size of the bill & flipper vary together ?\nHow are these measures distributed among the 3 penguin species ?\n\nHow can we graphically address these questions with ggplot2?"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#ggplot-layers",
    "href": "static/slides/Rplot/graph.html#ggplot-layers",
    "title": "",
    "section": "ggplot() layers",
    "text": "ggplot() layers\n\n\ndata\naesthetics\ngeometries\nfacets and coordinates\nAll in a single code\n\n\n\n\nCodelibrary(ggplot2)\nggplot(data = penguins)\n\n\n\n\n\n\n\n\n\n\nCodeggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm))\n\n\n\n\n\n\n\n\n\n\nCodeggplot(data = penguins,\n       aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nCodeggplot(data = penguins,\n       aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() +  facet_wrap(~species) +\n  coord_trans(x = \"log10\", y = \"log10\")\n\n\n\n\n\n\n\n\n\nLet us explore how some of this data is structured by species:\n\nCodeggplot(data = penguins,               # Data\n       aes(x = bill_length_mm,        # Your X-value\n           y = bill_depth_mm,         # Your Y-value\n           col = species)) +          # Aesthetics\n  geom_point(size = 5, alpha = 0.8) + # Point\n  geom_smooth(method = \"lm\")         # Linear regression"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#customize-our-plot",
    "href": "static/slides/Rplot/graph.html#customize-our-plot",
    "title": "",
    "section": "Customize Our Plot",
    "text": "Customize Our Plot\nCustomizing plots involves adjusting various elements to enhance their readability, presentation, and informativeness.\n\nHere are some key aspects you can customize:\n\nAxes, Titles and Legends\nTitle and axes components: changing size, colour and face"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-axis-titles",
    "href": "static/slides/Rplot/graph.html#change-axis-titles",
    "title": "",
    "section": "Change Axis Titles",
    "text": "Change Axis Titles\nAxes, Titles and Legends\n\nCustomizing Axis Labels with labs()\n\n\nlabs(): This function is used to modify plot labels, including x-axis, y-axis, and plot title.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-2",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-2",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\nxlab() and ylab(): These functions specifically set the x-axis and y-axis labels, respectively.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  xlab(\"Bill length (mm)\")+ ylab(\"Bill depth (mm)\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-3",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-3",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\nexpression(): This function allows you to include mathematical expressions, special characters, and symbols in axis labels, such as Greek letters, superscripts, and subscripts.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = expression(paste(\"X Axis Label with \", mu^2, \" and \", sigma)))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-4",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-4",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\nIncreasing Space Between Axis and Axis Titles\n\n\nelement_text(): While primarily used in theme() for overall theme customization, element_text() can be used to specify text properties such as size, color, and font face for axis labels.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n      theme(axis.title = element_text(size = 15, face = \"italic\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-5",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-5",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\nTo change vertical alignment using vjust which controls the vertical alignment, typically ranging between 0 and 1, but can extend beyond this range.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.title.x = element_text(vjust = 0, size = 15),\n        axis.title.y = element_text(vjust = 2, size = 15))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-6",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-6",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\nTo change the distance, you can specify the margin() function‚Äôs with parameters t and r which refer to top and right, respectively.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.title.x = element_text(margin = margin(t = 10), size = 15),\n        axis.title.y = element_text(margin = margin(r = 10), size = 15))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-7",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-7",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\nTo adjust the space on the y-axis, change the right margin, not the bottom margin.\nThe face argument can be set to bold, italic, or bold.italic:\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.title = element_text(color = \"sienna\", size = 15, face = \"bold\"),\n        axis.title.y = element_text(face = \"bold.italic\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-8",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-8",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\naxis.text can modify the appearance of axis text (numbers) and its sub-elements axis.text.x and axis.text.y:\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.text = element_text(color = \"dodgerblue\", size = 12),\n        axis.text.x = element_text(face = \"italic\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-9",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-9",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\nangle, hjust and vjust can rotate any text element. hjust and vjust used to adjust the position horizontally (0 = left, 1 = right) and vertically (0 = top, 1 = bottom):\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\ntheme(axis.text.x = element_text(angle = 50, vjust = 1, hjust = 1, size = 12))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-10",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-10",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\nelement_blank() used to remove axis text and ticks,\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-11",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-11",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\nThe element_blank() function is used to remove an element entirely but to remove axis titles by setting them to NULL or empty quotes \" \" in the labs() function:\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n labs(x = NULL, y = \"\")\n\n\nüí° Using NULL removes the element, while empty quotes ‚Äù ‚Äù keep the space for the axis title but print nothing."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-12",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-12",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\nylim() and xlim() functions are used to limiting axis range\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+ ylim(c(0, 20))\n\n\n\nAlternatively, use scale_y_continuous(limits = c(0, 20)) or coord_cartesian(ylim = c(0, 20)). The former removes data points outside the range, while the latter adjusts the visible area without removing data points."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axes-titles-and-legends-13",
    "href": "static/slides/Rplot/graph.html#axes-titles-and-legends-13",
    "title": "",
    "section": "Axes, Titles and Legends",
    "text": "Axes, Titles and Legends\n\n\nscale_x_continuous() and scale_y_continuous(): While primarily for scaling continuous axes, these functions can also adjust axis labels using name.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      scale_x_continuous(name = \"New X Axis Label\") +\n    scale_y_continuous(name = \"New Y Axis Label\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#adding-title",
    "href": "static/slides/Rplot/graph.html#adding-title",
    "title": "",
    "section": "Adding Title",
    "text": "Adding Title\n\nTo customize titles in ggplot2, you can use a combination of ggtitle(), labs(), and theme() functions. Below is a list of the main functions and their key arguments for title customization:\n\nMain Functions and Arguments\n\n\nggtitle(): used to label the text for the main title.\n\nExample: ggtitle(\"Main Title\")\n\n\n\n\nlabs():\n\n\ntitle: The text for the main title.\n\nsubtitle: The text for the subtitle.\n\ncaption: The text for the caption.\n\ntag: The text for a tag.\nExample: labs(title = \"Main Title\", subtitle = \"Subtitle\", caption = \"Caption\", tag = \"Fig. 1\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#adding-title-1",
    "href": "static/slides/Rplot/graph.html#adding-title-1",
    "title": "",
    "section": "Adding Title",
    "text": "Adding Title\n\n\ntheme(): Customize the appearance of the text elements.\n\n\nplot.title: Customize the main title text, subtitle, caption and tag text. Example:\ntheme(plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5))\ntheme(plot.subtitle = element_text(size = 12, hjust = 0.5))\ntheme(plot.caption = element_text(size = 10, hjust = 0))\ntheme(plot.tag = element_text(size = 8, hjust = 1))\n\nelement_text(face, size, family, hjust, vjust, margin, lineheight): Control the font face, size, family, alignment, margin, and line height."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#adding-title-2",
    "href": "static/slides/Rplot/graph.html#adding-title-2",
    "title": "",
    "section": "Adding Title",
    "text": "Adding Title\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n           title = \"Relationship between bill length and depth\", \n           subtitle = \"for different penguin species\", caption = \"scatter plot\",  \n           tag = \"Fig. 1\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#bold-title-and-margin",
    "href": "static/slides/Rplot/graph.html#bold-title-and-margin",
    "title": "",
    "section": "Bold Title and Margin",
    "text": "Bold Title and Margin\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\",  \n           title = \"Relationship between bill length and depth\")+ \n  theme(plot.title = element_text(face = \"bold\", \n                                  margin = margin(10, 0, 10, 0), size = 14))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#using-non-traditional-fonts",
    "href": "static/slides/Rplot/graph.html#using-non-traditional-fonts",
    "title": "",
    "section": "Using Non-Traditional Fonts",
    "text": "Using Non-Traditional Fonts\n\nCodelibrary(showtext) \nfont_add_google(\"Playfair Display\", \"Playfair\") \nfont_add_google(\"Bangers\", \"Bangers\") \nshowtext_auto()\nggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\",  \n           title = \"Relationship between bill length and depth\") + \n  theme(plot.title = element_text(family = \"Bangers\", hjust = 0.5, size = 25),\n        plot.subtitle = element_text(family = \"Playfair\", hjust = 0.5, size = 15))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#changing-line-height-in-multi-line-text",
    "href": "static/slides/Rplot/graph.html#changing-line-height-in-multi-line-text",
    "title": "",
    "section": "Changing Line Height in Multi-Line Text",
    "text": "Changing Line Height in Multi-Line Text\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  ggtitle(\"Relationship between bill length and depth acrrossdifferent \\n \n          species using scatter plot\") +   \n  theme(plot.title = element_text(lineheight = 0.8, size = 16))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#legends",
    "href": "static/slides/Rplot/graph.html#legends",
    "title": "",
    "section": "Legends",
    "text": "Legends\n\nOne nice thing about ggplot2 is that it adds a legend by default when mapping a variable to an aesthetic. You can see that by default the legend title is what we specified in the color argument:\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#legends-1",
    "href": "static/slides/Rplot/graph.html#legends-1",
    "title": "",
    "section": "Legends",
    "text": "Legends\nThe main functions and methods to customize legends in ggplot2:\n\n\nTo Turn Off the Legend: we can use the following code\n\ntheme(legend.position = \"none\")\nguides(color = \"none\")\nscale_color_discrete(guide = \"none\")\n\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n theme(legend.position = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#legends-2",
    "href": "static/slides/Rplot/graph.html#legends-2",
    "title": "",
    "section": "Legends",
    "text": "Legends\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+  \n  guides(color = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#to-remove-legend-titles",
    "href": "static/slides/Rplot/graph.html#to-remove-legend-titles",
    "title": "",
    "section": "To Remove Legend Titles",
    "text": "To Remove Legend Titles\n\ntheme(legend.title = element_blank())\nscale_color_discrete(name = NULL)\nlabs(color = NULL\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\ntheme(legend.title = element_blank())"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#to-remove-legend-titles-1",
    "href": "static/slides/Rplot/graph.html#to-remove-legend-titles-1",
    "title": "",
    "section": "To Remove Legend Titles",
    "text": "To Remove Legend Titles\nüíÅ You can achieve the same by setting the legend name to NULL, either via scale_color_discrete(name = NULL) or labs(color = NULL). Expand to see examples.\n\n\nChange Legend Position\n\ntheme(legend.position = \"top\")\n\ntheme(legend.position = c(x, y), legend.background = element_rect(fill = \"transparent\")) to add legend inside the plot\n\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\ntheme(legend.position = \"top\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#to-remove-legend-titles-2",
    "href": "static/slides/Rplot/graph.html#to-remove-legend-titles-2",
    "title": "",
    "section": "To Remove Legend Titles",
    "text": "To Remove Legend Titles\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\ntheme(legend.position = c(.15, .15),  \n      legend.background = element_rect(fill = \"transparent\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#to-remove-legend-titles-3",
    "href": "static/slides/Rplot/graph.html#to-remove-legend-titles-3",
    "title": "",
    "section": "To Remove Legend Titles",
    "text": "To Remove Legend Titles\n\n\nChange Legend Direction\n\nguides(color = guide_legend(direction = \"horizontal\"))\n\n\n\nChange Style of the Legend Title\n\ntheme(legend.title = element_text(family, color, size, face))\n\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\ntheme(legend.title = element_text(family = \"Playfair\", \n                                  color = \"chocolate\", size =14, face =\"bold\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-legend-title",
    "href": "static/slides/Rplot/graph.html#change-legend-title",
    "title": "",
    "section": "Change Legend Title",
    "text": "Change Legend Title\n\nlabs(color = \"new title\")\nscale_color_discrete(name = \"new title\")\nguides(color = guide_legend(\"new title\"))\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n           color = \"species\\nindicated\\nby colors:\") +   \n  theme(legend.title = element_text(family = \"Playfair\", \n                                    color = \"blue\", size = 14, face = \"bold\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-order-of-legend-keys",
    "href": "static/slides/Rplot/graph.html#change-order-of-legend-keys",
    "title": "",
    "section": "Change Order of Legend Keys",
    "text": "Change Order of Legend Keys\n-   `factor(penguins$species, levels = c(\"Chinstrap\", \"Gentoo\", \"Adelie\"))`\n\nCodelibrary(dplyr)\npenguins1 &lt;- penguins %&gt;%\n  mutate(species = factor(species, levels = c(\"Chinstrap\", \"Gentoo\",\"Adelie\")))\nggplot(data = penguins1) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-legend-labels",
    "href": "static/slides/Rplot/graph.html#change-legend-labels",
    "title": "",
    "section": "Change Legend Labels",
    "text": "Change Legend Labels\n\nscale_color_discrete(name = \"species:\", labels = c(\"Adelie type\", \"Chinstrap type\", \"Gentoo typ\"))\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\nscale_color_discrete(name = \"species:\", labels = c(\"Adelie type\",\"Chinstrap type\", \"Gentoo typ\"))+\n  theme(legend.title = element_text(family = \"Playfair\", color = \"chocolate\", size = 14, face = 2))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-legend-labels-1",
    "href": "static/slides/Rplot/graph.html#change-legend-labels-1",
    "title": "",
    "section": "Change Legend Labels",
    "text": "Change Legend Labels\n\n\nChange Background Boxes in the Legend\n\ntheme(legend.key = element_rect(fill = \"color\"))\n\n\n\nChange Size of Legend Symbols\n\nguides(color = guide_legend(override.aes = list(size = size)))\n\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\ntheme(legend.key = element_rect(fill = NA), \n      legend.title = element_text(color = \"chocolate\", size = 14, face = 2)) +\n  scale_color_discrete(\"species:\") + \n  guides(color = guide_legend(override.aes = list(size = 6)))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-legend-labels-2",
    "href": "static/slides/Rplot/graph.html#change-legend-labels-2",
    "title": "",
    "section": "Change Legend Labels",
    "text": "Change Legend Labels\n\n\nUse Other Legend Styles\n\nguides(color = guide_legend())\nguides(color = guide_bins()\n\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\nguides(color = guide_legend())"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#change-legend-labels-3",
    "href": "static/slides/Rplot/graph.html#change-legend-labels-3",
    "title": "",
    "section": "Change Legend Labels",
    "text": "Change Legend Labels\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  guides(color = guide_bins())"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#theme",
    "href": "static/slides/Rplot/graph.html#theme",
    "title": "",
    "section": "theme",
    "text": "theme\ntheme()\n\nElements of a theme by (Isabella Benabaye)."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#theme-1",
    "href": "static/slides/Rplot/graph.html#theme-1",
    "title": "",
    "section": "theme",
    "text": "theme\nDefault theme: The default theme is theme_gray()."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#theme-2",
    "href": "static/slides/Rplot/graph.html#theme-2",
    "title": "",
    "section": "theme",
    "text": "theme\n\nThe predefined theme takes two arguments for the base font size (base_size) and font family (base_family).\nbase_size input is a number, and base_family is a string (e.g.¬†‚Äúserif‚Äù, ‚Äúsans‚Äù, ‚Äúmono‚Äù).\nIn addition, ggthemes pacakge offers additional predefined themes.\nWe will start with 8 predefined themes provided by ggplot2:\n\n\n\n\nplot+theme_gray()\nplot+theme_bw()\nplot+theme_linedraw()\nplot+theme_light()\n\n\n\nplot + theme_dark()\nplot + theme_minimal()\nplot + theme_classic()\nplot + theme_void()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#theme-3",
    "href": "static/slides/Rplot/graph.html#theme-3",
    "title": "",
    "section": "theme",
    "text": "theme\n\n\ntheme() has many arguments to control and modify individual components of a plot theme, including:\nall line, rectangular, text and title elements\naspect ratio of the panel\naxis title, text, ticks, and lines\nlegend background, margin, text, title, position, and more\npanel aspect ratio, border, and grid lines"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#backgrounds-grid-lines",
    "href": "static/slides/Rplot/graph.html#backgrounds-grid-lines",
    "title": "",
    "section": "Backgrounds & Grid Lines",
    "text": "Backgrounds & Grid Lines\nThe main functions to customize the background of the plot in the provided code and explanation involve modifying elements of the theme function in ggplot2. Here are the key functions and elements used:\n\nChanging the Panel Background Color\n\nThe panel background refers to the area where the data is plotted.\n\n\npanel.background: Adjusts the background color and outline of the panel area.\ntheme(panel.background = element_rect(fill = \"#64D2AA\", color = \"#64D2AA\", linewidth = 2))\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(panel.background = element_rect(fill = \"#64D2AA\", color = \"#64D2AA\", \n                                        linewidth = 2))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#changing-the-panel-border-color",
    "href": "static/slides/Rplot/graph.html#changing-the-panel-border-color",
    "title": "",
    "section": "Changing the Panel Border Color",
    "text": "Changing the Panel Border Color\nThe panel border is an overlay on top of the panel.background which outlines the panel.\n\n\npanel.border: Sets the border properties of the panel.\ntheme(panel.border = element_rect(fill = \"#64D2AA99\", color = \"#64D2AA\", linewidth = 2))\n\n\n\nCodeggplot(data = penguins) +\n    geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n          labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n      theme(panel.border = element_rect(fill = \"#64D2AA99\", color = \"#64D2AA\", \n                                        linewidth = 2))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#changing-grid-lines",
    "href": "static/slides/Rplot/graph.html#changing-grid-lines",
    "title": "",
    "section": "Changing Grid Lines",
    "text": "Changing Grid Lines\nGrid lines help in referencing the data points against the axes.\n\n\npanel.grid: Changes properties for all grid lines.\n\npanel.grid.major: Changes properties for major grid lines.\n\npanel.grid.minor: Changes properties for minor grid lines.\n\npanel.grid.major.x and panel.grid.major.y: Change properties for major grid lines on the x and y axes separately.\n\npanel.grid.minor.x and panel.grid.minor.y: Change properties for minor grid lines on the x and y axes separately.\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(panel.grid.major = element_line(color = \"gray10\",linewidth = .5),       \n        panel.grid.minor = element_line(color = \"gray70\", linewidth = .25))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#changing-grid-lines-1",
    "href": "static/slides/Rplot/graph.html#changing-grid-lines-1",
    "title": "",
    "section": "Changing Grid Lines",
    "text": "Changing Grid Lines\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(panel.grid.major = element_line(linewidth = .5, linetype = \"dashed\"),\n        panel.grid.minor = element_line(linewidth = .25, linetype = \"dotted\"), \n        panel.grid.major.x = element_line(color = \"red1\"),       \n        panel.grid.major.y = element_line(color = \"blue1\"), \n        panel.grid.minor.x = element_line(color = \"red4\"), \n        panel.grid.minor.y = element_line(color = \"blue4\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#removing-grid-lines",
    "href": "static/slides/Rplot/graph.html#removing-grid-lines",
    "title": "",
    "section": "Removing Grid Lines",
    "text": "Removing Grid Lines\nGrid lines can be selectively removed.\n\n\nelement_blank(): Used to remove specific theme elements.\n\ntheme(panel.grid.minor = element_blank())\ntheme(panel.grid = element_blank())\n\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+ theme(panel.grid = element_blank())"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#changing-the-spacing-of-gridlines",
    "href": "static/slides/Rplot/graph.html#changing-the-spacing-of-gridlines",
    "title": "",
    "section": "Changing the Spacing of Gridlines",
    "text": "Changing the Spacing of Gridlines\nYou can specify the spacing of grid lines using scale_*_continuous functions.\n\n\nscale_y_continuous(): Defines the breaks for the y-axis. scale_y_continuous(breaks = seq(0, 100, 10), minor_breaks = seq(0, 100, 2.5))\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  scale_y_continuous(breaks = seq(0, 30, 5), minor_breaks = seq(0, 60, 2.5))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#changing-the-plot-background-color",
    "href": "static/slides/Rplot/graph.html#changing-the-plot-background-color",
    "title": "",
    "section": "Changing the Plot Background Color",
    "text": "Changing the Plot Background Color\nThe plot background refers to the entire area of the plot, including the panel and surrounding space.\n\n\nplot.background: Adjusts the background color and outline of the entire plot area.\ntheme(plot.background = element_rect(fill = \"gray60\", color = \"gray30\", linewidth = 2))\n\n\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(plot.background = element_rect(fill = \"gray60\", \n                                       color = \"gray30\", \n                                       linewidth = 2))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#customizing-multi-panel-plots",
    "href": "static/slides/Rplot/graph.html#customizing-multi-panel-plots",
    "title": "",
    "section": "Customizing multi-panel plots",
    "text": "Customizing multi-panel plots\nWhen creating multi-panel plots in ggplot2, there are several functions and themes available to customize their appearance. Here‚Äôs a breakdown of the main functions and customization options based on the provided code:\nCreating Facets with facet_grid and facet_wrap\n\n\nfacet_wrap(variable ~ .):\nCreates a ribbon of panels based on a single variable.\n\n\nCodeggplot(data = penguins) +\n  geom_point(mapping = aes(x = bill_length_mm, y = bill_depth_mm,\n                           colour = species)) +\n  facet_grid(~ species, scales = \"free\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#facet_gridrows-columns",
    "href": "static/slides/Rplot/graph.html#facet_gridrows-columns",
    "title": "",
    "section": "facet_grid(rows ~ columns):",
    "text": "facet_grid(rows ~ columns):\n\nCreates a grid of panels based on two variables.\n\n\nCodeggplot(data = penguins) +\n  geom_point(mapping = aes(x = bill_length_mm, y = bill_depth_mm,\n                           colour = species)) +\n  facet_grid(year ~ species, scales = \"free\")\n\n\n\nCustomizing Layout of Facets\n\nncol and nrow:\n\nControl the number of columns and rows in facet_wrap."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#facet_wrap",
    "href": "static/slides/Rplot/graph.html#facet_wrap",
    "title": "",
    "section": "facet_wrap",
    "text": "facet_wrap\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n          labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  facet_wrap( ~ species+sex, ncol = 3)\n\n\n\n\nscales:\nAllows axes to have free scales with scales = \"free\" or control specific axis with scales = \"free_x\" or scales = \"free_y\"."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#scales",
    "href": "static/slides/Rplot/graph.html#scales",
    "title": "",
    "section": "scales",
    "text": "scales\n\nCodeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  facet_wrap( ~ species, ncol = 3, scales = \"free\")\n\n\n\nStyling Facet Labels\n\nModifying strip text and background:\nUse theme to customize the appearance of facet labels."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#section",
    "href": "static/slides/Rplot/graph.html#section",
    "title": "",
    "section": "",
    "text": "Codeggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  facet_wrap(~ species, ncol = 3, scales = \"free_x\") + \n  theme(strip.text = element_text(face = \"bold\", color = \"white\", hjust = 0, size = 20), \n        strip.background = element_rect(fill = \"chartreuse4\", linetype = \"dotted\"))\n\n\n\n\nHighlight specific labels using element_textbox_highlight:"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#section-1",
    "href": "static/slides/Rplot/graph.html#section-1",
    "title": "",
    "section": "",
    "text": "Codelibrary(ggtext)\nlibrary(purrr)  # for %||%\nggplot(data = penguins) +\n  geom_point(aes( x = bill_length_mm, y = bill_depth_mm, color = species)) +\n      labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  facet_wrap(~ species, ncol = 3, scales = \"free_x\") +  \n  theme(strip.background = element_blank(), \n        strip.text = element_textbox_highlight(family = \"Playfair\", size = 12, \n                                               face = \"bold\", fill = \"white\", \n                                               box.color = \"chartreuse4\", \n                                               color = \"chartreuse4\", \n                                               halign = .5, linetype = 1, \n                                               r = unit(5, \"pt\"), \n                                               width = unit(1, \"npc\"), \n                                               padding = margin(5, 0, 3, 0), \n                                               margin = margin(0, 1, 3, 1), \n                                               hi.labels = c(\"1997\", \"1998\",\"1999\", \"2000\"), \n                                               hi.fill = \"chartreuse4\", \n                                               hi.box.col = \"black\", \n                                               hi.col = \"white\" ))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#combining-different-plots",
    "href": "static/slides/Rplot/graph.html#combining-different-plots",
    "title": "",
    "section": "Combining Different Plots",
    "text": "Combining Different Plots\n\n\npatchwork package:\n\nCombine multiple plots with simple syntax.\np1 + p2 p1 / p2 (g + p2) / p1\n\n\ncowplot package:\nAnother package for combining multiple plots.\n\n\nCodelibrary(cowplot) \n# plot_grid(plot_grid(g, p1), p2, ncol = 1)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#combining-different-plots-1",
    "href": "static/slides/Rplot/graph.html#combining-different-plots-1",
    "title": "",
    "section": "Combining Different Plots",
    "text": "Combining Different Plots\n\n\n{gridExtra} package:\n\nProvides functions to arrange multiple plots.\n\n\n\n\nCodelibrary(gridExtra) \n# grid.arrange(g, p1, p2, layout_matrix = rbind(c(1, 2), c(3, 3)))`\n\n\n\n\nCustom layout with patchwork:\n\nDefine complex layouts using a design matrix.\n\n\n\n\nCode# layout &lt;- \"AABBBB#   AACCDDE   ##CCDD#   ##CC### \" \n# p2 + p1 + p1 + g + p2 + plot_layout(design = layout)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#colors",
    "href": "static/slides/Rplot/graph.html#colors",
    "title": "",
    "section": "Colors",
    "text": "Colors\nSeveral functions and techniques are highlighted for customizing colors in ggplot2 plots.\n\n\ncolor and fill Arguments: Define the outline color (color) and the filling color (fill) of plot elements.\n\ngeom_point(color = \"steelblue\", size = 2)\ngeom_point(shape = 21, size = 2, stroke = 1, color = \"#3cc08f\", fill = \"#c08f3c\")\n\n\n\n\nCode# default\np &lt;- ggplot(penguins, aes( x = bill_length_mm, y = bill_depth_mm, colour= species)) +\n  geom_point() + labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#colors-1",
    "href": "static/slides/Rplot/graph.html#colors-1",
    "title": "",
    "section": "Colors",
    "text": "Colors\n\nCodep +  \n  geom_point(shape= 21, size= 2, stroke= 1, color= \"#3cc08f\", fill = \"#c08f3c\")\n\n\n\n\nscale_color_* and scale_fill_* Functions: Modify colors when they are mapped to variables. - These functions differ based on whether the variable is categorical (qualitative) or continuous (quantitative)."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#qualitative-variables",
    "href": "static/slides/Rplot/graph.html#qualitative-variables",
    "title": "",
    "section": "Qualitative Variables:",
    "text": "Qualitative Variables:\n**`scale_color_manual` and `scale_fill_manual`**: Manually specify colors for categorical variables. `scale_color_manual(values = c(\"dodgerblue4\", \"darkolivegreen4\", \"darkorchid3\", \"goldenrod1\"))`\n\nCodep + scale_color_manual(values = c(\"dodgerblue4\", \"darkorchid3\", \"goldenrod1\"))\n\n\n\n\nscale_color_brewer and scale_fill_brewer: Use predefined color palettes from ColorBrewer."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#scale_color_brewerpalette-set1",
    "href": "static/slides/Rplot/graph.html#scale_color_brewerpalette-set1",
    "title": "",
    "section": "scale_color_brewer(palette = ‚ÄúSet1‚Äù)",
    "text": "scale_color_brewer(palette = ‚ÄúSet1‚Äù)\n\nCodep+  scale_color_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#quantitative-variables",
    "href": "static/slides/Rplot/graph.html#quantitative-variables",
    "title": "",
    "section": "Quantitative Variables:",
    "text": "Quantitative Variables:\n\nscale_color_gradient and scale_fill_gradient: Apply a sequential gradient color scheme for continuous variables.\nscale_color_gradient(low = \"darkkhaki\", high = \"darkgreen\")\n\n\nCodep2 &lt;- ggplot(penguins, aes( x = bill_length_mm, y = bill_depth_mm, \n                            colour = body_mass_g)) + geom_point()+ \n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\") \n    p2 + scale_color_gradient(low = \"darkkhaki\", high = \"darkgreen\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#quantitative-variables-1",
    "href": "static/slides/Rplot/graph.html#quantitative-variables-1",
    "title": "",
    "section": "Quantitative Variables:",
    "text": "Quantitative Variables:\n\n\nscale_color_viridis_c and scale_fill_viridis_c: Use the Viridis color palettes, which are perceptually uniform and suitable for colorblind viewers.\n\nscale_color_viridis_c(option = \"inferno\")\n\nCodep2 + scale_color_viridis_c(option = \"inferno\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#additional-color-palettes-from-extension-packages",
    "href": "static/slides/Rplot/graph.html#additional-color-palettes-from-extension-packages",
    "title": "",
    "section": "Additional Color Palettes from Extension Packages:",
    "text": "Additional Color Palettes from Extension Packages:\nThe {ggthemes} package for example lets R users access the Tableau colors. Tableau is a famous visualiztion software with a well-known color palette.\n\n\nscale_color_tableau and scale_fill_tableau (from ggthemes): Use Tableau color palettes."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#scale_color_tableau",
    "href": "static/slides/Rplot/graph.html#scale_color_tableau",
    "title": "",
    "section": "scale_color_tableau()",
    "text": "scale_color_tableau()\n\nCodelibrary(ggthemes)\np + scale_color_tableau()\n\n\nThe {ggsci} package provides scientific journal and sci-fi themed color palettes with colors that look like being published in Science or Nature."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#scale_color_tableau-1",
    "href": "static/slides/Rplot/graph.html#scale_color_tableau-1",
    "title": "",
    "section": "scale_color_tableau()",
    "text": "scale_color_tableau()\n\n\nscale_color_aaas, scale_color_npg (from ggsci): Use scientific journal and sci-fi themed color palettes.\nscale_color_aaas()\nscale_color_npg()\n\n\nCodelibrary(ggsci)\np + scale_color_aaas()\n\n\n\n\n\n\nCodep+ scale_color_npg()\n\n\n\n\n\n\n\n\n\nscale_color_carto_c and scale_fill_carto_c (from rcartocolor): Use CARTO color palettes.\nscale_color_carto_c(palette = \"BurgYl\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#scale_color_tableau-2",
    "href": "static/slides/Rplot/graph.html#scale_color_tableau-2",
    "title": "",
    "section": "scale_color_tableau()",
    "text": "scale_color_tableau()\n\nCodelibrary(rcartocolor)\np2 + scale_color_carto_c(palette = \"BurgYl\")\n\n\n\n\n\n\n\n\n\nscale_color_scico and scale_fill_scico (from scico): Use perceptually uniform color palettes from the Scico package.\nThe {scico} package provides access to the color palettes developed by Fabio Crameri.\nscale_color_scico(palette = \"berlin\")\n\n\nCodelibrary(scico)\np2 + scale_color_scico(palette = \"berlin\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#scale_color_tableau-3",
    "href": "static/slides/Rplot/graph.html#scale_color_tableau-3",
    "title": "",
    "section": "scale_color_tableau()",
    "text": "scale_color_tableau()\n\nCode# Manual\np + scale_colour_manual(values = c(\"grey55\", \"orange\",  \"skyblue\")) +\n  labs(title = \"Manual\")\n\n\n\n\n\n\n\n\nUse a predefined colour palette\n\n\nCode#install.packages(\"RColorBrewer\")\nrequire(RColorBrewer)\ndisplay.brewer.all()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#section-2",
    "href": "static/slides/Rplot/graph.html#section-2",
    "title": "",
    "section": "",
    "text": "Codelibrary(RColorBrewer)\np+scale_colour_brewer(palette = \"Dark2\") +\n  labs(title = \"Palette for groups\")\n\n\n\n\n\n\n\n\nUse a predefined colour palette\n\n\nCode# Palette for continuous values\np2 + scale_colour_viridis_c()+\n  labs(title= \"Palette for continuous values\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#use-colourblind-friendly-palettes",
    "href": "static/slides/Rplot/graph.html#use-colourblind-friendly-palettes",
    "title": "",
    "section": "Use colourblind-friendly palettes",
    "text": "Use colourblind-friendly palettes\nHave you ever considered how your figure might appear under various forms of colourblindness? We can use the package colorBlindness to consider this.\n\nCodelibrary(colorBlindness)\ncvdPlot(p)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#use-colourblind-friendly-palettes-1",
    "href": "static/slides/Rplot/graph.html#use-colourblind-friendly-palettes-1",
    "title": "",
    "section": "Use colourblind-friendly palettes",
    "text": "Use colourblind-friendly palettes\n\nCode# Palette for groups\np + \n  scale_colour_viridis_d() +\n  labs(title =\"Viridis palette for groups\")\n\n\n\n\n\n\n\n\nCode# Palette for continuous values\np2 + \n  scale_colour_viridis_c() +\n  labs(title = \"Viridis palette \n       for continuous values\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#lines",
    "href": "static/slides/Rplot/graph.html#lines",
    "title": "",
    "section": "Lines",
    "text": "Lines\n\n\ngeom_hline(): Adds horizontal lines to a plot at specified y-axis values.\nyintercept: A numeric vector indicating where to draw the horizontal lines. geom_hline(yintercept = c(12, 23))\n\n\n\nCodep + geom_hline(yintercept = c(12, 23))\n\n\n\n\ngeom_vline(): Adds vertical lines to a plot at specified x-axis values.\n\n\nxintercept: A numeric vector or aesthetic mapping for x-axis intercepts.\n\ncolor, linewidth, linetype: Aesthetics for customizing the appearance of the line."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#lines-1",
    "href": "static/slides/Rplot/graph.html#lines-1",
    "title": "",
    "section": "Lines",
    "text": "Lines\ngeom_vline(aes(xintercept = 45), linewidth = 1.5, color = \"firebrick\", linetype = \"dashed\")\n\nCodep + geom_vline(aes(xintercept = 45), linewidth = 1.5, color = \"firebrick\", \n               linetype = \"dashed\")\n\n\n\n\ngeom_abline(): Adds lines with a specified slope and intercept to a plot.\n\n\nintercept: The intercept of the line.\n\nslope: The slope of the line.\n\ncolor, linewidth: Aesthetics for customizing the appearance of the line. geom_abline(intercept = coefficients(reg)[1], slope = coefficients(reg)[2], color = \"darkorange2\", linewidth = 1.5)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#lines-2",
    "href": "static/slides/Rplot/graph.html#lines-2",
    "title": "",
    "section": "Lines",
    "text": "Lines\n\nCodereg &lt;- lm(body_mass_g ~ bill_depth_mm, data = penguins) \np + geom_abline(intercept = coefficients(reg)[1], slope = coefficients(reg)[2], \n                color = \"darkorange2\", linewidth = 1.5)\n\n\n\n\ngeom_linerange(): Adds line segments that do not span the entire plot range. Can be used for highlighting specific ranges.\n\n\nx, y: Aesthetics for the starting and ending points of the line.\n\nxmin, xmax, ymin, ymax: Coordinates for the start and end of the line segments.\n\ncolor, linewidth: Aesthetics for customizing the appearance of the line."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#lines-3",
    "href": "static/slides/Rplot/graph.html#lines-3",
    "title": "",
    "section": "Lines",
    "text": "Lines\n\nCodep+geom_linerange(aes(x = 45, ymin = 15, ymax = 22), color = \"steelblue\", linewidth = 1)+\n  geom_linerange(aes(y = 16, xmin = 30, xmax = 45), color = \"red\", linewidth = 1)\n\n\n\n\nannotate(geom = \"segment\"): Adds line segments with specified start and end points. Useful for creating lines with arbitrary slopes.\n\n\nx, xend, y, yend: Coordinates for the start and end of the line segments.\n\ncolor, linewidth: Aesthetics for customizing the appearance of the line."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#lines-4",
    "href": "static/slides/Rplot/graph.html#lines-4",
    "title": "",
    "section": "Lines",
    "text": "Lines\n\nCodep + annotate(geom = \"segment\", x = 10, xend = 75, y = 20, yend = 5, \n             color = \"purple\", linewidth = 2)\n\n\n\n\ngeom_encircle() in the ggalt package is used to automatically enclose points in a polygon, creating an encircling effect around specified groups of points in a ggplot2 plot. This can be useful for highlighting clusters or groups of points within your data visualization."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#lines-5",
    "href": "static/slides/Rplot/graph.html#lines-5",
    "title": "",
    "section": "Lines",
    "text": "Lines\n\nCodelibrary(ggalt)\np + geom_encircle(data=subset(penguins, species ==\"Adelie\"),\n                  colour=\"blue\", spread=0.002) + \n  geom_encircle(data=subset(penguins, species ==\"Chinstrap\"), \n                colour=\"purple\", spread=0.002) +\n  geom_encircle(data=subset(penguins, species ==\"Gentoo\"), \n                colour=\"red\", spread=0.002) + ylim(10, 23)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text",
    "href": "static/slides/Rplot/graph.html#text",
    "title": "",
    "section": "Text",
    "text": "Text\nThere are a range of functions and techniques to customize text and labels in ggplot2 plots. Let us seea a detailed explanation of each function mentioned, along with examples and their purposes:\n\n\ngeom_label(): Adds labels to points on the plot with a rectangle around the text.\n\n\nCodep + geom_label(aes(label = species), hjust = .5, vjust = -.5) + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text-1",
    "href": "static/slides/Rplot/graph.html#text-1",
    "title": "",
    "section": "Text",
    "text": "Text\n\n\nhjust and vjust control the horizontal and vertical justification of the labels.\n\ngeom_text(): Similar to geom_label(), but without the rectangle around the text.\n\n\nCodep + geom_text(aes(label = sex), hjust = .5, vjust = -.5)+ \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text-2",
    "href": "static/slides/Rplot/graph.html#text-2",
    "title": "",
    "section": "Text",
    "text": "Text\n\n\nggrepel Package: Provides functions to repel overlapping text labels.\n\n\ngeom_text_repel(): Repels text labels to avoid overlap.\n\ngeom_label_repel(): Repels text labels with a rectangle around them.\n\n\n\n\nCodelibrary(ggrepel)\np + geom_label_repel(aes(label = species), fontface = \"bold\")+ \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text-3",
    "href": "static/slides/Rplot/graph.html#text-3",
    "title": "",
    "section": "Text",
    "text": "Text\n\ngeom_label_repel() avoids overlapping by adjusting the position of labels.\nannotate(): Adds annotations to a plot.\n\n\nCodep + annotate(geom = \"text\", x = 45, y = 25, fontface = \"bold\", \n             label = \"This is a useful annotation\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text-4",
    "href": "static/slides/Rplot/graph.html#text-4",
    "title": "",
    "section": "Text",
    "text": "Text\n\n\nannotate() is used to add single text or label annotations at specified coordinates.\n\nannotation_custom(): Adds custom annotations using grid graphical objects.\n\n\nCodelibrary(grid)\nmy_grob &lt;- grobTree(textGrob(\"This is species type!\", x = .1, y = .9, \n                             hjust = 0, gp = gpar(col = \"black\", fontsize = 15, \n                                                  fontface = \"bold\"))) \np + annotation_custom(my_grob) + facet_wrap(~species, scales = \"free_x\") + scale_y_continuous(limits = c(NA, 20)) + theme(legend.position = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text-5",
    "href": "static/slides/Rplot/graph.html#text-5",
    "title": "",
    "section": "Text",
    "text": "Text\n\n\nggtext Package: Enhances text rendering with support for markdown and HTML.\n\nFunctions:\n\n\ngeom_richtext(): Renders text as markdown or HTML.\n\ngeom_textbox(): Provides dynamic wrapping for longer text annotations.\n\n\n\n\nCodelibrary(ggtext) \nlab_md &lt;- \"This plot shows **Bill lngth** in *¬∞mm* versus **Bill depth** in *mm* across Species type\" \np + geom_richtext(aes(x = 45, y = 22.5, label = lab_md, stat = \"unique\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#text-6",
    "href": "static/slides/Rplot/graph.html#text-6",
    "title": "",
    "section": "Text",
    "text": "Text\n\nIf we need to add long text to annotate our plot\n\n\nCodelab_long &lt;- \"**Association**&lt;br&gt;&lt;i style='font-size:8pt;color:black;'&gt;This graph is a scatter plot showing the association between bill length and bill weidth for each specias type. So we can see that there is a crear association.&lt;/i&gt;\"\n\np + geom_textbox(aes(x = 45, y = 20, label = lab_long),\n                 width = unit(25, \"lines\"), stat = \"unique\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#coordinates",
    "href": "static/slides/Rplot/graph.html#coordinates",
    "title": "",
    "section": "Coordinates",
    "text": "Coordinates\nTo customize plots in ggplot2, you can use a variety of functions that modify the coordinates, axes, scales, and themes of your plot. Here is a detailed explanation of the functions mentioned in your provided text, as well as some additional ones commonly used in ggplot2 for customization:\n\n\ncoord_flip(): Flips the x and y coordinates, making horizontal plots vertical and vice versa. This is particularly useful for bar charts and boxplots.\n\n\nCodep + coord_flip()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#coordinates-1",
    "href": "static/slides/Rplot/graph.html#coordinates-1",
    "title": "",
    "section": "Coordinates",
    "text": "Coordinates\n\n\ncoord_fixed(ratio = 1): Fixes the aspect ratio of the plot, ensuring a specific ratio of units on the x and y axes.\n\n\nCodep + scale_x_continuous(breaks = seq(0, 60, by = 5)) + \n  coord_fixed(ratio = 1)\n\n\n\n\ncoord_fixed(ratio = 1/3): Sets a different aspect ratio, ensuring a 1:3 ratio of units on the x and y axes."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#coordinates-2",
    "href": "static/slides/Rplot/graph.html#coordinates-2",
    "title": "",
    "section": "Coordinates",
    "text": "Coordinates\n\nCodep + scale_x_continuous(breaks = seq(0, 60, by = 15)) +   \n  coord_fixed(ratio = 2/3) +   \n  theme(plot.background = element_rect(fill = \"grey80\"))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#coordinates-3",
    "href": "static/slides/Rplot/graph.html#coordinates-3",
    "title": "",
    "section": "Coordinates",
    "text": "Coordinates\n\n\ncoord_polar(): Converts the plot to polar coordinates, often used for circular bar charts and pie charts.\n\n\nCodepenguins %&gt;%   dplyr::group_by(species) %&gt;%   dplyr::summarize(bd = median(flipper_length_mm)) %&gt;%   ggplot(aes(x = species, y = bd)) +   \n  geom_col(aes(fill = species), color = NA) +   \n  labs(x = \"\", y = \"Median Ozone Level\") +   coord_polar() +   \n  guides(fill = \"none\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#coordinates-4",
    "href": "static/slides/Rplot/graph.html#coordinates-4",
    "title": "",
    "section": "Coordinates",
    "text": "Coordinates\n\n\ncoord_polar(theta = \"y\"): Used for creating pie charts by specifying the theta parameter as ‚Äúy‚Äù.\n\n\nCodelibrary(dplyr)\nchic_sum &lt;- penguins %&gt;%  mutate(n_all = n()) %&gt;% group_by(species) %&gt;% dplyr::summarize(Total = n() / unique(n_all))  \nggplot(chic_sum, aes(x = \"\", y = Total)) +\n  geom_col(aes(fill = species), width = 1, color = NA) + \n  coord_polar(theta = \"y\") +   \n  scale_fill_brewer(palette = \"Set1\", name = \"Species:\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axis-functions",
    "href": "static/slides/Rplot/graph.html#axis-functions",
    "title": "",
    "section": "Axis Functions",
    "text": "Axis Functions\n\n\nscale_x_continuous() / scale_y_continuous(): Customize the breaks, labels, and limits of continuous scales.\n\n\nCodep +  scale_x_continuous(breaks = seq(0, 60, by = 10))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axis-functions-1",
    "href": "static/slides/Rplot/graph.html#axis-functions-1",
    "title": "",
    "section": "Axis Functions",
    "text": "Axis Functions\n\n\nscale_x_reverse() / scale_y_reverse(): Reverses the direction of the x or y axis, making higher values appear on the left or bottom.\n\n\nCodep + scale_y_reverse()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#axis-functions-2",
    "href": "static/slides/Rplot/graph.html#axis-functions-2",
    "title": "",
    "section": "Axis Functions",
    "text": "Axis Functions\n\n\nscale_y_log10(): Transforms the y-axis to a logarithmic scale (base 10), useful for data with a wide range.\n\n\nCodep+scale_y_sqrt()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#smoothings",
    "href": "static/slides/Rplot/graph.html#smoothings",
    "title": "",
    "section": "Smoothings",
    "text": "Smoothings\nTo make smoothing our plot, we can simply use stat_smooth().\n\nThis adds a LOESS (locally weighted scatter plot smoothing, method = \"loess\") if you have fewer than 1000 points or a GAM (generalized additive model, method = \"gam\") otherwise.\n\n\nCodep + geom_point(color = \"gray40\", alpha = .5) + stat_smooth()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#adding-a-linear-fit",
    "href": "static/slides/Rplot/graph.html#adding-a-linear-fit",
    "title": "",
    "section": "Adding a Linear Fit",
    "text": "Adding a Linear Fit\nThough the default is a LOESS or GAM smoothing, it is also easy to add a standard linear fit:\n\nCodep + geom_point(color = \"gray40\", alpha = .5) + \n  stat_smooth(method = \"lm\", se = FALSE, color = \"firebrick\", linewidth = 1.3)+ \n  labs(x = \"Temperature (¬∞F)\", y = \"Dewpoint\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#specifying-the-formula-for-smoothing",
    "href": "static/slides/Rplot/graph.html#specifying-the-formula-for-smoothing",
    "title": "",
    "section": "Specifying the Formula for Smoothing",
    "text": "Specifying the Formula for Smoothing\nggplot2 allows you to specify the model you want it to use. Maybe you want to use a polynomial regression?\n\nCodep + geom_point(color = \"gray40\", alpha = .3) + \n  geom_smooth(method = \"lm\",\n              formula = y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5),\n              color = \"black\", fill = \"firebrick\") +\n  labs(x = \"Ozone Level\", y = \"Temperature (¬∞F)\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#interactive-plots",
    "href": "static/slides/Rplot/graph.html#interactive-plots",
    "title": "",
    "section": "Interactive Plots",
    "text": "Interactive Plots\n\nInteractive plots in R are a great way to enhance the user experience by providing dynamic and visually appealing graphics. Some libraries that can be used in combination with ggplot2 or on their own to create interactive visualizations:\n\nThere are different interactive Plot Libraries. The following are among the few\nPlot.ly} is a tool for creating online, interactive graphics and web apps. The plotly package in R allows you to easily convert your ggplot2 plots into interactive plots.\n\nCodelibrary(plotly)\nggplotly(p)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#interactive-plots-1",
    "href": "static/slides/Rplot/graph.html#interactive-plots-1",
    "title": "",
    "section": "Interactive Plots",
    "text": "Interactive Plots\nThis function ggplotly(p) converts the ggplot2 object p into an interactive plot.\n\n\nggiraph() is an R package that allows you to create dynamic ggplot2 graphs, adding tooltips, animations, and JavaScript actions.\n\n\nCodelibrary(ggiraph)\np3 &lt;- p+   geom_line(color = \"grey\") + \n  geom_point_interactive(aes(color = species, tooltip = species, data_id = species)) +\n  scale_color_brewer(palette = \"Dark2\", guide = \"none\")\ngirafe(ggobj = p3)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#interactive-plots-2",
    "href": "static/slides/Rplot/graph.html#interactive-plots-2",
    "title": "",
    "section": "Interactive Plots",
    "text": "Interactive Plots\nThe function girafe(ggobj = p3) creates an interactive plot with tooltips.\n\n\nhighcharter() is a software library for interactive charting. The highcharter package brings this functionality to R.\n\n\nCodelibrary(highcharter) \nhchart(penguins, \"scatter\", hcaes(x = bill_length_mm, y = bill_depth_mm, \n                                  group = species))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#interactive-plots-3",
    "href": "static/slides/Rplot/graph.html#interactive-plots-3",
    "title": "",
    "section": "Interactive Plots",
    "text": "Interactive Plots\n\nThe hchart function generates a scatter plot using the Highcharts library.\necharts4r is a free, powerful charting and visualization library. The echarts4r package provides an interface to use this library in R.\n\n\nCodelibrary(echarts4r) \npenguins %&gt;%   e_charts(bill_length_mm) %&gt;% \n  e_scatter(body_mass_g, symbol_size = 7) %&gt;%   \n  e_visual_map(body_mass_g) %&gt;%   e_y_axis(name = \"Bill length\") %&gt;%   \n  e_legend(FALSE)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#interactive-plots-4",
    "href": "static/slides/Rplot/graph.html#interactive-plots-4",
    "title": "",
    "section": "Interactive Plots",
    "text": "Interactive Plots\n\nThe e_charts function initializes the chart, and subsequent functions customize it.\nThe charter package allows you to use this framework in R.\n\n\nCode#remotes::install_github(\"JohnCoene/charter\")\nlibrary(charter) \nchart(data = penguins, caes(bill_length_mm, body_mass_g)) %&gt;%   \n  c_scatter(caes(color = species, group = species)) %&gt;% \n  c_colors(RColorBrewer::brewer.pal(4, name = \"Dark2\"))\n\n\n\n\n\n\n\n\nThe chart function initializes the chart, and subsequent functions customize it."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#create-different-plots-using-geom_",
    "href": "static/slides/Rplot/graph.html#create-different-plots-using-geom_",
    "title": "",
    "section": "Create different plots using geom_*()",
    "text": "Create different plots using geom_*()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#create-different-plots-using-geom_-1",
    "href": "static/slides/Rplot/graph.html#create-different-plots-using-geom_-1",
    "title": "",
    "section": "Create different plots using geom_*()",
    "text": "Create different plots using geom_*()\n\nCodep1 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = species)) +\n  geom_point()\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = species)) +\n  geom_density2d()\np3 &lt;- ggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar()\np4 &lt;- ggplot(penguins, aes(x = species, y = bill_depth_mm, fill = species)) +\n  geom_boxplot()\n\nlibrary(patchwork)\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#create-different-plots-using-geom_-2",
    "href": "static/slides/Rplot/graph.html#create-different-plots-using-geom_-2",
    "title": "",
    "section": "Create different plots using geom_*()",
    "text": "Create different plots using geom_*()\nThis is a blank plot, before we add any geom_* to represent variables in the dataset.\n\nCodeggplot(penguins)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#bar-plot-using-geom_bar",
    "href": "static/slides/Rplot/graph.html#bar-plot-using-geom_bar",
    "title": "",
    "section": "Bar plot using geom_bar()",
    "text": "Bar plot using geom_bar()\n\nBar chart of number of penguins by species. I would like to know how many species we have in this dataset.\n\n\nCodeggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() + labs(title = \"Number of Penguins by Species\",\n       x = \"Species\",  y = \"Count\", fill = \"Species\") + theme_minimal()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#bar-plot-using-geom_bar-1",
    "href": "static/slides/Rplot/graph.html#bar-plot-using-geom_bar-1",
    "title": "",
    "section": "Bar plot using geom_bar()",
    "text": "Bar plot using geom_bar()\n\nNumber of Penguin species on each Island\n\n\nCodeggplot(data = penguins)+ geom_bar(mapping=aes(x=island, fill=species))+\n  labs(title=\"Population of Penguin species on each Island\", y=\"count of species\")+\ntheme(text=element_text(size=14))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#bar-plot-using-geom_bar-2",
    "href": "static/slides/Rplot/graph.html#bar-plot-using-geom_bar-2",
    "title": "",
    "section": "Bar plot using geom_bar()",
    "text": "Bar plot using geom_bar()\n\nchart of body mass by species & sex.\n\n\nCodeggplot(penguins, aes(x = species, y = body_mass_g, fill = sex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Body Mass by Species and Sex\",\n       x = \"Species\", y = \"Body Mass (g)\", fill = \"Sex\") +\n  theme_minimal()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#labels",
    "href": "static/slides/Rplot/graph.html#labels",
    "title": "",
    "section": "Labels",
    "text": "Labels\nYou can add labels with geom_label or geom_text. geom_text is just text and geom_label is text inside a rounded white box (this, of course, can be changed)."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#histograms-geom_histogram",
    "href": "static/slides/Rplot/graph.html#histograms-geom_histogram",
    "title": "",
    "section": "Histograms: geom_histogram()",
    "text": "Histograms: geom_histogram()\nA histogram is an accurate graphical representation of the distribution of numeric data. There is only one aesthetic required: the x variable.\n\nCodeggplot(penguins,\n       aes(x = bill_length_mm)) + geom_histogram() +\n  ggtitle(\"Histogram of penguin bill length \")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#boxplot-geom_boxplot",
    "href": "static/slides/Rplot/graph.html#boxplot-geom_boxplot",
    "title": "",
    "section": "Boxplot: geom_boxplot()",
    "text": "Boxplot: geom_boxplot()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#boxplot-geom_boxplot-1",
    "href": "static/slides/Rplot/graph.html#boxplot-geom_boxplot-1",
    "title": "",
    "section": "Boxplot: geom_boxplot()",
    "text": "Boxplot: geom_boxplot()\n\nBoxplot of body mass distribution of penguins by species\n\n\nCodeggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Body Mass Distribution of Penguins by Species\",\n       x = \"Species\",   y = \"Body Mass (g)\", fill = \"Species\") +\n  theme_minimal()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#boxplot-geom_boxplot-2",
    "href": "static/slides/Rplot/graph.html#boxplot-geom_boxplot-2",
    "title": "",
    "section": "Boxplot: geom_boxplot()",
    "text": "Boxplot: geom_boxplot()\n\nCodeggplot(data = penguins,\n       aes(x = species, y = bill_length_mm, fill = species)) +\n  geom_boxplot() + labs(title = \"Boxplot\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#boxplot-geom_boxplot-3",
    "href": "static/slides/Rplot/graph.html#boxplot-geom_boxplot-3",
    "title": "",
    "section": "Boxplot: geom_boxplot()",
    "text": "Boxplot: geom_boxplot()\n\nBoxplot with annotations: geom_boxplot() and geom_signif()\n\n\n\nCodelibrary(ggsignif)\nggplot(data = penguins, aes(x = species, y= bill_length_mm, fill = species)) +\n  geom_boxplot() +\n  # specify the comparison we are interested in\n  geom_signif(comparisons = list(c(\"Adelie\", \"Gentoo\")), map_signif_level=TRUE)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#violin-plot-geom_violin",
    "href": "static/slides/Rplot/graph.html#violin-plot-geom_violin",
    "title": "",
    "section": "Violin plot: geom_violin()",
    "text": "Violin plot: geom_violin()\nViolin plot allows to visualize the distribution of a numeric variable for one or several groups. It is really close to a boxplot, but allows a deeper understanding of the distribution."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#violin-plot-geom_violin-1",
    "href": "static/slides/Rplot/graph.html#violin-plot-geom_violin-1",
    "title": "",
    "section": "Violin plot: geom_violin()",
    "text": "Violin plot: geom_violin()"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#violin-plot-geom_violin-2",
    "href": "static/slides/Rplot/graph.html#violin-plot-geom_violin-2",
    "title": "",
    "section": "Violin plot: geom_violin()",
    "text": "Violin plot: geom_violin()\n\nCodeviolin &lt;- ggplot(data = penguins, aes(x = species, y = bill_length_mm)) +\n  geom_violin(trim = FALSE, fill = \"grey70\", alpha = .5) +\n  labs(title = \"Violin plot\")\nviolin"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#violin-plot-geom_violin-_boxplot-_jitter",
    "href": "static/slides/Rplot/graph.html#violin-plot-geom_violin-_boxplot-_jitter",
    "title": "",
    "section": "Violin plot: geom_violin() + _boxplot() + _jitter()",
    "text": "Violin plot: geom_violin() + _boxplot() + _jitter()\n\nCodeviolin + geom_jitter(shape = 16, position = position_jitter(0.2),\n              alpha = .3) + geom_boxplot(width = .05)"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#pie-chart",
    "href": "static/slides/Rplot/graph.html#pie-chart",
    "title": "",
    "section": "Pie chart",
    "text": "Pie chart\nSummarise y values: stat_summary()\n\n\nCodeggplot(mtcars, aes(cyl, mpg)) + geom_point() +\n  stat_summary(fun.y = \"median\", geom = \"point\",\n               colour = \"red\",size = 6) + labs(title = \"Medians\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#pie-chart-1",
    "href": "static/slides/Rplot/graph.html#pie-chart-1",
    "title": "",
    "section": "Pie chart",
    "text": "Pie chart\n\nCodeggplot(mtcars, aes(cyl, mpg)) + geom_point() +\n  stat_summary(fun.data = \"mean_cl_boot\", colour = \"red\", size = 1.6) + \n  labs(title = \"Means and CIs\")"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#line-charts",
    "href": "static/slides/Rplot/graph.html#line-charts",
    "title": "",
    "section": "Line Charts",
    "text": "Line Charts\nYou can use geom_line() for line charts to display values over time. geom_line() requires an additional group= aesthetic. If there should be only 1 line because there is only 1 time variable, then use group=1. If you want to split the lines based on another variable, use group=variable_name.\nA line graph displaying a single line for year\n\nCodedata(AirPassengers)\nairpassengers &lt;- data.frame(AirPassengers, year = trunc(time(AirPassengers)), \nmonth = month.abb[cycle(AirPassengers)])\n\nairpassengers %&gt;% group_by(year) %&gt;% summarize(sum =sum(AirPassengers, na.rm=T)) %&gt;%\n  ggplot()+ geom_line(aes(x=year, y=sum, group=1))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#line-charts-1",
    "href": "static/slides/Rplot/graph.html#line-charts-1",
    "title": "",
    "section": "Line Charts",
    "text": "Line Charts\nA line graph displaying 1 line per month\n\nCodeggplot(airpassengers)+ geom_line(aes(x=year, y=AirPassengers, group=month))\n\n\nWe can add labels to the ends of the line using geom_label() (see Labels) but the lines are very close together, so we will use ggrepel() instead. This gives the labels space and connects them with their lines."
  },
  {
    "objectID": "static/slides/Rplot/graph.html#line-charts-2",
    "href": "static/slides/Rplot/graph.html#line-charts-2",
    "title": "",
    "section": "Line Charts",
    "text": "Line Charts\n\nCodelibrary(ggrepel)\n\nggplot(airpassengers)+ geom_line(aes(x=year, y=AirPassengers, group=month))+\n  geom_label_repel(data=airpassengers %&gt;% filter(year == max(year)),\n                  aes(x=year, y=AirPassengers, label=month))"
  },
  {
    "objectID": "static/slides/Rplot/graph.html#refer-more-one",
    "href": "static/slides/Rplot/graph.html#refer-more-one",
    "title": "",
    "section": "Refer more one",
    "text": "Refer more one\nThe Ultimate Guide to Get Started With ggplot2: Albert Rapp\nVisualization"
  },
  {
    "objectID": "pages/about.html#intersts",
    "href": "pages/about.html#intersts",
    "title": "About Me",
    "section": "Intersts",
    "text": "Intersts\nI am quite interested in\n\n\n\nCausal inference,\nLongitudinal data analysis,\nSurvival analysis,\nJoint Model\n\n\n\nSpatial data analysis,\nMissing data management,\nDesign of clinical trials."
  },
  {
    "objectID": "pages/about.html#about-me",
    "href": "pages/about.html#about-me",
    "title": "About Me",
    "section": "",
    "text": "I have dual master‚Äôs degrees in Biostatistics from Stellenbosch University and Statistics from Addis Ababa University, coupled with 11+ years of experience in project leading, data management, analysis, and teaching. These experiences equipped me with a deep understanding of complex data, its management, analysis and use with application of different statistical methods. I have a piece of statistical software training experiences (R, SAS, SPSS)."
  },
  {
    "objectID": "pages/about.html#section",
    "href": "pages/about.html#section",
    "title": "About Me",
    "section": "",
    "text": "I have dual master‚Äôs degrees in Biostatistics from Stellenbosch University and Statistics from Addis Ababa University, coupled with 11+ years of experience in project leading, data management, analysis, and teaching. These experiences equipped me with a deep understanding of complex data, its management, analysis and use with application of different statistical methods. I have a piece of statistical software training experiences (R, SAS, SPSS)."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#data-manipulation-and-claning-using-dplyr-package",
    "href": "static/slides/Datamanagment/datamanagment.html#data-manipulation-and-claning-using-dplyr-package",
    "title": "",
    "section": "Data Manipulation and Claning using dplyr() package",
    "text": "Data Manipulation and Claning using dplyr() package\nWhat is Tidyverse?\n\nThe tidyverse library is a collection of several R packages that are designed to work together to make data manipulation and visualization tasks easier.\na suite of packages that implement tidy methods for data importing, cleaning, and wrangling.\nContains a series of packages useful for data analysis that work together well.\nAll packages included in tidyverse are automatically installed when installing the tidyverse package:\n\n\nCode`install.packages(\"tidyverse\")`\n\n\n\nCodetidyverse_packages() \n\n\n\nThen to work functions under tidyverse package we must always load the package into the workplace.\n\n\nCodelibrary(tidyverse) \n\n\n\nSome packages under tidyverse are considered core packages and others called friend packages."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#haven",
    "href": "static/slides/Datamanagment/datamanagment.html#haven",
    "title": "",
    "section": "haven()",
    "text": "haven()\n\n\nhaven is not a core member of the tidyverse, so you need to load it.\n\n\n\nImport data using haven()\n\n\n\nread_sas(): SAS\n\nread_spss(): SPSS\n\nread_sav(): SPSS\n\nread_por(): SPSS\n\nread_stata(): Stata\n\nread_dta(): Stata\n\n\nExport data\n\n\nwrite_csv(): Comma separated values\n\nwrite_excel_csv(): CSV to open in Excel\n\nwrite_delim(): General delimited files\n\nwrite_sas(): SAS .sas7bdat files\n\nwrite_sav(): SPSS .sav files\n\nwrite_stata(): Stata .dta files"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#intro-to-dplyr-package",
    "href": "static/slides/Datamanagment/datamanagment.html#intro-to-dplyr-package",
    "title": "",
    "section": "Intro to dplyr package",
    "text": "Intro to dplyr package\n\nThe dplyr provides a ‚Äúgrammar‚Äù (the verbs) for data manipulation and for operating on data frames in a tidy way.\nThe key operator and the essential verbs are:\n\n\n%&gt;%: the ‚Äúpipe‚Äù operator used to connect multiple verb actions together into a pipeline.\nselect(): return a subset of the columns of a data frame.\nmutate(): add new variables/columns or transform existing variables.\nfilter(): extract a subset of rows from a data frame based on logical conditions.\narrange(): reorder rows of a data frame according to single or multiple variables.\nsummarise() / summarize(): reduce each group to a single row by calculating aggregate measures.\n\n\nWe can have a look at the data and its structure by using the glimpse() function from the dplyr package."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#a-manipulating-variables",
    "href": "static/slides/Datamanagment/datamanagment.html#a-manipulating-variables",
    "title": "",
    "section": "a) Manipulating variables:",
    "text": "a) Manipulating variables:\nselect(): To extract variables\n\nOur first verb on the list is select which allows to keep or drop variables from your dataframe. Choosing your variables is the first step in cleaning your data.\n\n\n\n\nselect() is especially useful because it is quite flexible in its use to create new tables."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#the-covid-19-dataset",
    "href": "static/slides/Datamanagment/datamanagment.html#the-covid-19-dataset",
    "title": "",
    "section": "The COVID-19 dataset",
    "text": "The COVID-19 dataset\n\nLets us use a COVID-19 which is serological survey conducted in Yaounde, Cameroon in late 2020.\nThe survey estimated how many people had been infected with COVID-19 in the region, by testing for IgG and IgM antibodies.\nThe full dataset can be obtained from Zenodo, and the paper can be viewed here.\nThere are some demographic, socio-economic and COVID-related variables.\nThe results of the IgG and IgM antibody tests are in the columns igg_result and igm_result.\n\n\nCodelibrary(readr); library(dplyr)\nyaounde &lt;- yaounde_data &lt;- read_csv(\"C:/quarto_site/static/slides/Datamanagment/data/yaounde_data.csv\")"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#helper-functions-for-select",
    "href": "static/slides/Datamanagment/datamanagment.html#helper-functions-for-select",
    "title": "",
    "section": "Helper functions for select()\n",
    "text": "Helper functions for select()\n\ndplyr has a number of helper functions to make selecting easier by using patterns from the column names. Let‚Äôs take a look at some of these.\n\nstarts_with() and ends_with()\n\nThese two helpers work exactly as their names suggest!\n\nCodeyao %&gt;% select(starts_with(\"is_\")) # Columns that start with \"is\"\nyao %&gt;% select(ends_with(\"_result\")) # Columns that end with \"result\""
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#change-column-names-with-rename",
    "href": "static/slides/Datamanagment/datamanagment.html#change-column-names-with-rename",
    "title": "",
    "section": "Change column names with rename()\n",
    "text": "Change column names with rename()\n\n\ndplyr::rename() is used to change column names:\n\nCode## Rename `age` and `sex` to `patient_age` and `patient_sex`\nyaounde %&gt;% \n  rename(patient_age = age, patient_sex = sex)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#rename-within-select",
    "href": "static/slides/Datamanagment/datamanagment.html#rename-within-select",
    "title": "",
    "section": "Rename within select()\n",
    "text": "Rename within select()\n\nYou can also rename columns while selecting them:\n\nCode## Select `age` and `sex`, and rename them to `patient_age` and `patient_sex`\nyaounde %&gt;% \n  select(patient_age = age, patient_sex = sex)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#rename-within-select-1",
    "href": "static/slides/Datamanagment/datamanagment.html#rename-within-select-1",
    "title": "",
    "section": "Rename within select()\n",
    "text": "Rename within select()\n\n\nDelete columns you don‚Äôt need with - (remember, if you delete multiple columns use select(-c()) so that - is applied to all of them).\n\n\nCodeairquality %&gt;%\n  select(-c(2:3, 5)) %&gt;% # Delete columns with `-`\n  glimpse()\n\n\n\nDefine the chosen columns in a vector beforehand and then recall it with !!.\n\n\nCodeair_cols &lt;- c(\"Wind\", \"Tem\", \"Day\")\nairquality %&gt;%\n  select(!!air_cols) %&gt;% # Call a vector of column names with `!!`\n  glimpse()"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#rename-to-rename-variables",
    "href": "static/slides/Datamanagment/datamanagment.html#rename-to-rename-variables",
    "title": "",
    "section": "\nrename() : To rename variables",
    "text": "rename() : To rename variables\n\nIf we want to change lowercase variables to uppercase, we can use rename(), or rename_with().\n\n\nCodeairquality %&gt;%\n  rename(day = Day,\n         month = Month,\n         wind = Wind) %&gt;%  # renames only chosen columns\n  glimpse()\n\n\n\nIf we just want column names to be changed with a function, we can use rename_with which is useful in this case since we can rename them with tolower.\n\n\nCodeairquality %&gt;%\n  rename_with(tolower) %&gt;%\n  glimpse()\n# If you didn't want all of them renamed, you could specify columns with `.cols =`"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filter",
    "href": "static/slides/Datamanagment/datamanagment.html#filter",
    "title": "",
    "section": "filter()",
    "text": "filter()\n\nDropping abnormal data entries or keeping subsets of your data points is another essential aspect of data wrangling.\n\nLet‚Äôs go !\n\n\nCodeyao &lt;- yaounde %&gt;% \n  select(age, sex, weight_kg, highest_education, neighborhood, \n         occupation, is_smoker, is_pregnant, \n         igg_result, igm_result)\n#yao"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#relational-operators",
    "href": "static/slides/Datamanagment/datamanagment.html#relational-operators",
    "title": "",
    "section": "Relational operators",
    "text": "Relational operators\n\nThe == operator introduced above is an example of a ‚Äúrelational‚Äù operator, as it tests the relation between two values. Here is a list of some of these operators:\n\n\n\nOperator\nis TRUE if\n\n\nA &lt; B\nA is less than B\n\n\nA &lt;= B\nA is less than or equal to B\n\n\nA &gt; B\nA is greater than B\n\n\nA &gt;= B\nA is greater than or equal to B\n\n\nA == B\nA is equal to B\n\n\nA != B\nA is not equal to B\n\n\nA %in% B\nA is an element of B"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#combining-conditions-with-and",
    "href": "static/slides/Datamanagment/datamanagment.html#combining-conditions-with-and",
    "title": "",
    "section": "Combining conditions with & and |\n",
    "text": "Combining conditions with & and |\n\n\nWe can pass multiple conditions to a single filter() statement separated by commas:\n\n\nCode### keep respondents who are pregnant and are ex-smokers\nyao %&gt;% filter(is_pregnant == \"Yes\", is_smoker == \"Ex-smoker\") ## only one row\n\n\n\nWhen multiple conditions are separated by a comma, they are implicitly combined with an and (&)."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#negating-conditions-with",
    "href": "static/slides/Datamanagment/datamanagment.html#negating-conditions-with",
    "title": "",
    "section": "Negating conditions with !\n",
    "text": "Negating conditions with !\n\n\nTo negate conditions, we wrap them in !().\nBelow, we drop respondents who are children (less than 18 years) or who weigh less than 30kg:\n\n\nCode### drop respondents &lt; 18 years OR &lt; 30 kg\nyao %&gt;% filter(!(age &lt; 18 | weight_kg &lt; 30))\n\n\n\nThe ! operator is also used to negate %in% since R does not have an operator for NOT in.\n\n\nCode### drop respondents whose highest education is NOT \"Primary\" or \"Secondary\"\nyao %&gt;% filter(!(highest_education %in% c(\"Primary\", \"Secondary\")))"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#na-values",
    "href": "static/slides/Datamanagment/datamanagment.html#na-values",
    "title": "",
    "section": "\nNA values",
    "text": "NA values\n\nThe relational operators introduced so far do not work with NA.\n\nLet‚Äôs make a data subset to illustrate this.\n\nCodeyao_mini &lt;- yao %&gt;% \n  select(sex, is_pregnant) %&gt;% \n  slice(1,11,50,2) ## custom row order\n\nyao_mini\n\n\n\nIn yao_mini, the last respondent has an NA for the is_pregnant column, because he is male.\nTrying to select this row using == NA will not work.\n\n\nCodeyao_mini %&gt;% filter(is_pregnant == NA) ## does not work\nyao_mini %&gt;% filter(is_pregnant == \"NA\") ## does not work\n\n\n\nThis is because NA is a non-existent value. So R cannot evaluate whether it is ‚Äúequal to‚Äù or ‚Äúnot equal to‚Äù anything."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#how-to-use-mutate-function-in-r",
    "href": "static/slides/Datamanagment/datamanagment.html#how-to-use-mutate-function-in-r",
    "title": "",
    "section": "How to Use Mutate function in R",
    "text": "How to Use Mutate function in R\n\nThe dplyr library has the following functions that can be used to add additional variables to a data frame.\nmutate() ‚Äì adds new variables while retaining old variables to a data frame.\ntransmute() ‚Äì adds new variables and removes old ones from a data frame.\nmutate_all() ‚Äì changes every variable in a data frame simultaneously.\nmutate_at() ‚Äì changes certain variables by name.\nmutate_if() ‚Äì alterations all variables that satisfy a specific criterion"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#mutate",
    "href": "static/slides/Datamanagment/datamanagment.html#mutate",
    "title": "",
    "section": "mutate()",
    "text": "mutate()\n\nA data frame‚Äôs existing variables are preserved when new variables are added using the mutate() function.\nThe mutate() basic syntax is as follows.\n\n\nCodedata &lt;- mutate(new_variable = existing_variable/3)\n\n\n\n\ndata: the fresh data frame where the fresh variables will be placed\n\nnew_variable: the name of the new variable\n\nexisting_variable: the current data frame variable that you want to modify in order to generate a new variable\nSet the new column‚Äôs root sepal width to the sepal‚Äôs square root. variable width\n\n\nCodelibrary(dplyr)\ndata &lt;- head(iris)\ndata %&gt;% mutate(root_sepal_width = sqrt(Sepal.Width)) %&gt;% head(3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species root_sepal_width\n1          5.1         3.5          1.4         0.2  setosa         1.870829\n2          4.9         3.0          1.4         0.2  setosa         1.732051\n3          4.7         3.2          1.3         0.2  setosa         1.788854"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#transmute",
    "href": "static/slides/Datamanagment/datamanagment.html#transmute",
    "title": "",
    "section": "transmute()",
    "text": "transmute()\n\nA data frame‚Äôs variables are added and removed via the transmute() method.\nThe code that follows demonstrates how to eliminate all of the existing variables and add two new variables to a dataset.\n\n\nCodedata %&gt;% transmute(root_sepal_width = sqrt(Sepal.Width), \n                   root_petal_width = sqrt(Petal.Width))\n\n  root_sepal_width root_petal_width\n1         1.870829        0.4472136\n2         1.732051        0.4472136\n3         1.788854        0.4472136\n4         1.760682        0.4472136\n5         1.897367        0.4472136\n6         1.974842        0.6324555"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#mutate_all",
    "href": "static/slides/Datamanagment/datamanagment.html#mutate_all",
    "title": "",
    "section": "mutate_all()",
    "text": "mutate_all()\n\nThe mutate_all() function changes every variable in a data frame at once, enabling you to use the funs() function to apply a certain function to every variable.\nThe use of mutate_all() to divide each column in a data frame by ten is demonstrated in the code below.\ndivide 10 from each of the data frame‚Äôs variables.\n\n\nCodedata2 &lt;- head(iris) %&gt;% select(-Species)\ndata2 %&gt;% mutate_all(funs(./10))\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1         0.51        0.35         0.14        0.02\n2         0.49        0.30         0.14        0.02\n3         0.47        0.32         0.13        0.02\n4         0.46        0.31         0.15        0.02\n5         0.50        0.36         0.14        0.02\n6         0.54        0.39         0.17        0.04"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#mutate_at",
    "href": "static/slides/Datamanagment/datamanagment.html#mutate_at",
    "title": "",
    "section": "mutate_at()",
    "text": "mutate_at()\n\nUsing names, the mutate at() function changes particular variables.\nThe use of mutate_at() to divide two particular variables by 10 is demonstrated in the code below:\n\n\nCodedata2 %&gt;% mutate_at(c(\"Sepal.Length\", \"Sepal.Width\"), funs(mod = ./10))\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_mod\n1          5.1         3.5          1.4         0.2             0.51\n2          4.9         3.0          1.4         0.2             0.49\n3          4.7         3.2          1.3         0.2             0.47\n4          4.6         3.1          1.5         0.2             0.46\n5          5.0         3.6          1.4         0.2             0.50\n6          5.4         3.9          1.7         0.4             0.54\n  Sepal.Width_mod\n1            0.35\n2            0.30\n3            0.32\n4            0.31\n5            0.36\n6            0.39"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#mutate_if",
    "href": "static/slides/Datamanagment/datamanagment.html#mutate_if",
    "title": "",
    "section": "mutate_if()",
    "text": "mutate_if()\n\nAll variables that match a specific condition are modified by the mutate_if() function.\nThe mutate_if() function can be used to change any variables of type factor to type character, as shown in the code below.\n\n\nCodedata &lt;- head(iris)\nsapply(data, class)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\" \n\n\n\nevery factor variable can be converted to a character variable.\n\n\nCodenew_data &lt;- data %&gt;% mutate_if(is.factor, as.character)\nsapply(new_data, class)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"  \"character\""
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#use-across-inside-mutate-function",
    "href": "static/slides/Datamanagment/datamanagment.html#use-across-inside-mutate-function",
    "title": "",
    "section": "Use across() inside mutate() function",
    "text": "Use across() inside mutate() function\n\nCodeiris %&gt;% as_tibble() %&gt;% \n  mutate(across(c(Sepal.Length, Sepal.Width), round)) %&gt;% head(4) #&lt;&lt;\n\n# A tibble: 4 √ó 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1            5           4          1.4         0.2 setosa \n2            5           3          1.4         0.2 setosa \n3            5           3          1.3         0.2 setosa \n4            5           3          1.5         0.2 setosa \n\n\n\nCodeiris %&gt;% as_tibble() %&gt;%  \n  mutate(across(c(1, 2), round)) %&gt;% head(4)#&lt;&lt;\n\n# A tibble: 4 √ó 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1            5           4          1.4         0.2 setosa \n2            5           3          1.4         0.2 setosa \n3            5           3          1.3         0.2 setosa \n4            5           3          1.5         0.2 setosa"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#use-across-inside-mutate-function-1",
    "href": "static/slides/Datamanagment/datamanagment.html#use-across-inside-mutate-function-1",
    "title": "",
    "section": "Use across() inside mutate() function",
    "text": "Use across() inside mutate() function\n\nCodeiris %&gt;%  as_tibble() %&gt;% \n  mutate(across(1:Sepal.Width, round)) %&gt;% head(4) #&lt;&lt;\n\n# A tibble: 4 √ó 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1            5           4          1.4         0.2 setosa \n2            5           3          1.4         0.2 setosa \n3            5           3          1.3         0.2 setosa \n4            5           3          1.5         0.2 setosa \n\n\n\nCodeiris %&gt;%   as_tibble() %&gt;% \n  mutate(across(where(is.double) & !c(Petal.Length, Petal.Width), round)) %&gt;% head(4) #&lt;&lt;\n\n# A tibble: 4 √ó 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1            5           4          1.4         0.2 setosa \n2            5           3          1.4         0.2 setosa \n3            5           3          1.3         0.2 setosa \n4            5           3          1.5         0.2 setosa"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#b-manipulating-cases",
    "href": "static/slides/Datamanagment/datamanagment.html#b-manipulating-cases",
    "title": "",
    "section": "b) Manipulating cases",
    "text": "b) Manipulating cases\n\nfilter(): To extract cases\n\nThe function filter() is used to filter the dataset to return a subset of all rows that meet one or more specific conditions.\nfilter(dataframe, logical statement 1, logical statement 2, ...)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filtering-rows-based-on-a-numeric-variable",
    "href": "static/slides/Datamanagment/datamanagment.html#filtering-rows-based-on-a-numeric-variable",
    "title": "",
    "section": "Filtering rows based on a numeric variable",
    "text": "Filtering rows based on a numeric variable\n\n\n\nCodemsleep &lt;- ggplot2::msleep\nmsleep %&gt;% \n  select(name, sleep_total) %&gt;% \n  filter(sleep_total &gt; 18) %&gt;% head()\n\n\n\n\n\n# A tibble: 4 √ó 2\n  name                 sleep_total\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Big brown bat               19.7\n2 Thick-tailed opposum        19.4\n3 Little brown bat            19.9\n4 Giant armadillo             18.1\n\n\n\n\n\nTo select all animals with a total sleep time between 15 and 18 hours, use: filter(sleep_total &gt;= 16, sleep_total &lt;= 18), but there is a slightly shorter way by using the between() function.\n\n\n\n\nCodemsleep %&gt;% \n  select(name, sleep_total) %&gt;% \n  filter(between(sleep_total, 16, 18)) \n\n\n\n\n\n# A tibble: 4 √ó 2\n  name                   sleep_total\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Owl monkey                    17  \n2 Long-nosed armadillo          17.4\n3 North American Opossum        18  \n4 Arctic ground squirrel        16.6"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filtering-based-on-exact-character-variable-matches",
    "href": "static/slides/Datamanagment/datamanagment.html#filtering-based-on-exact-character-variable-matches",
    "title": "",
    "section": "Filtering based on exact character variable matches",
    "text": "Filtering based on exact character variable matches\n\nIf you want to select a specific group of animals for instance you can use the == comparison operator:\n\n\nCodemsleep %&gt;% \n  select(order, name, sleep_total) %&gt;% \n  filter(order == \"Didelphimorphia\") %&gt;% head()\n\n# A tibble: 2 √ó 3\n  order           name                   sleep_total\n  &lt;chr&gt;           &lt;chr&gt;                        &lt;dbl&gt;\n1 Didelphimorphia North American Opossum        18  \n2 Didelphimorphia Thick-tailed opposum          19.4\n\n\n\nSimilarly you can use the other operators:\n\n\nfilter(order != \"Rodentia\") will select everything except the Rodentia rows."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filtering-based-on-multiple-conditions",
    "href": "static/slides/Datamanagment/datamanagment.html#filtering-based-on-multiple-conditions",
    "title": "",
    "section": "Filtering based on multiple conditions",
    "text": "Filtering based on multiple conditions\n\nThe above examples return rows based on a single condition, but the filter option also allows AND and OR style filters:\n\n\n\nfilter(condition1, condition2) will return rows where both conditions are met.\n\nfilter(condition1, !condition2) will return all rows where condition one is true but condition 2 is not.\n\nfilter(condition1 | condition2) will return rows where condition 1 and/or condition 2 is met.\n\nfilter(xor(condition1, condition2) will return all rows where only one of the conditions is met, and not when both conditions are met.\n\n\nMultiple AND, OR and NOT conditions can be combined. The sample code will return all rows with a bodywt above 100 and either have a sleep_total above 15 or are not part of the Carnivora order.\n\n\nCodemsleep %&gt;% \n  select(name, order, sleep_total:bodywt) %&gt;% \n  filter(bodywt &gt; 100, (sleep_total &gt; 15 | order != \"Carnivora\"))"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#example-with-xor",
    "href": "static/slides/Datamanagment/datamanagment.html#example-with-xor",
    "title": "",
    "section": "Example with xor()\n",
    "text": "Example with xor()\n\n\n\n\nCodemsleep %&gt;%\n  select(name, bodywt:brainwt) %&gt;% \n  filter(xor(bodywt &gt; 100, brainwt &gt; 1))\n\n\n\n\n\n# A tibble: 5 √ó 3\n  name            bodywt brainwt\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Cow               600    0.423\n2 Horse             521    0.655\n3 Donkey            187    0.419\n4 Human              62    1.32 \n5 Brazilian tapir   208.   0.169\n\n\n\n\nExample with !:\n\nThe sample code will select all rows where brainwt is larger than 1, but bodywt does not exceed 100.\n\n\nCodemsleep %&gt;% \n  select(name, sleep_total, brainwt, bodywt) %&gt;% \n  filter(brainwt &gt; 1, !bodywt &gt; 100) \n\n# A tibble: 1 √ó 4\n  name  sleep_total brainwt bodywt\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Human           8    1.32     62"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filtering-out-empty-rows",
    "href": "static/slides/Datamanagment/datamanagment.html#filtering-out-empty-rows",
    "title": "",
    "section": "Filtering out empty rows",
    "text": "Filtering out empty rows\n\nTo filter out empty rows, you negate the is.na() function inside a filter: The sample code will remove any rows where the conservation is NA.\n\n\nCodemsleep %&gt;% \n  select(name, conservation:sleep_cycle) %&gt;% \n  filter(!is.na(conservation))"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filtering-across-multiple-columns",
    "href": "static/slides/Datamanagment/datamanagment.html#filtering-across-multiple-columns",
    "title": "",
    "section": "Filtering across multiple columns",
    "text": "Filtering across multiple columns\n\nHow dplyr package filter across multiple columns in one go?\nfilter_all(): will filter all columns\nfilter_if(): filter columns based on a function.\nfilter_at(): requires specify columns inside a vars() argument for which the filtering will be done.\nIn many cases you will need a . operator within the condition which refers to the values we are looking at."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filter_all",
    "href": "static/slides/Datamanagment/datamanagment.html#filter_all",
    "title": "",
    "section": "filter_all()",
    "text": "filter_all()\n\n\nUsed to wrap the condition in any_vars().\n\nFor example to find the string ‚ÄúCa‚Äù across all columns,\n\n\nCodemsleep %&gt;% \n  select(name:order, sleep_total, -vore) %&gt;% \n  filter_all(any_vars(str_detect(., pattern = \"Ca\")))\n\n\n\nThe same can be done for numerical values: This code will retain any rows that has any value below 0.1:\n\n\nCodemsleep %&gt;%  \n  select(name, sleep_total:bodywt) %&gt;% \n  filter_all(any_vars(. &lt; 0.1))"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filter_at",
    "href": "static/slides/Datamanagment/datamanagment.html#filter_at",
    "title": "",
    "section": "filter_at()",
    "text": "filter_at()\n\nOne of the more powerful functions is filter_at(): it does not filter all columns, nor does it need you to specify the type of column, you can just select columns to which the change should happen via the vars() argument.\nUse all_vars() if all columns need to return TRUE, or any_vars() in case just one variable needs to return TRUE.\nExample: refer to columns by their name:\n\n\nCodemsleep %&gt;% \n  select(name, sleep_total:sleep_rem, brainwt:bodywt) %&gt;% \n  filter_at(vars(sleep_total, sleep_rem), all_vars(.&gt;5))\n\n\n\nExample: using another select option:\n\n\nCodemsleep %&gt;% \n  select(name, sleep_total:sleep_rem, brainwt:bodywt) %&gt;% \n  filter_at(vars(contains(\"sleep\")), all_vars(.&gt;5))"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#sort-rows-with-arrange",
    "href": "static/slides/Datamanagment/datamanagment.html#sort-rows-with-arrange",
    "title": "",
    "section": "Sort rows with arrange\n",
    "text": "Sort rows with arrange\n\nRe-order rows by a particular column, by default in ascending order\nUse desc() for descending order.\narrange(data, variable1, desc(variable2), ...)\nExample: 1. Let us use the following code to create a scrambled version of the airquality dataset\n\nCodeair_mess &lt;- sample_frac(airquality, 1)\nhead(air_mess)\n\n  Ozone Solar.R Wind Temp Month Day\n1    30     322 11.5   68     5  19\n2    NA     220  8.6   85     6   5\n3    76     203  9.7   97     8  28\n4   122     255  4.0   89     8   7\n5    NA      91  4.6   76     6  23\n6     7      49 10.3   69     9  24"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#sort-rows-with-arrange-1",
    "href": "static/slides/Datamanagment/datamanagment.html#sort-rows-with-arrange-1",
    "title": "",
    "section": "Sort rows with arrange()\n",
    "text": "Sort rows with arrange()\n\nExample: 2. Now let us arrange the data frame back into chronological order, sorting by Month then Day\n\nCodeair_chron &lt;- arrange(air_mess, Month, Day)\nhead(air_chron)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n.comment[Try : arrange(air_mess, Day, Month) and see the difference.]"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#pipe-perator",
    "href": "static/slides/Datamanagment/datamanagment.html#pipe-perator",
    "title": "",
    "section": "Pipe perator (%>%)",
    "text": "Pipe perator (%&gt;%)\n\nPipes in R look like %&gt;% and strings together commands to be performed sequentially\nThe pipe passes the data frame output that results from the function right before the pipe to input it as the first argument of the function right after the pipe.\n\n\nCodethird(second(first(x)))\n\n\n\nThis nesting is not a natural way to think about a sequence of operations.\nThe %&gt;% operator allows you to string operations in a left-to-right fashion.\n\n\nCodefirst(x) %&gt;%\nsecond %&gt;% third"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#advantages-of-pipe-oprator",
    "href": "static/slides/Datamanagment/datamanagment.html#advantages-of-pipe-oprator",
    "title": "",
    "section": "Advantages of Pipe oprator",
    "text": "Advantages of Pipe oprator\n\nPipes used to reduce multiple steps, that can be hard to keep track of.\nless redundant code\nEasy to read and write because functions are executed in order\n\nDifficult to read if too many functions are nested\n\n\nLook at the three syntax\n\n\nCodedata1&lt;-filter(sampledata, Age &gt; 15) #&lt;&lt;\ndata2&lt;-select(data1, Sex, Weight1, Age) #&lt;&lt;\n\n\n\nCodenon_piped &lt;-select(filter(mydata, Age&gt;15), Sex, Weight1, Age) #&lt;&lt;\n\n\n\nCodepipeddata&lt;-mydata %&gt;% filter(Age &gt; 15) %&gt;% select(Sex, Weight1, Height1, Age)#&lt;&lt;"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#group_by-and-summarise",
    "href": "static/slides/Datamanagment/datamanagment.html#group_by-and-summarise",
    "title": "",
    "section": "\ngroup_by() and summarise()\n",
    "text": "group_by() and summarise()\n\n\nThe dplyr verbs become especially powerful when they are are combined using the pipe operator %&gt;%.\nThe following dplyr functions allow us to split our data frame into groups on which we can perform operations individually\ngroup_by(): group data frame by a factor for downstream operations (usually summarise)\nsummarise(): summarise values in a data frame or in groups within the data frame with aggregation functions (e.g.¬†min(), max(), mean(), etc‚Ä¶)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#dplyr---split-apply-combine",
    "href": "static/slides/Datamanagment/datamanagment.html#dplyr---split-apply-combine",
    "title": "",
    "section": "\ndplyr - Split-Apply-Combine",
    "text": "dplyr - Split-Apply-Combine\nThe group_by function is key to the Split-Apply-Combine strategy"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#the-summarize-function",
    "href": "static/slides/Datamanagment/datamanagment.html#the-summarize-function",
    "title": "",
    "section": "The summarize() function",
    "text": "The summarize() function\n\nThe summarize() function is used in the R program to summarize the data frame into just one value or vector.\nThis summarization is done through grouping observations by using categorical values at first, using the group_by() function.\nThe summarize() function offers the summary that is based on the action done on grouped or ungrouped data."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#summarize-grouped-data",
    "href": "static/slides/Datamanagment/datamanagment.html#summarize-grouped-data",
    "title": "",
    "section": "Summarize grouped data",
    "text": "Summarize grouped data\n\nThe operations that can be performed on grouped data are average, factor, count, mean, etc.\nExample: we are interested in the mean temperature and standard deviation within each month of the airquality dataset\n\n\nCodemonth_sum &lt;- airquality %&gt;%\n      group_by(Month) %&gt;%\n      summarise(mean_temp = mean(Temp),\n                sd_temp = sd(Temp))\nmonth_sum\n\n# A tibble: 5 √ó 3\n  Month mean_temp sd_temp\n  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     5      65.5    6.85\n2     6      79.1    6.60\n3     7      83.9    4.32\n4     8      84.0    6.59\n5     9      76.9    8.36"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#summarize-ungrouped-data",
    "href": "static/slides/Datamanagment/datamanagment.html#summarize-ungrouped-data",
    "title": "",
    "section": "Summarize ungrouped data",
    "text": "Summarize ungrouped data\n\nWe can also summarize ungrouped data. This can be done by using three functions.\nsummarize_all()\nsummarize_at()\nsummazrize_if()\n\n1. summarize_all()\n\nThis function summarizes all the columns of data based on the action which is to be performed.\n\nsummarize_all(action)\n\nexample The code airquality %&gt;% summarize_all(mean) will show the mean of all columns.\n\n\nCode# Caculating mean value.\nairquality %&gt;% summarize_all(mean, na.rm=T)\n\n     Ozone  Solar.R     Wind     Temp    Month      Day\n1 42.12931 185.9315 9.957516 77.88235 6.993464 15.80392"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#summarize_at",
    "href": "static/slides/Datamanagment/datamanagment.html#summarize_at",
    "title": "",
    "section": "2. summarize_at()",
    "text": "2. summarize_at()\n\nIt performs the action on the specific column and generates the summary based on that action.\n\n\nCodesummarize_at(vector_of_columns, action)\n\n-   `vector_of_columns`: The list of column names or character vector of column names.\n\n-   The `airquality %&gt;% group_by(Month) %&gt;% summarize_at(c(\"Wind\",\"Temp\"),mean)` will show the mean of the `'Wind'` and `'Temp'` observations in the result, grouping with `Month`.\n\nairquality %&gt;% group_by(Month) %&gt;%\nsummarize_at(c(\"Wind\",\"Temp\"),mean)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#summarize_if",
    "href": "static/slides/Datamanagment/datamanagment.html#summarize_if",
    "title": "",
    "section": "3. summarize_if()",
    "text": "3. summarize_if()\n\nIn this function, we specify a condition and the summary will be generated if the condition is satisfied.\n\n\nCodesummarize_if(.predicate, .action)\n\n-   In the code snippet below, we use the `predicate` function `is.numeric` and `mean` as an action.\n\nz&lt;- head(data)\nairquality %&gt;% group_by(Month) %&gt;%\nsummarize_if(is.numeric, mean)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#recoding-variables",
    "href": "static/slides/Datamanagment/datamanagment.html#recoding-variables",
    "title": "",
    "section": "Recoding Variables",
    "text": "Recoding Variables\n\nuse recode() inside a mutate() statement.\n\nExample of Recoding\n\nCodedata_diet &lt;- tibble(Diet = rep(c(\"A\", \"B\", \"B\"), times = 4), \n                    Gender = c(\"Male\",\"m\",\"Other\",\"F\",\"Female\",\"M\",\n                               \"f\",\"O\",\"Man\",\"f\",\"F\",\"O\"), \n                    Weight_start = sample(100:250, size = 12),\n                    Weight_change = sample(-10:20, size = 12))\nhead(data_diet)\n\n# A tibble: 6 √ó 4\n  Diet  Gender Weight_start Weight_change\n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n1 A     Male            241            -2\n2 B     m               113            17\n3 B     Other           134             4\n4 A     F               188             5\n5 B     Female          161            10\n6 B     M               249            12\n\n\n\nSay we have some data about samples in a diet study but this needs lots of recoding.\n\n\nCodedata_diet %&gt;%\n  count(Gender)\n\n# A tibble: 9 √ó 2\n  Gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 F          2\n2 Female     1\n3 M          1\n4 Male       1\n5 Man        1\n6 O          2\n7 Other      1\n8 f          2\n9 m          1"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#dplyr-can-help",
    "href": "static/slides/Datamanagment/datamanagment.html#dplyr-can-help",
    "title": "",
    "section": "\ndplyr can help!",
    "text": "dplyr can help!\nUsing Excel to find all of the different ways gender has been coded, could be hectic!\nIn dplyr you can use the recode function (need mutate here too!):\n\nCode# General Format - this is not code!\n{data_input} %&gt;%\n  mutate({variable_to_fix} = recode({Variable_fixing}, {old_value} = {new_value},\n                                    {another_old_value} = {new_value}))\n\n\n\nCodedata_diet %&gt;% \n  mutate(Gender = recode(Gender, M = \"Male\", m = \"Male\", Man = \"Male\",\n                                 O = \"Other\", f = \"Female\",F = \"Female\")) %&gt;%\n  count(Gender, Diet)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#recode",
    "href": "static/slides/Datamanagment/datamanagment.html#recode",
    "title": "",
    "section": "recode()",
    "text": "recode()\n\nCodedata_diet %&gt;% \n  mutate(Gender = recode(Gender, M = \"Male\", m = \"Male\", Man = \"Male\",\n                                 O = \"Other\",f = \"Female\",F = \"Female\")) %&gt;%\n  count(Gender, Diet)\n\n# A tibble: 5 √ó 3\n  Gender Diet      n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 Female A         3\n2 Female B         2\n3 Male   A         1\n4 Male   B         3\n5 Other  B         3"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#or-you-can-use-case_when",
    "href": "static/slides/Datamanagment/datamanagment.html#or-you-can-use-case_when",
    "title": "",
    "section": "Or you can use case_when()\n",
    "text": "Or you can use case_when()\n\nThe case_when() function of dplyr can help us to do this as well.\n\n\n\n\nNote that automatically values not reassigned explicitly by case_when() will be NA unless otherwise specified.\n\n\nCodedata_diet %&gt;% \n  mutate(Gender = case_when(Gender == \"M\" ~ \"Male\"))\n\n# A tibble: 12 √ó 4\n   Diet  Gender Weight_start Weight_change\n   &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n 1 A     &lt;NA&gt;            241            -2\n 2 B     &lt;NA&gt;            113            17\n 3 B     &lt;NA&gt;            134             4\n 4 A     &lt;NA&gt;            188             5\n 5 B     &lt;NA&gt;            161            10\n 6 B     Male            249            12\n 7 A     &lt;NA&gt;            169            -3\n 8 B     &lt;NA&gt;            222           -10\n 9 B     &lt;NA&gt;            211             8\n10 A     &lt;NA&gt;            245             9\n11 B     &lt;NA&gt;            115            -9\n12 B     &lt;NA&gt;            179            -1"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#use-of-case_when-without-automatic-na",
    "href": "static/slides/Datamanagment/datamanagment.html#use-of-case_when-without-automatic-na",
    "title": "",
    "section": "Use of case_when() without automatic NA\n",
    "text": "Use of case_when() without automatic NA\n\n\n\n\n\nHere we use the original values of Gender to replace all values of Gender that do not meet the condition == \"M\".\n\n\nCodedata_diet %&gt;% \n  mutate(Gender = case_when(Gender == \"M\" ~ \"Male\", TRUE ~ Gender))\n\n# A tibble: 12 √ó 4\n   Diet  Gender Weight_start Weight_change\n   &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n 1 A     Male            241            -2\n 2 B     m               113            17\n 3 B     Other           134             4\n 4 A     F               188             5\n 5 B     Female          161            10\n 6 B     Male            249            12\n 7 A     f               169            -3\n 8 B     O               222           -10\n 9 B     Man             211             8\n10 A     f               245             9\n11 B     F               115            -9\n12 B     O               179            -1"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#more-complicated-case_when",
    "href": "static/slides/Datamanagment/datamanagment.html#more-complicated-case_when",
    "title": "",
    "section": "More complicated case_when()",
    "text": "More complicated case_when()\n\nCodedata_diet %&gt;% \n  mutate(Gender = case_when(\n    Gender %in% c(\"M\", \"male\", \"Man\", \"m\", \"Male\") ~ \"Male\",\n    Gender %in% c(\"F\", \"Female\", \"f\", \"female\") ~ \"Female\",\n    Gender %in% c(\"O\", \"Other\") ~ \"Other\")) %&gt;% head()\n\n# A tibble: 6 √ó 4\n  Diet  Gender Weight_start Weight_change\n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n1 A     Male            241            -2\n2 B     Male            113            17\n3 B     Other           134             4\n4 A     Female          188             5\n5 B     Female          161            10\n6 B     Male            249            12"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#another-reason-for-case_when",
    "href": "static/slides/Datamanagment/datamanagment.html#another-reason-for-case_when",
    "title": "",
    "section": "Another reason for case_when()\n",
    "text": "Another reason for case_when()\n\ncase_when can do very sophisticated comparisons\n\nCodedata_diet1 &lt;-data_diet %&gt;% \n      mutate(Effect = case_when(Weight_change &gt; 0 ~ \"Increase\",\n                                Weight_change == 0 ~ \"Same\",\n                                Weight_change &lt; 0 ~ \"Decrease\"))\nhead(data_diet)\n\n# A tibble: 6 √ó 4\n  Diet  Gender Weight_start Weight_change\n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n1 A     Male            241            -2\n2 B     m               113            17\n3 B     Other           134             4\n4 A     F               188             5\n5 B     Female          161            10\n6 B     M               249            12"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#creating-new-discrete-column-with-two-levels",
    "href": "static/slides/Datamanagment/datamanagment.html#creating-new-discrete-column-with-two-levels",
    "title": "",
    "section": "Creating new discrete column with two levels\n",
    "text": "Creating new discrete column with two levels\n\n\nThe ifelse() statement can be used to turn a numeric column into a discrete one.\n\n\nCodedata_diet %&gt;%\n  mutate(Temp_cat = ifelse(Weight_change &gt; 0, \"Increased\", \"decreased\")) %&gt;% head()\n\n# A tibble: 6 √ó 5\n  Diet  Gender Weight_start Weight_change Temp_cat \n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt; &lt;chr&gt;    \n1 A     Male            241            -2 decreased\n2 B     m               113            17 Increased\n3 B     Other           134             4 Increased\n4 A     F               188             5 Increased\n5 B     Female          161            10 Increased\n6 B     M               249            12 Increased"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#working-with-strings-by-stringr-package",
    "href": "static/slides/Datamanagment/datamanagment.html#working-with-strings-by-stringr-package",
    "title": "",
    "section": "Working with strings by stringr package",
    "text": "Working with strings by stringr package\nThe stringr package:\n\nModifying or finding part or all of a character string\nWe will not cover grep or gsub - base R functions\n\nare used on forums for answers\n\n\nAlmost all functions start with str_*"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#str_detect",
    "href": "static/slides/Datamanagment/datamanagment.html#str_detect",
    "title": "",
    "section": "str_detect()",
    "text": "str_detect()\n\nstr_detect, and str_replace search for matches to argument pattern within each element of a character vector (not data frame or tibble!).\nstr_detect - returns TRUE if pattern is found\nstr_replace - replaces pattern with replacement"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#str_replace",
    "href": "static/slides/Datamanagment/datamanagment.html#str_replace",
    "title": "",
    "section": "str_replace()",
    "text": "str_replace()\n\nThe replacement argument specifies what to replace the pattern with\n\n\n\nCodex&lt;-c(\"cat\", \"dog\", \"mouse\")\nstr_replace(string = x, pattern = \"d\", replacement = \"D\")\n\n[1] \"cat\"   \"Dog\"   \"mouse\""
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filter-and-stringr-functions",
    "href": "static/slides/Datamanagment/datamanagment.html#filter-and-stringr-functions",
    "title": "",
    "section": "\nfilter and stringr functions",
    "text": "filter and stringr functions\n\nCodehead(data_diet,n = 4)\n\n# A tibble: 4 √ó 4\n  Diet  Gender Weight_start Weight_change\n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n1 A     Male            241            -2\n2 B     m               113            17\n3 B     Other           134             4\n4 A     F               188             5\n\nCodedata_diet %&gt;% \n  filter(str_detect(string = Gender,\n                    pattern = \"M\"))\n\n# A tibble: 3 √ó 4\n  Diet  Gender Weight_start Weight_change\n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n1 A     Male            241            -2\n2 B     M               249            12\n3 B     Man             211             8"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#case_when-improved-with-stringr",
    "href": "static/slides/Datamanagment/datamanagment.html#case_when-improved-with-stringr",
    "title": "",
    "section": "\ncase_when() improved with stringr\n",
    "text": "case_when() improved with stringr\n\n\nCodedata_diet %&gt;% \n  mutate(Gender = case_when(\n    Gender %in% c(\"M\", \"male\", \"Man\", \"m\", \"Male\") ~ \"Male\",\n    Gender %in% c(\"F\", \"Female\", \"f\", \"female\")~ \"Female\",\n    Gender %in% c(\"O\", \"Other\") ~ \"Other\")) %&gt;% head()\n\n# A tibble: 6 √ó 4\n  Diet  Gender Weight_start Weight_change\n  &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;         &lt;int&gt;\n1 A     Male            241            -2\n2 B     Male            113            17\n3 B     Other           134             4\n4 A     Female          188             5\n5 B     Female          161            10\n6 B     Male            249            12"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#case_when-improved-with-stringr-1",
    "href": "static/slides/Datamanagment/datamanagment.html#case_when-improved-with-stringr-1",
    "title": "",
    "section": "\ncase_when() improved with stringr\n",
    "text": "case_when() improved with stringr\n\n\n^ indicates the beginning of a character string\n$ indicates the end\n\n\nCodedata_diet %&gt;% \n  mutate(Gender = case_when(\n    str_detect(string = Gender, pattern = \"^m|^M\") ~ \"Male\",\n    str_detect(string = Gender, pattern = \"^f|^F\") ~ \"Female\",\n    str_detect(string = Gender, pattern = \"^o|^O\") ~ \"Other\")) %&gt;%\n  count(Gender)\n\n# A tibble: 3 √ó 2\n  Gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Female     5\n2 Male       4\n3 Other      3"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#data-mergining",
    "href": "static/slides/Datamanagment/datamanagment.html#data-mergining",
    "title": "",
    "section": "Data mergining",
    "text": "Data mergining\n\n\n\nThe 4 mutating join verbs:\nleft_join()\nright_join()\ninner_join()\nfull_join()\n\n\n\nThe 2 binding join verbs:\nbind_rows()\nbind_cols()\n\n\n\n\n\n\nThe 2 filtering join verbs:\nsemi_join()\nanti_join()\n\n\n\nThe 3 set operations:\nintersect()\nunion()\nsetdiff()"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#left_join",
    "href": "static/slides/Datamanagment/datamanagment.html#left_join",
    "title": "",
    "section": "left_join()",
    "text": "left_join()\n\nA left_join keeps all the data from the first (left) table and joins anything that matches from the second (right) table.\nIf the right table has more than one match for a row in the right table, there will be more than one row in the joined table (see ids 4 and 5).\n\n\n\n\nleft_joint()\n\n\nCodeleft_join(subject, exp, by = \"id\")\n\n\n\n\n\n\n\nThe order of the exp and subject tables is different.\n\n\nCodeleft_join(exp, subject, by = \"id\")"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#right_join",
    "href": "static/slides/Datamanagment/datamanagment.html#right_join",
    "title": "",
    "section": "right_join()",
    "text": "right_join()\n\nA right_join keeps all the data from the second (right) table and joins anything that matches from the first (left) table.\n\n\nCoderight_join(subject, exp, by = \"id\")\n\n\n\n\n\n\nThis table has the same information as left_join(exp, subject, by = \"id\"), but the columns are in a different order (left table, then right table)."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#inner_join",
    "href": "static/slides/Datamanagment/datamanagment.html#inner_join",
    "title": "",
    "section": "inner_join()",
    "text": "inner_join()\n\nAn inner_join returns all the rows that have a match in the other table.\n\n\nCodeinner_join(subject, exp, by = \"id\")"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#full_join",
    "href": "static/slides/Datamanagment/datamanagment.html#full_join",
    "title": "",
    "section": "full_join()",
    "text": "full_join()\n\nA full_join lets you join up rows in two tables while keeping all of the information from both tables.\nIf a row doesn‚Äôt have a match in the other table, the other table‚Äôs column values are set to NA.\n\n\nCodefull_join(subject, exp, by = \"id\")"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#semi_join",
    "href": "static/slides/Datamanagment/datamanagment.html#semi_join",
    "title": "",
    "section": "semi_join()",
    "text": "semi_join()\n\nA semi_join returns all rows from the left table where there are matching values in the right table, keeping just columns from the left table.\n\n\nCodesemi_join(subject, exp, by = \"id\")\n\n\n\n\n\n\nUnlike an inner join, a semi join will never duplicate the rows in the left table if there is more than one maching row in the right table.\n\nOrder matters in a semi join.\n\nCodesemi_join(exp, subject, by = \"id\")"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#anti_join",
    "href": "static/slides/Datamanagment/datamanagment.html#anti_join",
    "title": "",
    "section": "anti_join()",
    "text": "anti_join()\n\nA anti_join() return all rows from the left table where there are not matching values in the right table, keeping just columns from the left table.\n\n\n\n\nanti_join().\n\n\nCodeanti_join(subject, exp, by = \"id\")\n\n\n\n\n\n\n\nOrder matters in an anti_join().\n\n\nCodeanti_join(exp, subject, by = \"id\")"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#bind_rows",
    "href": "static/slides/Datamanagment/datamanagment.html#bind_rows",
    "title": "",
    "section": "bind_rows()",
    "text": "bind_rows()\n\nYou can combine the rows of two tables with bind_rows.\nHere we‚Äôll add subject data for subjects 6-9 and bind that to the original subject table.\n\n\nCodenew_subjects &lt;- tibble(\n  id = seq(6, 9),  age = c(19, 16, 20, 19),sex = c(\"m\", \"m\", \"f\", \"f\"))\n\n\n\nCodebind_rows(subject, new_subjects)\n\n\n\n\n\n\nThe columns just have to have the same names, they don‚Äôt have to be in the same order."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#bind_cols",
    "href": "static/slides/Datamanagment/datamanagment.html#bind_cols",
    "title": "",
    "section": "bind_cols()",
    "text": "bind_cols()\n\nYou can merge two tables with the same number of rows using bind_cols.\nThis is only useful if the two tables have their rows in the exact same order.\nThe only advantage over a left join is when the tables don‚Äôt have any IDs to join by and you have to rely solely on their order.\n\n\nCodenew_info &lt;- tibble(\n  colour = c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\"))\n\n\n\nCodebind_cols(subject, new_info)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#intersect",
    "href": "static/slides/Datamanagment/datamanagment.html#intersect",
    "title": "",
    "section": "intersect()",
    "text": "intersect()\n\n\nintersect() returns all rows in two tables that match exactly.\n\nunion() returns all the rows from both tables, removing duplicate rows.\n\n\nCodenew_subjects &lt;- tibble(\n  id = seq(4, 9), age = c(19, 18, 19, 16, 20, 19),\n  sex = c(\"f\", \"f\", \"m\", \"m\", \"f\", \"f\"))\n\n\n\n\n\nIntersect:\n\n\nCodedplyr::intersect(subject, new_subjects)\n\n\n\n\n\n\n\nUnion:\n\n\nCodedplyr::union(subject, new_subjects)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#setdiff",
    "href": "static/slides/Datamanagment/datamanagment.html#setdiff",
    "title": "",
    "section": "setdiff()",
    "text": "setdiff()\n\n\nsetdiff returns rows that are in the first table, but not in the second table.\n\n::: columns ::: {.column width=‚Äú50%‚Äù}\n\nsetdiff()\n\n\nCodesetdiff(subject, new_subjects)\n\n\n\n\n\n\n\n\nOrder matters for setdiff.\n\n\nCodesetdiff(new_subjects, subject)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#converting-data-from-wide-to-long-or-long-to-wide",
    "href": "static/slides/Datamanagment/datamanagment.html#converting-data-from-wide-to-long-or-long-to-wide",
    "title": "",
    "section": "Converting data from wide to long or long to wide",
    "text": "Converting data from wide to long or long to wide\nReshaping data using tidyr package\n\nFollowing are the four important functions to tidy (clean) the data:\n\n\n\n\n\n\n\n\nFunction\nObjective\nArguments\n\n\n\ngather()\nTransform the data from wide to long\n(data, key, value, na.rm = FALSE)\n\n\nspread()\nTransform the data from long to wide\n(data, key, value)\n\n\nseparate()\nSplit one variables into two\n(data, col, into, sep= ‚Äù ‚Äú, remove = TRUE)\n\n\nunit()\nUnit two variables into one\n(data, col, conc ,sep= ‚Äù ‚Äú, remove = TRUE)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#gather",
    "href": "static/slides/Datamanagment/datamanagment.html#gather",
    "title": "",
    "section": "gather()",
    "text": "gather()\n\nThe objectives of the gather() function is to transform the data from wide to long.\n\nSyntax\n\nCodegather(data, key, value, na.rm = FALSE)\n\n\nArguments:\n\ndata: The data frame used to reshape the dataset\nkey: Name of the new column created\nvalue: Select the columns used to fill the key column\nna.rm: Remove missing values. FALSE by default"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#reshape-the-data",
    "href": "static/slides/Datamanagment/datamanagment.html#reshape-the-data",
    "title": "",
    "section": "Reshape the data",
    "text": "Reshape the data\n\nIn the gather() function, we create two new variable quarter and growth because our original dataset has one group variable: i.e.¬†treatment and the key-value pairs.\n\n\nCodelong &lt;-data %&gt;%\ngather(quarter, growth, q1_2021:q4_2022)\nhead(long, 12)\n\n   Treatment quarter growth\n1          A q1_2021   0.03\n2          B q1_2021   0.05\n3          C q1_2021   0.01\n4          A q2_2021   0.05\n5          B q2_2021   0.07\n6          C q2_2021   0.02\n7          A q3_2021   0.04\n8          B q3_2021   0.05\n9          C q3_2021   0.01\n10         A q4_2021   0.03\n11         B q4_2021   0.02\n12         C q4_2021   0.04"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#spread",
    "href": "static/slides/Datamanagment/datamanagment.html#spread",
    "title": "",
    "section": "spread()",
    "text": "spread()\n\nThe spread() function does the opposite of gather.\n\nSyntax\n\nCodespread(data, key, value)\n\n\narguments:\n\ndata: The data frame used to reshape the dataset\nkey: Column to reshape long to wide\nvalue: Rows used to fill the new column"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#separate",
    "href": "static/slides/Datamanagment/datamanagment.html#separate",
    "title": "",
    "section": "separate()",
    "text": "separate()\n\nThe separate() function splits a column into two according to a separator.\nThis function is helpful in some situations where the variable is a date.\nOur analysis can require focussing on month and year and we want to separate the column into two new variables.\n\nSyntax\n\nCodeseparate(data, col, into, sep= \"\", remove = TRUE)\n\n\narguments:\n\n\ndata: The data frame used to reshape the dataset\n\ncol: The column to split\n\ninto: The name of the new variables\n\nsep: Indicates the symbol used that separates the variable, i.e.: ‚Äú-‚Äù, ‚Äú_‚Äù, ‚Äú&‚Äù\n\nremove: Remove the old column. By default sets to TRUE."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#unite",
    "href": "static/slides/Datamanagment/datamanagment.html#unite",
    "title": "",
    "section": "unite()",
    "text": "unite()\n\nThe unite() function concanates two columns into one.\n\nSyntax\n\nCodeunit(data, col, conc ,sep= \"\", remove = TRUE)\n\n\narguments: - data: The data frame used to reshape the dataset - col: Name of the new column - conc: Name of the columns to concatenate - sep: Indicates the symbol used that unites the variable, i.e: ‚Äú-‚Äù, ‚Äú_‚Äù, ‚Äú&‚Äù - remove: Remove the old columns. By default, sets to TRUE ‚Äî\nExample\n\nIn the above example, we separated quarter from year.\nWhat if we want to merge them.\nWe use the following code:\n\n\nCodeunit_tidier &lt;- separate_tidier %&gt;%\n  unite(Quarter, Qrt, year, sep =\"_\")\nhead(unit_tidier)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#dealing-with-missing-data",
    "href": "static/slides/Datamanagment/datamanagment.html#dealing-with-missing-data",
    "title": "",
    "section": "Dealing with Missing Data",
    "text": "Dealing with Missing Data\nMissing data types\n\nOne of the most important aspects of data cleaning is missing values.\n\nTypes of ‚Äúmissing‚Äù data:\n\n\nNA - general missing data\n\nNaN - stands for ‚ÄúNot a Number‚Äù, happens when you do 0/0.\n\nInf and -Inf - Infinity, happens when you divide a positive number (or negative number) by 0.\n\nFinding Missing data\n\n\nis.na - looks for NAN and NA\n\n\nis.nan- looks for NAN\n\n\nis.infinite - looks for Inf or -Inf\n\n\nCodetest&lt;-c(0,NA, -1)\ntest/0\n\n[1]  NaN   NA -Inf"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#naniar",
    "href": "static/slides/Datamanagment/datamanagment.html#naniar",
    "title": "",
    "section": "naniar",
    "text": "naniar\n\nSometimes you need to look at lots of data though‚Ä¶ the naniar package is a good option.\nThe pct_complete() function shows the percentage that is complete for a given data object, (vector or data frame).\n\n\nCode#install.packages(\"naniar\")\nlibrary(naniar)\npct_complete(airquality)\n\n[1] 95.20697\n\n\n\nCodeairquality %&gt;% select(Ozone) %&gt;%\npct_complete()\n\n[1] 75.81699"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#naniar-plots",
    "href": "static/slides/Datamanagment/datamanagment.html#naniar-plots",
    "title": "",
    "section": "\nnaniar plots",
    "text": "naniar plots\n\nThe gg_miss_var() function creates a nice plot about the number of missing values for each variable, (need a data frame).\n\n\nCodegg_miss_var(airquality)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#missing-data-issues",
    "href": "static/slides/Datamanagment/datamanagment.html#missing-data-issues",
    "title": "",
    "section": "Missing Data Issues",
    "text": "Missing Data Issues\nRecall that mathematical operations with NA often result in NAs.\n\nCodesum(c(1,2,3,NA))\n\n[1] NA\n\nCodemean(c(2,4,NA))\n\n[1] NA\n\nCodemedian(c(1,2,3,NA))\n\n[1] NA"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filter-and-missing-data",
    "href": "static/slides/Datamanagment/datamanagment.html#filter-and-missing-data",
    "title": "",
    "section": "filter() and missing data",
    "text": "filter() and missing data\n\nBe careful with missing data using subsetting!\nfilter() removes missing values by default. Because R can‚Äôt tell for sure if an NA value meets the condition.\nTo keep them need to add is.na() conditional.\n\n\nCodedf &lt;-tibble(Dog = c(0, NA, 2, 3, 1, 1), \n            Cat = c(NA, 8, 6, NA, 2, NA))\n\n\n\n\n\nCodedf \n\n# A tibble: 6 √ó 2\n    Dog   Cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0    NA\n2    NA     8\n3     2     6\n4     3    NA\n5     1     2\n6     1    NA\n\n\n\n\nCodedf %&gt;% filter(Dog &lt; 3)\n\n# A tibble: 4 √ó 2\n    Dog   Cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0    NA\n2     2     6\n3     1     2\n4     1    NA"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#filter-and-missing-data-1",
    "href": "static/slides/Datamanagment/datamanagment.html#filter-and-missing-data-1",
    "title": "",
    "section": "filter() and missing data",
    "text": "filter() and missing data\n\nCodedf %&gt;% filter(Dog &lt; 3 | is.na(Dog))\n\n# A tibble: 5 √ó 2\n    Dog   Cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0    NA\n2    NA     8\n3     2     6\n4     1     2\n5     1    NA"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#change-a-value-to-be-na",
    "href": "static/slides/Datamanagment/datamanagment.html#change-a-value-to-be-na",
    "title": "",
    "section": "Change a value to be NA\n",
    "text": "Change a value to be NA\n\n\nThe na_if() function of dplyr can be helpful for this. Let‚Äôs say we think that all 0 values should be NA.\n\n\n\n\nCodedf \n\n# A tibble: 6 √ó 2\n    Dog   Cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0    NA\n2    NA     8\n3     2     6\n4     3    NA\n5     1     2\n6     1    NA\n\n\n\n\nCodedf %&gt;% \n  mutate(Dog = na_if(x = Dog, y = 0))\n\n# A tibble: 6 √ó 2\n    Dog   Cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1    NA    NA\n2    NA     8\n3     2     6\n4     3    NA\n5     1     2\n6     1    NA"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#summary",
    "href": "static/slides/Datamanagment/datamanagment.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nis.na(),any(is.na()), count(), and functions from naniar like gg_miss_var() can help determine if we have NA values\nfilter() automatically removes NA values - can‚Äôt confirm or deny if condition is met (need | is.na() to keep them)\ndrop_na() can help you remove NA values from a variable or an entire data frame\nNA values can change your calculation results\nthink about what NA values represent - don‚Äôt drop them if you shouldn‚Äôt"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#data-leaning",
    "href": "static/slides/Datamanagment/datamanagment.html#data-leaning",
    "title": "",
    "section": "Data leaning",
    "text": "Data leaning\n\nCleaning data is one of the most essential parts in data analysis.\nData cleaning is the process of converting messy data into reliable data that can be analyzed in R.\n\nData cleaning improves data quality and your productivity in R.\n\nFormat ugly data frame column names in R\nDelete all blank rows in R\nRemove duplicate rows in R\nConverting numeric to date format\nChecking consistency by using tably() function"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#clean_names",
    "href": "static/slides/Datamanagment/datamanagment.html#clean_names",
    "title": "",
    "section": "clean_names()",
    "text": "clean_names()\n\nThis function is used to change and clean up names of columns in data frames.\nIt can be used to ensure consistency.\nYou can choose to change all names to lower cases, separated by underscores, variations on internal capital letters between words, title case or other styles.\nIt can also be used to remove parts of names and any special characters, including replacing % symbols with the word percent.\nThe most recommended variable names in R is one word, lower case, without special characters.\nclean_names() function will be used available in janitor R package to clean column names."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#remove_empty-remove_constant",
    "href": "static/slides/Datamanagment/datamanagment.html#remove_empty-remove_constant",
    "title": "",
    "section": "\nremove_empty() & remove_constant()\n",
    "text": "remove_empty() & remove_constant()\n\n\nSuppose if you want to remove rows and/or columns of if contain completely empty, then you can use remove_empty() function available in janitor R package."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#get_dupes",
    "href": "static/slides/Datamanagment/datamanagment.html#get_dupes",
    "title": "",
    "section": "get_dupes()",
    "text": "get_dupes()\n\nThis function retrieves any duplicates in the dataset so that they can be examined during data clean-up operations.\nOne trick is determining if a duplicate is indeed a duplicate.\nThe function returns a data frame which includes a dupe_count column containing the number of duplicates of that value.\n\n\nCodedata3 %&gt;% get_dupes() \n\n  sepal_length sepal_width petal_length petal_width   species dupe_count\n1          5.8         2.7          5.1         1.9 virginica          2\n2          5.8         2.7          5.1         1.9 virginica          2"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#tabyl",
    "href": "static/slides/Datamanagment/datamanagment.html#tabyl",
    "title": "",
    "section": "tabyl()",
    "text": "tabyl()\n\nThis function is used to produce frequency tables and contingency tables, i.e.¬†counts of each category or combination of categories of data.\nUnlike the base R table() function, tabyl() returns a data frame which makes results easier to work with.\nThe code below creates a data frame showing the number of rows of data (n) for each location in the dataset.\n\n\n\n\nCodedata_cleaned2 %&gt;% tabyl(species)\n\n    species  n   percent\n     setosa  9 0.2093023\n versicolor 19 0.4418605\n  virginica 15 0.3488372\n\n\n\n\nAlso returned is a percent column, showing the percentage of rows containing data for that location.\n\n\n\nCodedata_cleaned2 %&gt;% tabyl(species)%&gt;%\n  adorn_pct_formatting(digits = 2)\n\n    species  n percent\n     setosa  9  20.93%\n versicolor 19  44.19%\n  virginica 15  34.88%"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#explore-missing-values",
    "href": "static/slides/Datamanagment/datamanagment.html#explore-missing-values",
    "title": "",
    "section": "Explore missing values",
    "text": "Explore missing values\n\nCode# plot missing data (using raw data)\nDataExplorer::plot_missing(\n  title   = \"% of Missing Data (filtered to cols w/missing data)\",\n  data    = airquality,\n  ggtheme = tidyquant::theme_tq(), \n  missing_only = F)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#convert_to_date",
    "href": "static/slides/Datamanagment/datamanagment.html#convert_to_date",
    "title": "",
    "section": "convert_to_date()",
    "text": "convert_to_date()\n\nA modern Excel always tries to automate things, and I hate it!\nFor instance you write a number into a cell and it sometimes immediately converts it into date. Then you try to have a date in a cell, and it returns a number.\nMoreover, Excel also has some strange date encoding systems, which can be confused with a normal numeric columns.\nLuckily, our dirty dataset has a ‚Äúdate‚Äù word in the name of a column ‚Äúhire_date‚Äù, otherwise we wouldn‚Äôt know that it is a date:\n\n\nCodeconvert_to_date(d$hire_date)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#excel_numeric_to_date",
    "href": "static/slides/Datamanagment/datamanagment.html#excel_numeric_to_date",
    "title": "",
    "section": "excel_numeric_to_date()",
    "text": "excel_numeric_to_date()\n\nEver load data from Excel and see a value like 42223 where a date should be?\nThis function converts those serial numbers to class Date, with options for different Excel date encoding systems, preserving fractions of a date as time (in which case the returned value is of class POSIXlt), and specifying a time zone.\n\n\nCodeexcel_numeric_to_date(41103)\n\n[1] \"2012-07-13\"\n\n\n\nCodeexcel_numeric_to_date(41103.01) # ignores decimal places, returns Date object\n\n[1] \"2012-07-13\"\n\n\n\nCodeexcel_numeric_to_date(41103.01, include_time = TRUE) # returns POSIXlt object\n\n[1] \"2012-07-13 00:14:24 EAT\""
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#convert-a-mix-of-date-and-datetime-formats-to-date",
    "href": "static/slides/Datamanagment/datamanagment.html#convert-a-mix-of-date-and-datetime-formats-to-date",
    "title": "",
    "section": "Convert a mix of date and datetime formats to date",
    "text": "Convert a mix of date and datetime formats to date\n\nBuilding on excel_numeric_to_date(), the new functions convert_to_date() and convert_to_datetime() are more robust to a mix of inputs.\nHandy when reading many spreadsheets that should have the same column formats, but don‚Äôt.\n\nFor instance, here a vector with a date and an Excel datetime sees both values successfully converted to Date class:\n\nCodeconvert_to_date(c(\"2020-02-29\", \"40000.1\"))\n\n[1] \"2020-02-29\" \"2009-07-06\""
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#dates",
    "href": "static/slides/Datamanagment/datamanagment.html#dates",
    "title": "",
    "section": "Dates",
    "text": "Dates\n\nBefore dates in a dataset are loaded into R, typically they are stored as a column of character (string) values.\nHowever, dates are inherently numeric and we lose this information when they are stored as strings.\n\n\nCode# string dates have no numeric value, so this errors\n\"2018-03-05\" + 1\n\n\n\nIn R, we want to convert our string dates to R class Date, which preserves the dates‚Äô numeric values and allows us to take advantage of the many date-related functions in R.\nOnce converted to class Date, the numeric value for the date represents the number of days since January 1, 1970 (1970-01-01)."
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#as.date",
    "href": "static/slides/Datamanagment/datamanagment.html#as.date",
    "title": "",
    "section": "as.Date()",
    "text": "as.Date()\n\nBase R provides as.Date() to convert strings to dates, but it can be intimidating and unwieldy to use, particularly if your dates are not stored in one of 2 default formats (see ?strptime for more about date formats).\n\n\nCode# as.Date only accepts a couple of formats by default\n# good\nas.Date(\"2015-02-14\")\n# bad\nas.Date(\"02/14/2014\")\n# specify a format to fix\nas.Date(\"02/14/2014\", format=\"%m/%d/%Y\")\n\n\n\nOnce our dates are class Date, we can perform date arithmetic.\n\n\nCodea &lt;- as.Date(\"1971-01-01\")\nclass(a)\n\n[1] \"Date\"\n\nCode# days since 1970-01-01\nas.numeric(a)\n\n[1] 365\n\nCode# date arithmetic\na - as.Date(\"1970/12/31\")\n\nTime difference of 1 days\n\nCode## Time difference of 1 days\na + 2\n\n[1] \"1971-01-03\"\n\n\n\nWe just need something easier to use to convert strings of various formats to class Date!"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#package-lubridate",
    "href": "static/slides/Datamanagment/datamanagment.html#package-lubridate",
    "title": "",
    "section": "Package lubridate",
    "text": "Package lubridate\n\ntidyverse provides the amusingly named package lubridate to help R users convert their string dates to R Date format more easily as well as functions to process those dates.\nThe Date conversion functions in lubridate accept a wide variety of date formats, removing the need for us to remember all of those format specifications.\nInstead, just take the letters y, m, and d and place them in the order of the year, month and day, respectively, as they are stored in the date column. That ordering produces the name of the function to convert that column to Date (e.g.ymd(), mdy(), dmy()).\n\n\nCodelibrary(lubridate)"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#using-lubridate",
    "href": "static/slides/Datamanagment/datamanagment.html#using-lubridate",
    "title": "",
    "section": "Using lubridate",
    "text": "Using lubridate\n\nWe‚Äôll first load a dataset with various date formats to demonstrate the flexibility of lubridate functions.\n\n\nCodelibrary(readr)\nd &lt;- read_csv(\"data/dates.csv\")\nd\n\n# A tibble: 3 √ó 5\n  fmt1     fmt2             fmt3           fmt4 decision_time             \n  &lt;chr&gt;    &lt;chr&gt;            &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;                     \n1 01/15/89 December 8, 2015 2015-02-27 20090101 Approved 10-10-15 07:15:55\n2 02/13/92 January 22, 2012 2016-11-15 20080819 Denied 09-27-11 14:57:23  \n3 03/15/84 March 3, 2010    2017-12-25 20071011 Approved 04-24-15 02:03:03"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#date-time-variables",
    "href": "static/slides/Datamanagment/datamanagment.html#date-time-variables",
    "title": "",
    "section": "Date-time variables",
    "text": "Date-time variables\n\nIf your date column additionally contains time information (i.e.¬†is date-time), you can add one or more of h, m, and s to y, m, and d to form the function name to convert the string to class POSIXct.\nClass POSIXct stores date-time variables as a number representing the number of seconds since the beginning of 1970.\nThe fifth column of our dates dataset, decision_time, is a date-time variable.\n(It also contains an ‚ÄúApproved/Denied‚Äù string at the beginning that the lubridate() function will ignore!).\nSpecifically, the date-time is recorded as month, day, year, hour, minute, second, so we want mdy_hms():\n\n\nCode# month day year hour minute second\nmdy_hms(d$decision_time)\n\n[1] \"2015-10-10 07:15:55 UTC\" \"2011-09-27 14:57:23 UTC\"\n[3] \"2015-04-24 02:03:03 UTC\""
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#extracting-information-from-date-variables",
    "href": "static/slides/Datamanagment/datamanagment.html#extracting-information-from-date-variables",
    "title": "",
    "section": "Extracting information from Date variables",
    "text": "Extracting information from Date variables\n\nlubridate provides several functions to extract specific information from Date variables including:\n\n\nday(): day of the month\n\nwday(): weekday\n\nyday(): day of the year\n\nmonth(): month of the year\n\nyear(): year\n\n\nSome examples of extracting information from Date variables.\n\n\nCode# we'll use the first column of our dates dataset\ndates$f1\n\n[1] \"1989-01-15\" \"1992-02-13\" \"1984-03-15\"\n\nCode# day of the month\nday(dates$f1)\n\n[1] 15 13 15\n\nCode# day of the year\nyday(dates$f1)\n\n[1] 15 44 75"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#extracting-information-from-posixct-variables",
    "href": "static/slides/Datamanagment/datamanagment.html#extracting-information-from-posixct-variables",
    "title": "",
    "section": "Extracting information from POSIXct variables",
    "text": "Extracting information from POSIXct variables\n\n\nlubridate also has functions to extract time information from POSIXct date-time variables.\n\nhour()\nminute()\nsecond()\n\n\nCodewith(dates,  ## with() tells R to look for variables in object \"dates\"\ndata.frame(time=decision_time, h=hour(decision_time),\n           m=minute(decision_time), s=second(decision_time)))\n\n                 time  h  m  s\n1 2015-10-10 07:15:55  7 15 55\n2 2011-09-27 14:57:23 14 57 23\n3 2015-04-24 02:03:03  2  3  3"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#section",
    "href": "static/slides/Datamanagment/datamanagment.html#section",
    "title": "",
    "section": "",
    "text": "Easy data cleaning with the janitor package"
  },
  {
    "objectID": "static/slides/Datamanagment/datamanagment.html#clean-column-names",
    "href": "static/slides/Datamanagment/datamanagment.html#clean-column-names",
    "title": "",
    "section": "Clean column names",
    "text": "Clean column names\n\nadd some points about\n\ncount\ntably\npercent for making quick graphs\n\n\n\nhttps://www.r-bloggers.com/2024/05/easy-data-cleaning-with-the-janitor-package/"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#planning-of-surveys",
    "href": "static/slides/Sampling/Survey.html#planning-of-surveys",
    "title": "Survey Methodology Design",
    "section": "Planning of surveys",
    "text": "Planning of surveys\nImportance of Survey Preparation\n\nFor a survey to yield the desired results, there is a need to pay particular attention to the preparations that precede the fieldwork.\nIn this regard, all surveys require careful and judicious preparations if they are to be successful.\nHowever, the amount of planning will vary depending on the type of survey, materials, and information required."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#survey-universe",
    "href": "static/slides/Sampling/Survey.html#survey-universe",
    "title": "Survey Methodology Design",
    "section": "Survey universe",
    "text": "Survey universe\n\nWhen planning a survey, it is necessary to define the geographical areas to be covered and the target population.\nIn defining the universe, the exact population to be sampled should be identified.\nIt should be pointed out, however, that in practice the target population is somewhat smaller than the population forming the universe."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#information-to-be-collected",
    "href": "static/slides/Sampling/Survey.html#information-to-be-collected",
    "title": "Survey Methodology Design",
    "section": "Information to be collected",
    "text": "Information to be collected\n\nFrom the list of questions requiring statistical answers, a list of items that could provide factual information bearing on issues under investigation can be produced.\nIt is always important to bear in mind that some of the required data could be available from existing sources.\nIn producing the list of items, there should be provisions made for the inclusion of supplementary items that are correlated with the main items.\nIn a survey of employment and earnings, for example, supplementary information on age, sex, and education may be gathered.\nWe may add that a tabulation plan should be produced at the time of planning the survey."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#survey-budget",
    "href": "static/slides/Sampling/Survey.html#survey-budget",
    "title": "Survey Methodology Design",
    "section": "Survey budget",
    "text": "Survey budget\n\nThe survey budget indicates the financial requirements of the survey that is to be conducted.\nThe budget is needed to support and guide the implementation of the survey and the construction of the timetable for producing the survey results.\nCost estimates must be as detailed as possible.\nIt is therefore necessary to understand all the detailed steps involved in the survey operation.\nThe budget shows the cost of personnel, equipment, and all other items of expense.\nIf there is a predetermined ceiling of funds available (which is usually the case), the overall survey budget must be within the predetermined framework.\nThe financial requests of the survey should be prepared at an early stage.\nIn general, the budget will depend largely on the survey design, the precision required, and the geographical coverage."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#importance-of-survey-preparation",
    "href": "static/slides/Sampling/Survey.html#importance-of-survey-preparation",
    "title": "Survey Methodology Design",
    "section": "Importance of Survey Preparation",
    "text": "Importance of Survey Preparation\n\nFor a survey to yield the desired results, there is a need to pay particular attention to the preparations that precede the fieldwork.\nIn this regard, all surveys require careful and judicious preparations if they are to be successful.\nHowever, the amount of planning will vary depending on the type of survey, materials, and information required."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#objectives-of-a-survey",
    "href": "static/slides/Sampling/Survey.html#objectives-of-a-survey",
    "title": "Survey Methodology Design",
    "section": "Objectives of a Survey",
    "text": "Objectives of a Survey\n\nIt is imperative that the objectives of a survey be spelled out from the start of the project.\nThere should be a clearly formulated statistical statement on the desired information, giving a clear description of the population and geographical coverage.\nIt is also necessary at this stage to stipulate how the results are going to be used. The given budget of the survey should guide the survey statistician in tailoring the objectives.\nTaking due cognizance of the budgetary constraints will facilitate the successful planning and execution of the survey.\nIt is very important that stakeholders, that is to say, various users and producers of statistics, be involved in defining the objective of the survey as well as its scope and coverage.\nThe consultations help to establish consensus or compromises on what data are needed, the form in which data are required, levels of disaggregation, dissemination strategies, and the frequency of data collection.\nIt should be noted that having clearly stated objectives is the first step in determining those survey questions for which statistical answers are required."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section",
    "href": "static/slides/Sampling/Survey.html#section",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Survey Execution:\n\nData Collection Methods: There are several methods for collecting survey data, each with advantages and limitations.\n\nThe choice of method depends on factors such as the type of data, budget, and respondent characteristics.\n\nDirect Observation: This method involves directly observing behaviors, events, or conditions.\n\nIt provides objective and accurate data, but it can be resource-intensive and time-consuming.\nDirect observation is ideal for small, detailed studies or where subjective reporting might introduce bias."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#survey-budget-1",
    "href": "static/slides/Sampling/Survey.html#survey-budget-1",
    "title": "Survey Methodology Design",
    "section": "Survey budget",
    "text": "Survey budget\n\nIt is essential that an effective cost control system be established in the organization that is conducting the survey.\nIn most large scale survey operations, there is a high risk of loss of control over monitoring the disbursement of funds once fieldwork starts.\n\nIn such circumstances, a large amount of funds tend to be channeled into areas unrelated to the major survey operations.\n\nJudicious cost control helps to monitor actual expenditures in relation to estimated costs and actual work accomplished.\nIt is imperative that management responsible for the survey ensure accountability of funds.\nThis greatly enhances the credibility of the survey organization."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#execution-of-surveys",
    "href": "static/slides/Sampling/Survey.html#execution-of-surveys",
    "title": "Survey Methodology Design",
    "section": "Execution of surveys",
    "text": "Execution of surveys\nData collection methods\n\nThere are several methods used in data collection, among them direct observation and measurement; the mail questionnaire; and telephone and personal interviews.\n\nA). Direct observation and measurement:\n\nDirect observation and measurement constitute the ideal method as they are usually more objective.\n\nNot affected with recall biases and subjectivity\nThis method is useful and practical when the sample sizes or populations are relatively small.\nBut expensive in terms of both resources and time.\n\n\nB). Mail questionnaires:\n- The method of using mail questionnaires is fairly cheap and rapid. \n- The major cost component at the data‚Äëcollection stage is postage. \n- Interviewer bias is eliminated.\n\nHowever, the non‚Äëresponse is usually high as it needs all respondents are literates; Not a suitable method for complex surveys."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-1",
    "href": "static/slides/Sampling/Survey.html#section-1",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Mail Questionnaires: This method is cost-effective and fast, especially for large, geographically dispersed populations.\n\nHowever, it generally has lower response rates and requires respondents to be literate and capable of completing the form independently.\nReminders and follow-ups can improve response rates, but item non-response and missing data may still be issues.\n\nPersonal Interviews: The most commonly used method for in-depth surveys, particularly for complex subjects or populations with low literacy rates.\n\nPersonal interviews often result in higher response rates and allow interviewers to clarify questions.\nHowever, they can introduce interviewer bias, and the process is resource-heavy, requiring trained interviewers and logistical support."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-2",
    "href": "static/slides/Sampling/Survey.html#section-2",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Mail Questionnaires: This method is cost-effective and fast, especially for large, geographically dispersed populations.\n\nHowever, it generally has lower response rates and requires respondents to be literate and capable of completing the form independently.\nReminders and follow-ups can improve response rates, but item non-response and missing data may still be issues.\n\nPersonal Interviews: The most commonly used method for in-depth surveys, particularly for complex subjects or populations with low literacy rates.\n\nPersonal interviews often result in higher response rates and allow interviewers to clarify questions.\nHowever, they can introduce interviewer bias, and the process is resource-heavy, requiring trained interviewers and logistical support."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#questionnaire-design",
    "href": "static/slides/Sampling/Survey.html#questionnaire-design",
    "title": "Survey Methodology Design",
    "section": "Questionnaire design",
    "text": "Questionnaire design\n\nThe size and format of the questionnaire plays a central role and need very serious consideration.\nIt is advisable to design questionnaires at the time of planning for the survey.\nTo enhance accuracy in the survey data, the questionnaire should be\n\nclear\nnot wording (should be easy reading, should be clear, precise and unambiguous)\npretested\ndeveloped with special emphasis on relevance, timeliness and accuracy."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-3",
    "href": "static/slides/Sampling/Survey.html#section-3",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Questionnaire Design:\n\nQuestionnaire Structure: A well-constructed questionnaire is the backbone of data collection.\n\nIt must be designed to collect accurate, relevant information efficiently.\nQuestions should be clearly worded, easy to understand, and ordered in a logical sequence.\nQuestions at the beginning should be simple and non-sensitive to build rapport with the respondent.\n\nTypes of Questions:\n\nOpen-ended Questions: Allow respondents to provide answers in their own words. These are useful for gathering qualitative insights, but they can be harder to analyze.\nClosed-ended Questions: Provide specific response options. These are easier to analyze but may limit the range of answers, missing nuances."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-4",
    "href": "static/slides/Sampling/Survey.html#section-4",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Pre-Testing: Pre-testing the questionnaire is essential to identify potential problems, such as ambiguous questions or response categories.\n\nPre-testing ensures that the survey tool will perform as expected in the field, reducing errors in the data collection process.\n\n\n4. Implementation of Fieldwork:\n\nFieldwork Preparation: Successful fieldwork requires logistical planning and necessary equipment, including vehicles, survey materials, and data collection tools.\n\nEnsuring that interviewers are well-prepared with all necessary supplies, such as questionnaires, pens, and clipboards, is crucial.\n\nManagement of Survey Operations: Surveys, especially large-scale ones, are complex operations that require effective management.\n\nClear communication and lines of authority are vital.\nProgress monitoring tools, such as control forms, help keep track of the survey‚Äôs progress and ensure deadlines are met."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#implementation-of-fieldwork",
    "href": "static/slides/Sampling/Survey.html#implementation-of-fieldwork",
    "title": "Survey Methodology Design",
    "section": "Implementation of fieldwork",
    "text": "Implementation of fieldwork\n\nTo conduct a survey,\nFieldwork should be properly organized and implemented\nThe resources of the survey team should be efficiently utilized effectively\nVehicles facilitate the rapid mobility of survey teams.\nAdequate materials, like folders, clipboards, pencils, pencil sharpeners, notebooks and fuel (for vehicles), should be available in adequate supplies for use during the survey operations."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#equipment-and-materials",
    "href": "static/slides/Sampling/Survey.html#equipment-and-materials",
    "title": "Survey Methodology Design",
    "section": "Equipment and materials",
    "text": "Equipment and materials\n\nVehicles and bicycles facilitate the rapid mobility of team leaders and supervisors/interviewers.\nAdequate materials, like folders, clipboards, pencils, pencil sharpeners, notebooks and fuel (for vehicles), should be available in adequate supplies for use during the survey operations.\n\nManagement of survey operations\n\nThere must be a clear and well‚Äëdefined line of command from the survey manager to the interviewer. It should be noted that control forms for monitoring the progress of the survey have been found useful."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#publicity",
    "href": "static/slides/Sampling/Survey.html#publicity",
    "title": "Survey Methodology Design",
    "section": "Publicity",
    "text": "Publicity\n\nTo reduce the high number of non-responses due to refusals, it is important to make publicity about the survey. For example, it may be necessary to arrange meetings with local opinion leaders in selected areas.\nDuring such meetings, people would be briefed on the objectives of the survey.\nIn addition, the leaders should be requested to persuade people in their area to provide requisite information to the interviewers.\nBefore entry into the field, it is important that the relevant legal provisions for conducting the survey be published.\nThe announcement should, among other information, give the survey‚Äôs objectives and duration and the topics to be covered therein."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#followup-of-nonrespondents",
    "href": "static/slides/Sampling/Survey.html#followup-of-nonrespondents",
    "title": "Survey Methodology Design",
    "section": "Follow‚Äëup of non‚Äërespondents",
    "text": "Follow‚Äëup of non‚Äërespondents\n\nSome respondents may refuse to cooperate with the interviewers; in some cases, certain items in the questionnaire may not be attended to.\nWhen a non‚Äëresponding unit has been reported to the supervisor, he or she has to contact the sample unit and try to elicit the information, owing to his/her better qualifications and greater experience."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#reducing-nonresponse",
    "href": "static/slides/Sampling/Survey.html#reducing-nonresponse",
    "title": "Survey Methodology Design",
    "section": "Reducing non‚Äëresponse",
    "text": "Reducing non‚Äëresponse\n\nIt is important in designing and executing a survey to develop good survey procedures aimed at maximizing the response rate.\nThe most important procedures to reduce the number of refusals, arranging to return to conduct an interview at the convenience of the respondent.\nthe objectives and uses of the surveys should be carefully explained to reluctant respondents to help win their cooperation.\nAssurance of confidentiality can also help alleviate the fear that by respondents may have about their responses, being used for purposes other than those stipulated by the survey.\n\nRepeated callbacks at different times of the day should be made when no one is at home.\n\n\nIt is recommended that as many as four callbacks be attempted.\nIt is also important to avoid the problem of failing to locate the selected sampling units, which can be an important source of non‚Äëresponse. This problem is best addressed by using the most current sampling frame."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sampling-strategies",
    "href": "static/slides/Sampling/Survey.html#sampling-strategies",
    "title": "Survey Methodology Design",
    "section": "Sampling Strategies",
    "text": "Sampling Strategies\nIntroduction to Sampling Strategies:\n\nSurveys typically do not involve collecting data from the entire population due to practical constraints.\nInstead, sampling strategies are employed to select a representative subset of the population, allowing researchers to make inferences about the broader population.\nThe goal of sampling is to ensure that the selected sample is representative and that the findings can be generalized with known levels of precision and accuracy.\n\nProbability Sampling vs.¬†Non-Probability Sampling:\n\nProbability Sampling: In probability sampling, every unit in the population has a known, non-zero chance of being selected.\n\nThis method is grounded in statistical theory and allows for the calculation of sampling error and the application of inferential statistics.\nIt is the preferred method when the goal is to make generalizable claims about the entire population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-5",
    "href": "static/slides/Sampling/Survey.html#section-5",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Selection and Training of Interviewers:\n\nSelecting Interviewers: Interviewers play a pivotal role in collecting reliable data.\nTheir selection should focus on communication skills, honesty, and the ability to follow instructions.\nA candidate‚Äôs education level and ability to understand the survey objectives should also be considered.\nGood interviewers ensure high-quality data and reduce errors due to misunderstanding or miscommunication with respondents.\nTraining: Interviewers must undergo thorough training to understand the survey objectives, questionnaire content, and data collection procedures.\n\nThis minimizes the risk of interviewer bias and ensures consistent administration of the survey across different respondents.\nTraining should include classroom sessions, role-playing, and field practice.\nOngoing supervision during fieldwork is also essential to maintain the quality of data collection."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#probability-sampling-in-stages",
    "href": "static/slides/Sampling/Survey.html#probability-sampling-in-stages",
    "title": "Survey Methodology Design",
    "section": "Probability sampling in stages",
    "text": "Probability sampling in stages\n\nprobability sampling must be used at each stage of the sample selection process in order for the requirements to be met.\nFor example, the first stage of selection generally involves choosing geographically defined units such as villages.\nThe last stage involves selecting the specific households or persons to be interviewed.\nThese two stages and any intervening ones must utilize probability methods for proper sampling."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#when-the-target-population-is-ill-defined",
    "href": "static/slides/Sampling/Survey.html#when-the-target-population-is-ill-defined",
    "title": "Survey Methodology Design",
    "section": "When the target population is ill-defined",
    "text": "When the target population is ill-defined\n\nSometimes the conditions for probability sampling are violated because of loose criteria for defining the target population that the survey is intended to cover.\nFor example, the desired target population may be all households nationwide.\nYet, when the survey is designed and/or implemented, often certain population subgroups such as nomadic households, boat people, and populations in whole areas that are inaccessible owing to the difficult terrain are intentionally excluded. In other cases, a target population intended to cover a restricted, special population such as ever-married women or young people under age 25 excludes important subgroups for various reasons. For example, a target population intended to cover youth under age 25 may exclude those who are in the military or in jail or are otherwise institutionalized.\n\n\nWhenever the actual target population covered by the survey differs from the one intended, the survey team should take care to redefine the target population more accurately. This is important not only to clarify the survey results for users but also to meet the conditions of probability sampling. In the aforementioned example of youth under age 25, the target population should be more precisely described and redefined as civilian, non-institutional youth under age 25. Otherwise, survey coverage should be expanded to include the excluded subgroups.\nThus, it is important to define the target population very carefully so as to cover only those members who will actually be given the chance of being selected for the survey. In cases where subgroups are intentionally excluded, it is of course crucial to apply probability methods to the actual population constituting the frame of the survey. Furthermore, it is incumbent upon the survey directors to clearly describe to the users, when results are released, which segments of the population have been included and which segments were excluded by the survey. Non-probability sampling methods There is no statistical theory, like that for probability sampling, to guide the use of non-probability samples. They can be assessed only through subjective evaluation. Failure to use probability techniques means, the survey estimates will be biased. Moreover, the magnitude of these biases and often their direction towards underestimation or overestimation will be unknown. The precision of sampling estimates, that is, to say their standard errors, can be estimated when probability sampling is used. This is necessary in order for the user to gauge the reliability of the survey estimates and to construct confidence intervals around the latter. In spite of their theoretical deficiencies, non-probability samples are frequently used in various settings and situations. The justification offered by practitioners is generally one based on cost, convenience or even the survey team‚Äôs apprehension that a ‚Äúrandom‚Äù sample may not properly represent the target population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#judgmental-sampling",
    "href": "static/slides/Sampling/Survey.html#judgmental-sampling",
    "title": "Survey Methodology Design",
    "section": "Judgmental sampling",
    "text": "Judgmental sampling\n\nJudgmental sampling is a method that relies upon ‚Äúexperts‚Äù to choose the sample elements.\nThe main difficulty with this type of sampling is the subjectivity of the determination of what constitutes a representative set of selected sample.\nIronically, the choice is also highly dependent on the choice of the experts themselves. ## Random walk and quota sampling\nAnother type of non probability sampling that is widely used is the so called random walk procedure undertaken at the last stage of a household survey.\nThe technique is often used even if the elements of the sample prior stages were selected through legitimate probability methods.\nThe illustration below shows a type of sampling that is a combination of random walk and quota sampling."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#convenience-samples",
    "href": "static/slides/Sampling/Survey.html#convenience-samples",
    "title": "Survey Methodology Design",
    "section": "Convenience samples",
    "text": "Convenience samples\n\nConvenience sampling is also widely used because of its simplicity of implementation.\nThough convenience sampling is not often applied in household surveys, many illustrations of its use can be provided, for example, in conducting of a survey of school youth in a purposively chosen sample of schools that are easily accessible and known to be cooperative, that is to say, convenient."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sample-size-determination-for-household-surveys",
    "href": "static/slides/Sampling/Survey.html#sample-size-determination-for-household-surveys",
    "title": "Survey Methodology Design",
    "section": "Sample size determination for household surveys",
    "text": "Sample size determination for household surveys\n\nThe sample size is important to the entire operation and cost of a survey. In terms of:\n\nhow many households are interviewed,\nhow many geographical areas primary sampling units (PSUs) are sampled,\nhow many interviewers are hired,\nhow big the workload is for each interviewer, etc.\n\nSample size is the pivotal feature that governs the overall design of the sample."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#magnitudes-of-survey-estimates",
    "href": "static/slides/Sampling/Survey.html#magnitudes-of-survey-estimates",
    "title": "Survey Methodology Design",
    "section": "Magnitudes of survey estimates",
    "text": "Magnitudes of survey estimates\n\nIn household surveys, every estimate (indicator) to be generated from the survey requires a different sample size for reliable measurement.\nThe size of the sample depends on the size of the estimate, that is to say, its expected proportion of the total population. For example, to estimate reliably the proportion of households with access to safe water requires a different sample size than estimating the proportion of adults not currently working.\nThe expressions for calculating a sample sizes are based on a probabilistic statements, that the true population parameter be contained in an interval with a given probability (confidence level).\nThe width (or precision) of the interval depends on the population variance; on the degree of confidence and on the sample size.\nIn general, the greater the population heterogeneity or the desired confidence level, the wider the interval.\nOn the other hand, the width of the interval will decrease as the sample size increases.\nIn practice, the survey itself can have only one sample size.\nTo calculate the sample size, a choice must be made from among the many estimates to be measured in the survey. For example, if the key estimate was the unemployment rate, calculation of the sample size would be based on it.\nWhen there are many key indicators, a convention sometimes applied is to calculate the sample size needed for each and then use the one that yields the largest sample.\nGenerally, this is the indicator for which the base population is the smallest ‚Äúsub-target population‚Äù, in terms of its proportion of the total population.\nThe desired precision must of course be taken into account. When the sample size is based on such an estimate, each of the other key estimates will be measured with the same or greater reliability.\nAlternatively, the sample size can be based on a comparatively small proportion of the target population instead of on the specification of a particular indicator.\nThis may likely be the best approach in a general-purpose household survey that focuses on several unrelated subjects, in which case it may be impractical or imprudent to base the sample size on an indicator that pertains to a single subject. Target population\nSample size depends also on the target population that will be covered by the survey. Like indicators, there are often several target populations in household surveys.\nA health survey, for example, may target households, to assess access to safe water and sanitation, all persons, to estimate chronic and acute conditions, women aged 14 49, for reproductive health indicators and children under age 5, for anthropometric measurements of height and weight. Calculation of the sample size must therefore take into consideration each of the target populations. However, household surveys frequently have multiple target populations, each of equal interest for the measurement objectives of the survey. It is again, viable, to focus on the smallest one in determining the sample size. For example, if children under age 5 are an important target group for the survey, the sample size should be based on that group."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#precision-and-statistical-confidence",
    "href": "static/slides/Sampling/Survey.html#precision-and-statistical-confidence",
    "title": "Survey Methodology Design",
    "section": "Precision and statistical confidence",
    "text": "Precision and statistical confidence\n\nSample size determination critically, depends on the degree of precision desired for the indicators.\nThe more precise or reliable the survey estimates must be, the bigger the sample size must be and by orders of magnitude.\nSimilarly, the sample size increases as the degree of statistical confidence desired increases to maintain a given precision.\nThe 95 percent confidence level is almost universally taken as the standard and the sample size necessary to achieve it is calculated accordingly. Taking account of the indicators, a convention in many well-conceived surveys is to use, as the precision requirement, a margin of relative error of 10 percent at the 95 percent confidence level on the key indicators to be estimated, meaning that the standard error of a key indicator should be no greater than 5 percent of the estimate itself. This is calculated as (2 * 0.05x, where x is the survey estimate). For example, if the estimated proportion of persons in the labour force is 65 percent, its standard error should be no larger than 3.25 percentage points, that is, 0.65 multiplied by 0.05. Two times 0.0325, or 0.065, is the relative margin of error at the 95 percent confidence level.\nThe sample size needed to achieve the criterion of 10 percent margin of relative error is thus one-fourth as big as one where the margin of relative error is set at 5 percent. A margin of relative error of 20 percent is generally regarded as the maximum allowable for important indicators (though we do not recommend it). This is because the confidence interval surrounding estimates with greater error tolerances is too wide for meaningful results to be achieved for most analytical or policy needs. In general, we recommend 5 to 10 percent relative errors for the main indicators, budget permitting."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#clustering-effects",
    "href": "static/slides/Sampling/Survey.html#clustering-effects",
    "title": "Survey Methodology Design",
    "section": "Clustering effects",
    "text": "Clustering effects\n\nThe degree to which a household survey sample is clustered affects the reliability or precision of the estimates and therefore the sample size.\nCluster effects in household surveys arise from the penultimate sampling units, generally referred to as the ‚Äúclusters‚Äù, which may be villages or city blocks, the sample households, the size and/ or variability of the clusters and from the method of sampling households within the selected clusters.\nClustering as well as the effects of stratification can be measured numerically by the design effect, or deff, which expresses how much larger the sampling variance (square of the standard error) for the stratified cluster sample is compared to a simple random sample of the same size.\nStratification tends to decrease the sampling variance but only to a small degree. By contrast, clustering increases the variance considerably. Therefore, deff indicates primarily how much clustering there is in the survey sample.\nEfficient sample design requires that clusters be used to control costs but also that the design effect be kept as low as possible for the results to be usable and reliable.\nUnfortunately, deff is not known before a survey is undertaken and can be estimated only afterwards from the data themselves.\nIn cases where previous surveys have been conducted or where similar ones have been conducted in other countries, the deff values from those surveys might be used as proxies in the calculation formula to estimate sample size.\n\nTo keep the design effect as low as possible, the sample design should follow these general principles: Use as many clusters as is feasible; Use the smallest cluster size in terms of number of households that is feasible; Use a constant cluster size rather than a variable one; Select a systematic sample of households at the last stage, geographically dispersed, rather than a segment of geographically contiguous households. Thus, for a sample of 12,000 households, it is preferable to select 600 clusters of 20 households each rather than 400 clusters of 30 households each. The sampling design effect is much lower in the former case. Moreover, deff is reduced if the households are chosen systematically from all the households in the cluster rather than selected in contiguous geographical subsegments. When these rules of thumb are followed, the design effect is likely to be reasonably low."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#adjusting-sample-size-for-anticipated-non-response",
    "href": "static/slides/Sampling/Survey.html#adjusting-sample-size-for-anticipated-non-response",
    "title": "Survey Methodology Design",
    "section": "Adjusting sample size for anticipated non-response",
    "text": "Adjusting sample size for anticipated non-response\n\nIt is common practice in surveys to increase the sample size by an amount equal to the anticipated non-response rate. This ensures that the actual number of interviews completed in the survey will closely approximate the target sample size. The degree of non-response in surveys varies widely by survey type.\n\nSurvey budget\nIt perhaps goes without saying that the survey budget cannot be ignored when determining an appropriate sample size for a household survey. While the budget is not a parameter that figures in the mathematical calculation of sample size, it does figure prominently at a practical level.\nIt is by the statistician, who, in taking account of each of the parameters discussed in this chapter initially calculates the sample size. \nIt is often the case, however, that the size proves to be larger than what the survey budget can support. When this occurs, the survey team must either seek additional funds for the survey or modify its measurement objectives, by reducing either the precision requirements or the number of domains.\nIt is the responsibility of the sampling expert to help guide the discussion on cost versus precision. \nThe number of clusters is also a key determinant of survey costs and survey precision, must be carefully weighed by the sampler in guiding the survey team,"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sample-size-calculation",
    "href": "static/slides/Sampling/Survey.html#sample-size-calculation",
    "title": "Survey Methodology Design",
    "section": "Sample size calculation",
    "text": "Sample size calculation\n\nIn the present section, we provide the formula for calculating the sample size, taking into account the parameters previously discussed. Because we are focusing on household surveys, the sample size is calculated in terms of the number of households that must be selected. In general, when a proportion p is included, the estimation formula for the sample size, nh, is n_h = z^2 r(1-r)(f)(k)/(p n ÃÉe^2 , where nh is the parameter to be calculated and is the sample size in terms of number of households to be selected; z is the statistic that defines the level of confidence desired; r is an estimate of a key indicator to be measured by the survey; f is the sample design effect, deff, assumed to be 2.0 (default value); k is a multiplier required to account for the anticipated rate of non response; p is the proportion of the total population accounted for by the target population and upon which the parameter r is based; n ÃÉ is the average household size (number of persons per household); and e is the margin of error to be attained. Recommended values for some of the parameters are as follows. The z statistic to be used should be 1.96 for the 95%CI The default value, of the sample design effect, is usually set at 2.0 unless there is supporting empirical data from previous or related surveys that suggest a different value. The non-response multiplier, k, should be chosen to reflect the country‚Äôs own experience with non response‚Äîtypically under 10%.\nThe parameter p can usually be calculated from the results of the most recent census. For the margin of error, e, it is recommended that the level of precision be set at 10 percent of r; thus e = 0.10r. A smaller sample size can be obtained with a less stringent margin of error, e = 0.15r, but the survey results would of course be much less reliable. Substituting some selected values gives a reduced formula.\n\nThe following example entails a smaller base population‚Äîchildren under age 5. Example In country X, the main survey indicator is determined to be the mortality rate among children under age 5, thought to be about 5 percentage points. In this case, r = 0.05, and p is estimated to be about 0.15, or 0.035. Again, we wish to estimate the mortality rate with a 10 percent margin of relative error: then e = 0.10r (or 0.005 standard error). The values for the expected non-response rate, design effect, and average household size are again the ones we have recommended. Formula gives nearly 10,704 households (84.50.95)/(0.05*0.15), a much larger sample size than that of the previous example. Again, the primary reason for this is related to the size of the base population, that is, to say children under age 5, who constitute only 15% of the total. The estimated parameter r is also small and this and a small p combine to force a large sample size. The final example is for a case where the total population is the target population. In that case, p = 1 and can be ignored; however, formulae (3.8) and (3.9) may be still used if the recommended values of the parameters are utilized. Example In country D, the main survey indicator is determined to be the proportion of persons in the entire population who had an acute health condition during the preceding week. That propor tion is thought to be between 5 and 10 per cent, in which case the smaller value is used because n r h = - ( . 84 5 1 ) ( )/( ) r p ( ). (3.9)"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sampling-strategies-43",
    "href": "static/slides/Sampling/Survey.html#sampling-strategies-43",
    "title": "Survey Methodology Design",
    "section": "Sampling strategies 43",
    "text": "Sampling strategies 43\nit will give a larger sample size (the conservative approach). In this case, r = 0.05 and p is of course 1.0. Again, we wish to estimate the acute rate with a 10 per cent margin of relative error: e = 0.10r 6 (or 0.005 standard error) and the values for the expected non response rate, design effect and average household size are again the ones that we have recommended. Formula (3.9) yields a little over 1,600 households (84.5*0.95)/(0.05)"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#management-of-survey-operations",
    "href": "static/slides/Sampling/Survey.html#management-of-survey-operations",
    "title": "Survey Methodology Design",
    "section": "Management of survey operations",
    "text": "Management of survey operations\n\nThere must be a clear and well‚Äëdefined line of command from the survey manager to the interviewer.\nIt should be noted that control forms for monitoring the progress of the survey have been found useful.\nTo reduce the high number of non-responses due to refusals, it is important to make publicity about the survey.\n\nFor example, it may be necessary to arrange meetings with local opinion leaders in selected areas.\n\nDuring such meetings, people would be briefed on the objectives of the survey.\nIn addition, the leaders should be requested to persuade people in their area to provide requisite information to the interviewers.\nBefore entry into the field, it is important that the relevant legal provisions for conducting the survey be published.\nThe announcement should, among other information, give the survey‚Äôs objectives and duration and the topics to be covered therein."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sampling-strategies-1",
    "href": "static/slides/Sampling/Survey.html#sampling-strategies-1",
    "title": "Survey Methodology Design",
    "section": "Sampling strategies",
    "text": "Sampling strategies\nit will give a larger sample size (the conservative approach). In this case, r = 0.05 and p is of course 1.0. Again, we wish to estimate the acute rate with a 10 per cent margin of relative error: e = 0.10r 6 (or 0.005 standard error) and the values for the expected non response rate, design effect and average household size are again the ones that we have recommended. Formula (3.9) yields a little over 1,600 households (84.5*0.95)/(0.05)"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#concept-of-survey-planning-and-execution",
    "href": "static/slides/Sampling/Survey.html#concept-of-survey-planning-and-execution",
    "title": "Survey Methodology Design",
    "section": "Concept of Survey Planning and Execution",
    "text": "Concept of Survey Planning and Execution\n\nSurveys are essential tools for collecting data from a defined population to make inferences about broader trends, behaviors, or characteristics.\nEffective survey execution involves careful planning, proper implementation, and monitoring.\n\nThe following key concepts are relevant to any survey process:\n1. Survey Planning:\n\n i. Objectives of the Survey: Every survey must have clearly defined objectives.\n\nThese objectives should articulate the purpose of the survey, the desired information, and the target population.\nClarity on objectives ensures that survey results are relevant and useful for decision-making.\nIt‚Äôs critical to align the survey objectives with the available resources, including time and budget."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sampling-strategies-2",
    "href": "static/slides/Sampling/Survey.html#sampling-strategies-2",
    "title": "Survey Methodology Design",
    "section": "Sampling Strategies",
    "text": "Sampling Strategies\nIntroduction to Sampling Strategies:\n\nSurveys typically do not involve collecting data from the entire population due to practical constraints.\nInstead, sampling strategies are employed to select a representative subset of the population, which allows researchers to make inferences about the broader population.\nThe goal of sampling is to ensure that the selected sample is representative and that the findings can be generalized with known levels of precision and accuracy.\n\nProbability Sampling vs.¬†Non-Probability Sampling:\n\nProbability Sampling: In probability sampling, every unit in the population has a known, non-zero chance of being selected.\n\nThis method is grounded in statistical theory and allows for the calculation of sampling error and the application of inferential statistics.\nIt is the preferred method for surveys when the goal is to make generalizable claims about the entire population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#construction-and-use-of-sample-weights",
    "href": "static/slides/Sampling/Survey.html#construction-and-use-of-sample-weights",
    "title": "Survey Methodology Design",
    "section": "Construction and Use of Sample Weights",
    "text": "Construction and Use of Sample Weights\nIntroduction to Sample Weights:\n\nIn most surveys, not every individual or unit has the same probability of being selected.\nTo ensure that survey results accurately reflect the entire population, sample weights are applied during the analysis phase.\nSample weights adjust for unequal probabilities of selection, non-response, and other factors that could lead to biased estimates.\nThey are essential for producing valid, generalizable results.\n\nNeed for Sample Weights:\n\nSample weights are necessary to ensure that each unit‚Äôs contribution to the survey results is proportional to its representation in the population.\nWithout weights, some groups may be over-represented or under-represented, leading to biased conclusions.\nUnequal Probabilities of Selection: In complex survey designs (e.g., stratified or cluster sampling), different units may have varying probabilities of being selected.\n\nWeights correct for these differences to ensure that the sample represents the population accurately."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#estimation-of-sampling-errors-for-survey-data",
    "href": "static/slides/Sampling/Survey.html#estimation-of-sampling-errors-for-survey-data",
    "title": "Survey Methodology Design",
    "section": "Estimation of Sampling Errors for Survey Data",
    "text": "Estimation of Sampling Errors for Survey Data\nIntroduction to Sampling Errors:\n\nIn survey research, sampling error refers to the difference between an estimate derived from a sample and the actual population parameter.\nSampling errors occur because surveys observe only a portion of the population rather than the entire population.\nUnderstanding and estimating these errors is crucial for determining the reliability and precision of survey estimates.\n\nImportance of Estimating Sampling Errors:\nEstimating sampling errors allows researchers to:\n\nQuantify Uncertainty: Sampling errors provide a measure of how much an estimate might differ from the true population value.\nEvaluate Precision: Larger sampling errors suggest less precision in survey estimates, while smaller errors indicate more reliable estimates.\nCompare Estimates: Sampling errors allow for the comparison of different survey estimates by providing confidence intervals and margins of error.\nDetermine Sample Size: Knowledge of sampling errors helps researchers calculate the optimal sample size to achieve a desired level of precision."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#reporting-and-using-survey-data",
    "href": "static/slides/Sampling/Survey.html#reporting-and-using-survey-data",
    "title": "Survey Methodology Design",
    "section": "Reporting and Using Survey Data",
    "text": "Reporting and Using Survey Data\nIntroduction to Reporting and Using Survey Data:\n\nOnce survey data has been collected and analyzed, the next crucial step is reporting and effectively using the results.\nSurvey data is typically used to inform decision-making, guide policy, support research, and provide insights into various populations or phenomena.\nClear, accurate, and transparent reporting is essential to ensure that the results are understandable, reliable, and actionable.\n\nImportance of Clear and Accurate Reporting:\n\nReporting survey results involves more than just presenting raw data.\nProper reporting includes a clear explanation of the survey methodology, the results, and any limitations.\nThis ensures that data users can interpret the findings correctly and apply them to their specific needs.\nKey Aspects of Survey Reporting:\n\n\nMethodology: A detailed description of the survey design, sampling methods, and data collection process is essential for understanding the validity of the results.\nFindings: Present the key results clearly and concisely, focusing on answering the research questions or survey objectives.\nInterpretation: Offer interpretations of the findings, highlighting their implications and how they can inform policy, research, or practice.\nLimitations: Acknowledge any potential biases, sampling errors, or limitations that may affect the interpretation of the results."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-6",
    "href": "static/slides/Sampling/Survey.html#section-6",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Field Supervision:\n\nImportance of Supervision: Supervision is critical to ensure that interviewers adhere to the survey protocols and to provide immediate feedback where errors are detected.\n\nSupervisors should also ensure that logistical issues, such as travel and material availability, do not disrupt the survey process.\n\nMonitoring: Monitoring fieldwork ensures the consistency and completeness of data collection.\n\nIt involves verifying that interviewers are following procedures correctly and that any deviations or errors are corrected in real-time.\nSupervisors may also randomly check completed questionnaires to ensure accuracy and adherence to guidelines."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-7",
    "href": "static/slides/Sampling/Survey.html#section-7",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Probability Sampling vs.¬†Non-Probability Sampling:\n\nProbability Sampling: In probability sampling, every unit in the population has a known, non-zero chance of being selected.\n\nThis method is grounded in statistical theory and allows for the calculation of sampling error and the application of inferential statistics.\nIt is the preferred method when the goal is to make generalizable claims about the entire population.\n\nAdvantages:\n\nAllows for generalization from sample to population.\nReduces selection bias.\nPermits the estimation of sampling errors."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-8",
    "href": "static/slides/Sampling/Survey.html#section-8",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Common Methods:\n\nSimple Random Sampling: Each unit in the population has an equal chance of being selected.\nSystematic Sampling: Every nth unit from a list of the population is selected after a random start.\nStratified Sampling: The population is divided into subgroups (strata) based on shared characteristics, and samples are drawn from each stratum.\nCluster Sampling: The population is divided into clusters (often geographic), and a random sample of clusters is selected, followed by data collection from all or a sample of units within the selected clusters."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-9",
    "href": "static/slides/Sampling/Survey.html#section-9",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Non-Probability Sampling: In non-probability sampling, not all units have a known or equal chance of being selected.\n\nThis method is often used when probability sampling is impractical, but it does not allow for generalization to the population.\n\nCommon Methods:\n\nConvenience Sampling: Selecting units that are easiest to reach.\nQuota Sampling: Ensuring specific quotas from various subgroups are met, but not through random selection.\nJudgmental Sampling: The researcher selects units based on their judgment of which will be most informative.\n\nLimitations:\n\nIncreased risk of selection bias.\nCannot calculate sampling error or confidently generalize to the population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-10",
    "href": "static/slides/Sampling/Survey.html#section-10",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Sample Size Determination:\n\nDetermining an Appropriate Sample Size: Sample size is a critical aspect of survey design, impacting the precision of estimates and the ability to detect meaningful differences in the population.\n\nDetermining the appropriate sample size involves balancing statistical, practical, and financial considerations.\n\n\nFactors Affecting Sample Size:\n\nMagnitude of Survey Estimates: If the population exhibits significant variability on the key variables, larger samples may be necessary.\nTarget Population: The overall size of the population and the subgroups within it affect the required sample size.\nPrecision and Confidence Levels: Higher precision and confidence levels require larger samples.\nClustering Effects: In cluster sampling, the need to account for intra-cluster correlation means larger samples are often required."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-11",
    "href": "static/slides/Sampling/Survey.html#section-11",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Non-Response: Anticipated non-response rates must be accounted for by adjusting the sample size upward to ensure sufficient data is collected.\nSample Size Calculation: The formula for calculating sample size typically considers the desired level of precision, the variability in the population, the confidence level, and the anticipated non-response rate.\n\nStratification:\n\nStratified Sampling: In stratified sampling, the population is divided into distinct subgroups (strata) that share similar characteristics.\n\nSamples are then drawn from each stratum either proportionately (based on the size of each stratum) or disproportionately (based on analytical needs)."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-12",
    "href": "static/slides/Sampling/Survey.html#section-12",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Benefits of Stratification:\n\nIncreases precision by reducing variability within each stratum.\nEnsures adequate representation of all subgroups.\nAllows for separate estimates for each stratum, which may be of interest for certain analyses.\n\nSample Allocation:\n\nProportional Allocation: The sample size in each stratum is proportional to the size of the stratum in the population.\nOptimal Allocation: Takes into account both the size of each stratum and the variability within each stratum to allocate the sample more efficiently."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-13",
    "href": "static/slides/Sampling/Survey.html#section-13",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Cluster Sampling:\n\nCluster Sampling Overview: In cluster sampling, the population is divided into naturally occurring groups or clusters (e.g., geographic areas), and a sample of clusters is selected.\n\nThis is often used when it is impractical to sample individuals directly, especially in large geographic areas.\n\nCharacteristics of Cluster Sampling:\n\nEconomical for surveys involving large populations spread over wide areas.\nReduces fieldwork costs, as data is collected from specific clusters rather than dispersed individuals."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-14",
    "href": "static/slides/Sampling/Survey.html#section-14",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Design Effects (deff): Clustering often increases the sampling error due to the similarity of units within each cluster (intra-cluster correlation).\n\nThe design effect (deff) is a factor that accounts for this increased variance in sample estimates.\nA larger design effect typically requires larger samples to achieve the desired level of precision.\n\nCalculating deff: It is calculated as the ratio of the actual variance of a survey estimate under the given sampling design to the variance under simple random sampling.\n\nMulti-Stage Sampling:\n\nSampling in Stages: Multi-stage sampling involves selecting samples in two or more stages.\n\nFor example, first selecting clusters (e.g., geographic areas), and then selecting units (e.g., individuals or households) within each cluster."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-15",
    "href": "static/slides/Sampling/Survey.html#section-15",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Two-Stage Sampling:\n\nStage 1: Select a random sample of clusters (e.g., villages or schools).\nStage 2: Select a random sample of individuals within each selected cluster.\n\nBenefits of Multi-Stage Sampling:\n\nReduces the overall cost of surveys.\nAllows for practical fieldwork implementation by focusing resources on specific clusters.\nProvides flexibility in sample design, especially for geographically dispersed populations."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-16",
    "href": "static/slides/Sampling/Survey.html#section-16",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Sampling with Probability Proportional to Size (PPS):\n\nPPS Sampling: In Probability Proportional to Size (PPS) sampling, the probability of selecting a cluster is proportional to its size (e.g., the number of individuals in a village).\n\nThis method ensures that larger clusters have a higher chance of being selected, which can improve the efficiency of the sample design.\n\nPPES (Probability Proportional to Estimated Size): In some cases, estimates of population sizes may be used when the actual sizes are unknown.\n\nPPES allows for more flexible sampling in cases where population data may not be fully accurate."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-17",
    "href": "static/slides/Sampling/Survey.html#section-17",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Special Topics in Sampling:\n\nTwo-Phase Sampling: Two-phase sampling involves conducting an initial survey on a broad sample and then selecting a subsample from the first phase for further, more detailed data collection.\n\nThis approach is useful when specific subpopulations need to be studied in more depth.\n\nSampling for Trend Estimation: When estimating trends over time, repeated surveys are conducted on the same population.\n\nSampling strategies need to account for changes in the population and the need for consistent, comparable estimates over multiple survey waves."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-18",
    "href": "static/slides/Sampling/Survey.html#section-18",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Dealing with Implementation Issues:\n\nCommon Implementation Challenges:\n\nTarget Population Definition and Coverage: Defining the population too narrowly or too broadly can affect the generalizability of results.\nSample Size Constraints: Budget limitations may result in smaller-than-ideal sample sizes, reducing the precision of estimates.\nNon-Response: High non-response rates can introduce bias. It is essential to develop strategies for dealing with non-response, such as reweighting data or conducting follow-ups with non-respondents.\n\nWhen Sampling Goes Wrong:\n\nIf issues arise during sampling implementation, such as oversampling or undersampling certain groups, corrective actions must be taken.\nThese could include adjusting the sample weights or re-sampling affected units."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-19",
    "href": "static/slides/Sampling/Survey.html#section-19",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Sampling Frames and Master Samples\n\nIntroduction to Sampling Frames:\n\nA sampling frame is a critical component in any survey design, essentially the list or database from which a sample is drawn.\nThe quality and completeness of the sampling frame directly impact the reliability and validity of the survey results.\n\n\nDefinition and Properties of Sampling Frames:\n\nSampling Frame: A comprehensive and up-to-date list of all units in the population from which a sample can be drawn.\n\nExamples: Lists of households, business directories, voter registers, and school rosters.\n\nProperties of a Good Sampling Frame:\n\nCompleteness: It should include every unit in the population without duplication or omissions.\nAccuracy: The information in the frame should be up-to-date to avoid sampling errors.\nNon-overlapping Units: Each unit should appear only once in the frame to avoid bias.\nOperational Feasibility: The frame must be easy to use and access for drawing samples."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-20",
    "href": "static/slides/Sampling/Survey.html#section-20",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Common Challenges with Sampling Frames:\n\nOutdated Information: Outdated data can lead to the selection of units that no longer exist or have changed.\nIncomplete Coverage: Important subgroups of the population may be missed, leading to under-coverage.\nDuplication: If units appear multiple times, they may be overrepresented, leading to bias.\n\n\nTypes of Sampling Frames:\n\nList Frames: A list frame consists of a list of individual units that make up the population.\n\nExamples: A list of registered voters for an electoral survey or a customer database for a market survey.\nAdvantages: Provides direct access to units and can be used for simple random sampling.\nDisadvantages: List frames can become outdated quickly, especially in dynamic populations."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sampling-frames-and-master-samples",
    "href": "static/slides/Sampling/Survey.html#sampling-frames-and-master-samples",
    "title": "Survey Methodology Design",
    "section": "Sampling Frames and Master Samples",
    "text": "Sampling Frames and Master Samples\nIntroduction to Sampling Frames:\n\nA sampling frame is a critical component in any survey design.\nIt is essentially the list or database from which a sample is drawn.\nThe quality and completeness of the sampling frame directly impact the reliability and validity of the survey results.\nA well-constructed sampling frame should accurately represent the population of interest and minimize the risk of coverage errors, where some units of the population are omitted or included multiple times."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-21",
    "href": "static/slides/Sampling/Survey.html#section-21",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Area Frames:\n\nArea Frame: An area frame divides a geographic region into identifiable areas or clusters (e.g., districts, neighborhoods), from which a sample is drawn.\n\nThis is commonly used in multi-stage sampling.\n\nAdvantages:\n\nEffective when no list of individuals exists or is feasible to compile.\nAllows sampling over large geographical areas and can incorporate complex designs.\n\nDisadvantages:\n\nArea frames may be less precise and result in clustering effects, requiring larger sample sizes to achieve the same level of precision."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-22",
    "href": "static/slides/Sampling/Survey.html#section-22",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Common Problems with Sampling Frames:\n\nUnder-Coverage: A common issue where certain segments of the population are missing from the frame.\n\nFor example, people in remote areas or individuals without formal addresses might be excluded from list frames.\n\nOver-Coverage: Occurs when units that should not be included appear in the sampling frame.\n\nFor example, a list of households might include vacant or abandoned properties, which should not be sampled.\n\nDuplication: If units appear multiple times in the frame, they may have a higher probability of being selected, leading to bias in the sample.\n\nIt is important to clean the frame to remove duplicates.\n\nFrame Maintenance: Maintaining an up-to-date frame is challenging, particularly in populations where frequent changes occur (e.g., households moving, new businesses opening).\n\nContinuous updates are necessary to ensure the frame remains accurate and representative."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-23",
    "href": "static/slides/Sampling/Survey.html#section-23",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Purpose of Documentation:\n\nProvides a clear and complete description of the sample design, including assumptions and decisions.\nFacilitates evaluation by allowing reviewers to understand how the sample was drawn and what challenges were encountered.\nEnhances transparency and credibility in the survey results.\nAllows future surveys to replicate or build upon the existing design.\n\nKey Elements of Sample Documentation:\n\n\nSample Design Overview: A description of the overall sampling strategy, including the population, sample size, and sampling methods.\nSelection Probabilities: Information on how units were selected, including the probabilities assigned at each stage of sampling.\nStratification and Clustering: Details on the use of stratification or clustering and how these decisions affected sample selection and design.\nWeighting Procedures: A description of how weights were assigned to account for unequal probabilities of selection, non-response, and other factors.\nSurvey Instruments: The questionnaires and tools used to collect data from the sampled units.\nField Implementation: Procedures for carrying out the survey in the field, including instructions given to interviewers and any challenges faced during data collection.\nResponse Rates: Documentation of response rates and any issues related to non-response or refusal to participate.\nAdjustments: Any changes made during the survey, such as sample modifications or corrective actions taken in response to field issues."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-24",
    "href": "static/slides/Sampling/Survey.html#section-24",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Types of Documentation:\n\nDesign Variables: In any survey, specific design variables are created to track the sampling process.\n\nThese include variables such as strata, clusters, and selection probabilities.\nProper labeling and documentation of these variables are essential for accurate analysis.\nExamples of Design Variables:\n\nStratum ID: Indicates the stratum to which each unit belongs.\nCluster ID: Identifies the cluster from which a unit was selected.\nSelection Probability: Records the probability with which each unit was selected.\n\n\nSelection Probabilities: Documenting the selection probabilities for each unit in the sample ensures that the sampling process is properly understood and can be evaluated.\n\nThese probabilities are critical for weighting the data during analysis.\n\nImportance: Ensures that analysts can correctly apply weights to adjust for unequal probabilities of selection and provide valid population estimates."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-25",
    "href": "static/slides/Sampling/Survey.html#section-25",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Response Rates and Coverage Rates:\n\nResponse Rates: Response rates indicate the proportion of sampled units that provided usable data.\n\nHigh response rates are essential for minimizing bias, while low response rates can introduce bias into survey results, particularly if non-respondents differ systematically from respondents.\n\nTypes of Response Rates:\n\nUnit Response Rate: The percentage of sampled units that responded to the survey.\nItem Response Rate: The percentage of responses for specific survey questions or items.\n\nCoverage Rates: Coverage refers to how well the sample frame covers the target population. Coverage rates measure the proportion of the population included in the sampling frame, as well as any groups that might have been excluded. Poor coverage can lead to under-coverage bias, where certain segments of the population are not represented in the sample.\nEvaluating Coverage:\n\nCompare the characteristics of the population with those of the sampled units to identify potential coverage gaps.\nUse external data sources (e.g., census data) to assess whether any key demographic groups were missed or over-represented."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-26",
    "href": "static/slides/Sampling/Survey.html#section-26",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Weighting and Adjustments:\n\nSurvey weights are essential for producing accurate and representative estimates, particularly in complex survey designs where unequal probabilities of selection, non-response, and coverage issues arise.\n\nWeighting adjusts for these factors to ensure that survey results accurately reflect the population.\n\nBase Weights: Base weights are calculated based on the inverse of the selection probabilities.\n\nFor example, if a unit has a selection probability of 1/1000, its base weight would be 1000, meaning that each selected unit represents 1000 units in the population.\n\nAdjustments for Non-Response: Weights are typically adjusted to account for non-response.\n\nFor instance, if a subgroup of the population has a lower response rate, the weights for the responding units in that subgroup are increased to compensate for the missing data.\n\nApproach: Post-stratification, where the sample is reweighted based on known population characteristics (e.g., age, gender, or region), is a common method to adjust for non-response."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-27",
    "href": "static/slides/Sampling/Survey.html#section-27",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Weighting for Unequal Selection Probabilities: In surveys where different units have different probabilities of being selected, weights are used to correct for these differences. For example, in cluster sampling, individuals in larger clusters might have a higher chance of being selected, requiring a weight adjustment to ensure that their data does not disproportionately influence the results.\n\nEvaluation of Sampling Costs:\n\nUnderstanding and documenting the costs associated with different stages of the survey is crucial for assessing the efficiency of the sample design.\nSurveys are resource-intensive, and sampling costs are influenced by factors such as the number of units sampled, the geographic area covered, and the complexity of the survey design.\nTypes of Costs:\n\n\nFieldwork Costs: Include interviewer salaries, transportation, and other expenses related to collecting data.\nData Processing Costs: Include costs for coding, data entry, and quality control.\nSampling Costs: Cover the expenses related to designing the sample, selecting units, and maintaining sampling frames."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-28",
    "href": "static/slides/Sampling/Survey.html#section-28",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Importance of Cost Evaluation:\n\nHelps optimize future surveys by identifying areas where efficiencies can be gained.\nAllows for comparison between different sampling designs in terms of cost-effectiveness.\n\n\nLimitations and Evaluation of Survey Data:\n\nAll surveys have limitations, and it is essential to evaluate these limitations as part of the documentation process.\nThe most common limitations involve sampling error, non-sampling error, and data quality issues.\nSampling Error: Sampling error occurs because the survey only observes a subset of the population rather than the entire population.\n\nIt is an inherent part of any sampling process and can be quantified through measures like standard error and confidence intervals.\nSampling errors can be minimized by increasing the sample size or improving the sampling design.\n\nNon-Sampling Error: Non-sampling errors arise from issues unrelated to the sampling process, such as measurement errors, interviewer bias, or data processing mistakes.\n\nThese errors are often more challenging to detect and correct than sampling errors."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-29",
    "href": "static/slides/Sampling/Survey.html#section-29",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Examples of Non-Sampling Errors:\n\nMeasurement Error: Respondents misinterpret questions or provide incorrect answers.\nProcessing Error: Mistakes during data entry or coding.\nInterviewer Error: Interviewers influencing responses or misrecording answers.\n\nEvaluating Data Quality:\n\nData Quality Checks: Include checks for consistency, outliers, and missing values.\nPilot Testing: Conducting a pilot survey before full data collection helps identify and address potential issues with the survey instrument or process."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-30",
    "href": "static/slides/Sampling/Survey.html#section-30",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Non-Response: In most surveys, some individuals or units do not respond.\n\nWeights are adjusted to compensate for non-response by increasing the influence of similar respondents who did participate.\n\nPopulation Estimates: Weights are used to extrapolate from the sample to the population, allowing survey results to estimate totals, proportions, and averages for the entire population.\n\nDevelopment of Sample Weights:\n\nThe process of developing sample weights involves several steps to ensure that the final weights reflect the population and correct for any biases that may have occurred during the sampling and data collection process.\n\n1. Base Weights:\n\nThe base weight is the initial weight assigned to each sampled unit and is the inverse of its probability of selection.\nFormula: \\[\\text{Base Weight} = \\frac{1}{\\text{Selection Probability}}\\]\n\nFor example, if a unit has a selection probability of 0.01 (i.e., 1 out of 100), its base weight would be 100. This means the selected unit represents 100 units in the population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#documentation-and-evaluation-of-sample-designs",
    "href": "static/slides/Sampling/Survey.html#documentation-and-evaluation-of-sample-designs",
    "title": "Survey Methodology Design",
    "section": "Documentation and Evaluation of Sample Designs",
    "text": "Documentation and Evaluation of Sample Designs\nIntroduction to Documentation and Evaluation of Sample Designs:\n\nSurvey sampling involves multiple steps, from designing the sample to collecting and analyzing data.\nProper documentation and evaluation are crucial to ensure that the sample design is accurately implemented and that its effectiveness is assessed.\nThis process helps in identifying potential issues, improving future surveys, and ensuring transparency and reproducibility in the sampling process.\n\nImportance of Sample Documentation:\nDocumentation involves recording all aspects of the sample design, implementation, and analysis.\n\nIt serves as a reference for survey practitioners, analysts, and stakeholders, ensuring that decisions made throughout the sampling process are transparent and can be replicated."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-31",
    "href": "static/slides/Sampling/Survey.html#section-31",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Adjustments for Unknown Eligibility:\n\nIn some cases, not all sampled units may be eligible for the survey.\nFor example, a household might no longer exist, or an individual may not meet the eligibility criteria (e.g., age).\nWeights must be adjusted to account for this.\nApproach: Survey managers may adjust the base weights to reflect the probability that a sampled unit is eligible, based on available data about the population.\n\n3. Adjustments for Duplicates:\n\nIf a sampling frame contains duplicate entries, adjustments are needed to ensure that units that appear more than once in the frame are not over-represented in the sample.\nMethod: By identifying duplicates in the frame, the corresponding weights can be reduced so that no unit is given more influence than it should have."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-32",
    "href": "static/slides/Sampling/Survey.html#section-32",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Weighting for Unequal Probabilities of Selection:\n\nUnequal selection probabilities arise in several situations, such as when the population is stratified, or when cluster sampling is used.\nIn these cases, weights must adjust for the differing probabilities of inclusion to ensure fair representation of all groups in the final analysis.\nStratified Sampling: In stratified sampling, the population is divided into subgroups (strata), and samples are drawn separately from each stratum.\n\nIf the sampling rates differ across strata, weights are needed to account for the varying probabilities of selection.\n\nCluster Sampling: In cluster sampling, individuals within selected clusters may have a higher probability of being included in the sample than individuals in non-selected clusters.\n\nWeights adjust for this by ensuring that the population within non-sampled clusters is represented proportionally."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-33",
    "href": "static/slides/Sampling/Survey.html#section-33",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Adjusting Weights for Non-Response:\n\nNon-response is a common issue in surveys, where some sampled units do not provide data.\nThis can lead to bias if the non-respondents differ from respondents in significant ways.\nWeights are adjusted to account for non-response, ensuring that the survey results remain representative of the population.\nNon-Response Bias: Non-response bias occurs when individuals who do not respond to the survey differ in key ways from those who do.\n\nFor example, younger individuals may be less likely to respond, leading to an over-representation of older individuals if weights are not adjusted.\n\nMethods to Adjust for Non-Response:\n\n\nPost-Stratification: After data collection, weights are adjusted to match known population totals (e.g., from census data) for specific demographic variables, such as age, gender, or region.\nResponse Propensity Modeling: This method uses statistical models to predict the likelihood of response based on available data and adjusts the weights accordingly.\n\nBy applying these techniques, survey managers can reduce the impact of non-response and produce more accurate, representative results."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-34",
    "href": "static/slides/Sampling/Survey.html#section-34",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Adjusting for Non-Coverage:\n\nNon-coverage occurs when certain segments of the population are excluded from the sampling frame.\nThis might happen due to practical limitations, such as the inability to reach remote areas or populations not included in administrative records.\nAdjusting weights for non-coverage ensures that these omitted groups are accounted for in the final analysis.\nSources of Non-Coverage:\n\nGeographic Non-Coverage: Some areas (e.g., rural or hard-to-reach regions) may not be included in the sampling frame, leading to an under-representation of these populations.\nDemographic Non-Coverage: Certain demographic groups (e.g., transient populations, undocumented individuals) might be excluded from the frame.\n\nCompensating for Non-Coverage: To adjust for non-coverage, survey managers typically use external data sources (e.g., census data, administrative records) to estimate the size and characteristics of the under-covered population. Weights are then adjusted to ensure that these groups are represented proportionally."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-35",
    "href": "static/slides/Sampling/Survey.html#section-35",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Sampling Variance and Weighting:\n\nOne challenge of using weights is that they tend to increase the variance of survey estimates.\nThis is known as sampling variance, and it occurs because weights introduce more variability into the analysis.\nIncrease in Variance: Weights increase the effective size of certain units in the sample, which can lead to higher variability in the survey estimates.\n\nThis is particularly true when there are large differences between the weights applied to different units (i.e., when the weights vary significantly across the sample)."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-36",
    "href": "static/slides/Sampling/Survey.html#section-36",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Compensating for Increased Variance:\n\n\nTrimming of Weights: Trimming involves reducing extreme weights to control for excessively high variability. This method helps in stabilizing the estimates, but care must be taken to ensure that this does not introduce bias.\nDesign Effect: The design effect (deff) measures the increase in variance due to the sample design and weighting.\n\nSurveys with complex designs (e.g., stratification, clustering, and weighting) often have higher design effects, meaning larger sample sizes are required to achieve the same level of precision as in simple random sampling.\n\n\nTrimming of Weights:\n\nWeight trimming is the process of reducing the influence of extreme weights to prevent inflated variance in survey estimates.\nWhile applying weights is necessary for accurate representation, extremely large or small weights can distort the results.\nReasons for Trimming:\n\nTo reduce the impact of outliers or highly variable units.\nTo improve the precision of survey estimates by controlling variance."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-37",
    "href": "static/slides/Sampling/Survey.html#section-37",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Approaches to Weight Trimming:\n\n\nThreshold Trimming: Set a maximum allowable weight, and any weight exceeding this threshold is capped at that value.\nRelative Trimming: Adjust weights relative to the overall distribution of weights in the sample, ensuring that no unit has an overly disproportionate influence on the results.\n\n\nWhile trimming can help improve the precision of estimates, it must be done carefully to avoid introducing bias.\nIt is important to evaluate the impact of trimming on the final survey results."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-38",
    "href": "static/slides/Sampling/Survey.html#section-38",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Sampling Variance Under Simple Random Sampling:\n\nThe simplest form of sampling error estimation occurs in simple random sampling (SRS), where every unit in the population has an equal chance of being selected.\nFor SRS, the sampling variance of an estimate can be calculated using standard formulas.\nFormula for the Sampling Variance: Var(Y^)=1n(N‚àínN‚àí1)S2\n\\[\\text{Var}(\\hat{Y}) = \\frac{1}{n}\\left(\\frac{N-n}{N-1} \\right)S^2\\]\n\nWhere: - \\(\\hat{Y}\\) is the estimate (e.g., mean or proportion). - n is the sample size. - N is the population size. - \\(S^2\\) is the variance of the population.\n\nThis formula provides the basis for estimating the standard error (SE), which is the square root of the sampling variance."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-39",
    "href": "static/slides/Sampling/Survey.html#section-39",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Standard Error: The standard error (SE) represents the average expected deviation of the sample estimate from the true population parameter. It is calculated as:\n\n\\[SE = \\sqrt{\\text{Var}(\\hat{Y})}\\]\n\nConfidence Intervals: Using the standard error, confidence intervals (CI) can be constructed to quantify the uncertainty of an estimate. A typical 95% CI is calculated as:\n\n\\[CI = \\hat{Y} \\pm 1.96 \\times SE\\]\n\nThis interval indicates that there is a 95% chance the true population parameter lies within the interval."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-40",
    "href": "static/slides/Sampling/Survey.html#section-40",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Sampling Variance in More Complex Designs:\n\nIn practice, most surveys use complex designs such as stratified sampling, cluster sampling, or multi-stage sampling, where calculating sampling variance is more complicated.\nThese designs introduce additional sources of variance that need to be accounted for.\n\n1. Stratified Sampling:\n\nIn stratified sampling, the population is divided into strata (subgroups), and separate samples are drawn from each stratum.\nStratified designs generally reduce sampling variance by increasing homogeneity within each stratum.\nStratified Variance Formula:\n\n\\[\\text{Var}(\\hat{Y}) = \\sum_{h=1}^{H} \\left(\\frac{N_h^2}{N^2} \\right) \\left( \\frac{1}{n_h} \\left(\\frac{N_h - n_h}{N_h-1} \\right) S_h^2 \\right)\\]\nWhere: - h is the stratum index. - H is the number of strata. - \\(N_h\\) and \\(n_h\\) are the population and sample sizes for stratum h. - \\(S_h^2\\) is the variance within stratum h.\n\nStratified sampling reduces overall variance because it allows for more efficient sampling by accounting for differences among strata."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-41",
    "href": "static/slides/Sampling/Survey.html#section-41",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Cluster Sampling:\n\nIn cluster sampling, the population is divided into clusters, and a random sample of clusters is selected.\nIndividuals within selected clusters are then sampled.\nClustering often increases sampling variance because individuals within clusters tend to be more similar to each other (intra-cluster correlation), leading to less diversity in the data.\nDesign Effect (deff): The design effect (deff) is a multiplier that adjusts the variance to account for the effects of clustering. It is calculated as:\n\n\\[deff = 1 + (m - 1) \\rho\\]\nWhere:\n\nm is the average number of units per cluster.\n\\(\\rho\\) is the intra-class correlation (the similarity between individuals within clusters).\nThe design effect increases as \\(\\rho\\) increases or the cluster size grows, leading to higher sampling variance."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-42",
    "href": "static/slides/Sampling/Survey.html#section-42",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Variance Formula for Cluster Sampling:\n\n\\[\\text{Var}(\\hat{Y}) = \\frac{deff}{n} \\left(\\frac{S^2}{m} \\right)\\]\n\nCluster sampling usually requires larger sample sizes to achieve the same precision as SRS due to increased variance within clusters.\n\n3. Multi-Stage Sampling:\n\nIn multi-stage sampling, the sampling process occurs in multiple stages.\n\nFor example, a country might be divided into districts (first stage), and then households are sampled within selected districts (second stage).\n\nEach stage adds a layer of complexity to variance estimation.\nVariance in Multi-Stage Designs: Multi-stage sampling typically combines aspects of stratification and clustering, so the variance depends on both the design effect and the stratification of units across stages.\nEstimating variance for multi-stage designs often requires specialized software or formulas that account for multiple levels of selection."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-43",
    "href": "static/slides/Sampling/Survey.html#section-43",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Design Effect (deff) and Its Impact on Variance:\n\nThe design effect quantifies how much the sampling variance is inflated due to the use of a complex design compared to simple random sampling.\nThe higher the design effect, the more variability there is in the sample estimate, and larger sample sizes may be needed to achieve the desired precision.\nInterpreting the Design Effect:\n\\(**deff = 1**:\\) No inflation in variance; this is equivalent to SRS.\n\\(**deff &gt; 1**:\\) The sampling variance is higher than under SRS, typically due to clustering or unequal weights.\n\\(**deff &lt; 1**:\\) Occurs when stratification or weighting reduces variance compared to SRS.\nThe design effect is a crucial factor in sample size calculations, as it directly affects the precision of survey estimates."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-44",
    "href": "static/slides/Sampling/Survey.html#section-44",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Methods of Estimating Sampling Variance for Complex Survey Designs:\n\nDue to the complexity of real-world survey designs, different methods are used to estimate sampling variance.\nThese include both exact methods and approximation techniques.\n\n1. Exact Methods:\n\nUltimate Cluster Method: The ultimate cluster method treats each cluster in a multi-stage design as a single unit and estimates the variance based on the variation between clusters rather than within them.\nThis method simplifies variance estimation but may lead to slightly inflated variance estimates.\n\n2. Approximation Techniques:\n\nLinearization: Linearization approximates the variance of a complex statistic (e.g., a ratio or nonlinear function) by treating it as a linear function of the data.\nThis approach is often used for calculating standard errors of complex survey statistics.\n\nReplication Methods: Replication methods create multiple ‚Äúreplicates‚Äù of the sample by repeatedly drawing subsamples (or jackknife samples) from the original data and calculating the variance across the replicates.\nJackknife Replication: Repeatedly removes one unit (or a small set of units) from the sample and recalculates the estimate, then measures the variability between the replicates.\nBalanced Repeated Replication (BRR): Uses specially designed replicate samples to calculate variances efficiently.\nBootstrap Methods: Randomly resamples the data with replacement and calculates variances across these resamples.\n\n\nReplication methods are computationally intensive but provide robust estimates of sampling variance, especially for complex survey designs."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-45",
    "href": "static/slides/Sampling/Survey.html#section-45",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Components of a Survey Report:\n\nA comprehensive survey report typically includes the following sections:\n\n1. Executive Summary:\n\nThe executive summary provides a brief overview of the survey, its objectives, key findings, and recommendations.\nIt is written for decision-makers who may not have time to read the full report.\nIt should focus on the most important results and their implications.\nKey Elements:\n\nObjectives of the survey.\nOverview of the methodology.\nSummary of the key findings.\nMajor recommendations or conclusions."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-46",
    "href": "static/slides/Sampling/Survey.html#section-46",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Introduction:\n\nThe introduction explains the context and purpose of the survey.\nIt should outline why the survey was conducted, what specific questions it aimed to answer, and the population or phenomena of interest.\nContextual Information:\n\nBackground on the issue or topic being studied.\nThe importance of the survey for policy or decision-making.\nSpecific objectives or research questions.\n\n\n3. Methodology:\n\nThis section provides a detailed description of how the survey was conducted.\nIt includes information about the survey design, sampling methods, data collection process, and analysis techniques.\nDetailed Breakdown:\n\nSampling Design: Explain how the sample was selected (e.g., stratified, random, cluster), the sampling frame used, and any stratification or clustering done.\nSample Size: Include the sample size and how it was determined, accounting for non-response and the target population size.\nData Collection Methods: Describe how data was collected (e.g., face-to-face interviews, online questionnaires) and any quality control measures.\nWeighting: If weights were applied, explain how they were calculated and why they were necessary.\nData Processing: Provide details on how the data was cleaned, coded, and prepared for analysis."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-47",
    "href": "static/slides/Sampling/Survey.html#section-47",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "4. Findings:\n\nThis section presents the survey‚Äôs results in a structured and easy-to-understand format.\nUse tables, charts, and graphs to display key data points, trends, and comparisons.\nData Presentation:\n\nPresent the results in a logical order, typically starting with overall findings and then breaking them down by subgroups (e.g., age, gender, region).\nUse descriptive statistics (e.g., means, medians, percentages) to summarize key data points.\nInclude cross-tabulations and comparisons between different subgroups, where relevant.\nHighlight significant findings, trends, and patterns in the data.\n\n\n5. Interpretation and Discussion:\n\nAfter presenting the data, this section provides an interpretation of the results.\nIt explains what the findings mean, how they answer the research questions, and what their implications are for the broader context or specific stakeholders."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-48",
    "href": "static/slides/Sampling/Survey.html#section-48",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Key Considerations:\n\nRelate the findings to the survey‚Äôs objectives and research questions.\n\nDiscuss how the results compare with previous research or expected outcomes.\nHighlight the practical implications of the results, such as how they can be used to inform policy, improve services, or address specific issues.\nConsider unexpected results and explore potential explanations.\n\n\n6. Limitations:\n\nAll surveys have limitations, and it‚Äôs important to acknowledge them transparently.\nThis section discusses any factors that may have affected the accuracy or reliability of the results, such as sampling errors, non-response bias, or measurement errors.\nExamples of Limitations:\nSampling Error: If the sample was not perfectly representative of the population, the results might not generalize as intended.\n\nNon-Response Bias: If certain groups were less likely to respond to the survey, their under-representation could skew the results.\n\n\n-   **Measurement Error**: If some questions were misinterpreted by respondents, this could affect the accuracy of the data.\n-   **Limitations of the Data Collection Method**: Online surveys, for instance, may exclude individuals without internet access, leading to coverage bias."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-49",
    "href": "static/slides/Sampling/Survey.html#section-49",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "7. Recommendations:\n\nBased on the survey‚Äôs findings, this section offers actionable recommendations for stakeholders, policymakers, or researchers.\nThese recommendations should be directly linked to the survey‚Äôs objectives and should address the key findings.\nTypes of Recommendations:\nPolicy Recommendations: Based on the findings, suggest specific policy actions or interventions.\nProgrammatic Recommendations: Propose ways to improve services, programs, or processes.\nResearch Recommendations: Highlight areas where further research is needed to explore questions that arose during the survey.\n\n8. Appendices:\n\nThe appendices provide additional information that supports the main report but is too detailed to include in the main text.\nThis might include the survey questionnaire, sampling tables, detailed statistical outputs, or technical notes on the survey design."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-50",
    "href": "static/slides/Sampling/Survey.html#section-50",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Presentation of Data:\nThe way data is presented is critical to ensuring that it is easily understood and effectively communicates the survey‚Äôs findings. Tables, graphs, and charts are essential tools for summarizing large amounts of data and making comparisons.\n\nTypes of Visual Data Presentation:\n\n\nTables: Use tables to display detailed numeric data, cross-tabulations, and comparisons across different subgroups.\nCharts and Graphs: Use bar charts, pie charts, line graphs, and histograms to visualize trends, distributions, and differences between groups.\nMaps: For geographically-based surveys, maps are useful for showing regional differences or spatial patterns.\n\n\nBest Practices for Data Presentation:\n\n\n\nKeep visuals simple and easy to interpret.\nLabel axes, categories, and values clearly.\nUse consistent colors and formats across charts and tables.\nAvoid overloading charts with too much information; each visual should focus on one key message."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-51",
    "href": "static/slides/Sampling/Survey.html#section-51",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Using Survey Data:\n\nSurvey data can be used in a variety of ways, depending on the survey‚Äôs purpose and the needs of the data users.\nCommon uses of survey data include informing policy decisions, guiding program development, conducting academic research, and making business or organizational decisions.\n\n1. Policy and Decision-Making:\n\nSurvey data is often used to inform policy at the national, regional, or local level.\nBy providing evidence on the needs, preferences, or behaviors of a population, survey data helps policymakers develop and implement effective policies.\nExamples:\n\nHealth surveys might inform public health strategies by identifying underserved populations or areas with high disease prevalence.\nLabor force surveys provide insights into employment trends, helping governments design job creation programs."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-52",
    "href": "static/slides/Sampling/Survey.html#section-52",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Program and Service Improvement:\n\nOrganizations and service providers use survey data to assess the effectiveness of their programs, identify areas for improvement, and develop strategies for better service delivery.\nExamples:\nEducational surveys may reveal gaps in student performance across different regions, prompting changes in resource allocation. - Customer satisfaction surveys help businesses identify strengths and areas for improvement in their services or products.\n\n3. Academic and Market Research:\n\nResearchers use survey data to test hypotheses, explore trends, and generate new knowledge.\nBusinesses use survey data for market research to understand consumer preferences, segment markets, and develop targeted marketing strategies.\n\nEthical Considerations in Reporting Survey Data:\n\nEthical reporting is an essential part of using survey data responsibly.\nResearchers must ensure that the data is presented accurately and without bias, and that the privacy and confidentiality of survey respondents are maintained."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-53",
    "href": "static/slides/Sampling/Survey.html#section-53",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Best Practices for Ethical Reporting:\n\nAccuracy: Present the data accurately, without manipulating or misrepresenting the findings.\nTransparency: Be open about the survey‚Äôs limitations, sampling errors, and any issues that may have affected the data.\nConfidentiality: Ensure that individual responses remain confidential, especially when dealing with sensitive data.\nNon-Bias: Avoid selective reporting of results that could lead to misleading conclusions."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-54",
    "href": "static/slides/Sampling/Survey.html#section-54",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3. Survey Planning and Data Processing System:\n\nImportance of Early Planning:\n\nSurvey planning must align with the data processing system to avoid complications during later stages.\nDefine what data will be collected, how it will be processed, and how the system will handle any anomalies or errors.\n\nObjectives and Content:\n\nThe complexity of the data processing system depends on the survey‚Äôs objectives and the amount of data collected.\n\nSurvey Procedures:\n\nProper documentation of survey procedures (questionnaire structure, coding schemes, etc.) is vital to efficient data processing."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-55",
    "href": "static/slides/Sampling/Survey.html#section-55",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "4. Design of Data Processing Systems:\n\nElements of a Household Survey Data Processing System:\n\nData Entry: Efficiently input collected data, whether manually or using automated systems.\nCoding: Assign numerical values to responses for standardized processing.\nQuality Control: Validate data at every stage to ensure consistency and reduce errors.\n\nSystem Design Principles:\n\nFlexibility: Handle a variety of data formats (e.g., numeric, categorical, open-ended).\nScalability: The system must accommodate large datasets if needed.\nSecurity: Safeguard sensitive respondent data through encryption and access control."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-56",
    "href": "static/slides/Sampling/Survey.html#section-56",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "5. Survey Operations and Data Processing:\n5.1 Frame Creation and Sample Design:\n\nSampling Frame:\n\nThe list or database of households or individuals from which the sample is drawn.\nThe quality of the sampling frame directly impacts the quality of the data.\n\nSample Design:\n\nDesign decisions (e.g., stratification, clustering) must be considered in data processing, especially when it comes to weighting and adjusting survey results.\n\n\n5.2 Data Collection and Management:\n\nData Collection Methods:\n\nPaper-based or electronic collection methods affect the type and complexity of the data processing system.\nManual systems require data entry, while electronic systems may automate parts of data validation.\n\nData Management:\n\nManage the flow of data from the field through a centralized data management system.\nSystems must track responses, follow up on non-response, and ensure completeness."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-57",
    "href": "static/slides/Sampling/Survey.html#section-57",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "5.3 Data Preparation:\n\nCoding and Data Cleaning:\n\nResponses need to be categorized using pre-defined codes, especially for open-ended questions.\nCleaning involves detecting and fixing errors, such as incorrect responses or missing data.\n\nData Validation:\n\nValidation checks confirm that all necessary responses are present, and that they follow logical consistency rules (e.g., no impossible answers).\n\n\n6. Quality Control in Data Processing:\n\nWhy Quality Control is Important:\n\nEnsures that data accurately reflects what was collected.\nMinimizes the risk of processing errors, which can skew analysis."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-58",
    "href": "static/slides/Sampling/Survey.html#section-58",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "Quality Control Techniques:\n\nDouble Data Entry: Enter data twice to catch entry errors by comparing both datasets.\nAutomated Error Checks: Use software to detect outliers or inconsistent answers.\nManual Reviews: Survey supervisors review and validate a sample of data entries to ensure accuracy.\n\n\n7. Coding and Classification:\n\nDefinition of Coding:\n\nAssigning numerical or categorical codes to survey responses for easier processing and analysis.\n\nCoding Open-Ended Questions:\n\nRequires careful design of coding categories to ensure that responses are classified meaningfully and consistently.\n\nClassification Systems:\n\nUse standard classification systems (e.g., for occupation, education, or industry) to categorize responses, ensuring comparability across surveys and datasets."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-59",
    "href": "static/slides/Sampling/Survey.html#section-59",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "8. Data Entry Systems:\n\nManual Data Entry:\n\nTyping survey responses into a database by hand, common for paper-based surveys.\nRequires careful validation to avoid errors.\n\nAutomated Data Entry:\n\nDigital tools (e.g., scanning technologies or direct electronic data capture) streamline entry and reduce human errors.\n\nError Detection and Prevention:\n\nAutomated systems should include real-time error checks, flagging outliers or impossible values during data entry.\n\n\n9. Data Cleaning and Editing:\n\nThe Role of Data Cleaning:\n\nData cleaning involves detecting and correcting errors, such as incomplete responses, duplicate records, or illogical data points.\n\nTypes of Errors:\n\nMissing Data: Responses that were not provided.\nInconsistent Data: Logical inconsistencies in the data (e.g., a respondent‚Äôs age is recorded as 200 years).\n\nEditing Rules:\n\nUse a set of predefined rules to ensure that corrections and edits are consistent across the dataset."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-60",
    "href": "static/slides/Sampling/Survey.html#section-60",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "10. Data Validation and Verification:\n\nValidation Checks:\n\nEnsure that all data points are accurate and logical.\nCross-validate data where possible (e.g., comparing age and date of birth to verify consistency).\n\nVerification:\n\nDouble-check a portion of the data to confirm that processing procedures were followed correctly.\nUse random spot checks or systematic verification of complex sections of the dataset.\n\n\n11. Final Dataset Preparation:\n\nProducing the Final Dataset:\n\nAfter cleaning and validation, the dataset is finalized for analysis.\nEnsure all variables are labeled clearly, and all missing data has been handled appropriately.\n\nDocumentation:\n\nAccompany the dataset with metadata explaining the survey methodology, coding schemes, variable definitions, and any changes made during data processing."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-61",
    "href": "static/slides/Sampling/Survey.html#section-61",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Importance of Sample Design:\n\nSample design is integral to ensuring that survey results are reliable and generalizable to the entire population.\nKey considerations in sample design:\n\nSample size and structure: Influences the precision and reliability of survey estimates.\nCost: Efficient design minimizes cost while ensuring accurate results.\n\n\n3. Stratified Sampling:\n3.1. Definition and Concept:\n\nStratified sampling divides the population into subgroups (strata) that are internally homogeneous but externally heterogeneous.\nThis technique reduces variability within each stratum and increases the precision of survey estimates."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#data-processing-for-household-surveys",
    "href": "static/slides/Sampling/Survey.html#data-processing-for-household-surveys",
    "title": "Survey Methodology Design",
    "section": "Data Processing for Household Surveys",
    "text": "Data Processing for Household Surveys\n1. Introduction to Data Processing for Household Surveys:\n\nData processing is the critical step that transforms raw survey data into usable information for analysis.\nIt involves activities such as data preparation, entry, validation, and cleaning to ensure that the final dataset is accurate and reliable.\nProper planning and execution of data processing is essential to maintain data integrity throughout the survey cycle.\n\n2. The Household Survey Cycle:\n\nThe survey cycle refers to the full sequence of steps from planning to reporting:\n\nSurvey Planning: Objectives, sample design, and questionnaire development.\nData Collection: Fieldwork to collect responses.\nData Processing: Entry, cleaning, and validation of survey data.\nAnalysis and Reporting: Producing summaries, tables, and conclusions from the processed data."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-62",
    "href": "static/slides/Sampling/Survey.html#section-62",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3.2. Advantages of Stratified Sampling:\n\nImproves precision by reducing variability within strata.\nAllows the use of different sampling procedures in different strata.\nUseful for skewed populations where larger sampling fractions are required for certain strata.\n\n3.3. Proportional Allocation:\n\nProportional allocation is used when each stratum‚Äôs sample size is proportional to the size of the stratum in the population.\nFormula: \\[n_h = \\frac{N_h}{N} \\times n\\] where:\n\n\\(n_h\\) = sample size for stratum h\n\\(N_h\\) = population size for stratum h\nN = total population size\nn = total sample size"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-63",
    "href": "static/slides/Sampling/Survey.html#section-63",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3.4. Optimum Allocation:\n\nIn optimum allocation, strata with higher variability receive larger sample sizes to minimize overall variance.\nFormula: \\[n_h = \\frac{W_h s_h}{\\sum W_h s_h} \\times n\\] where:\n\\(W_h\\) = weight of stratum h (proportion of the population in the stratum)\n\\(s_h\\) = standard deviation of stratum h\nn = total sample size"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-64",
    "href": "static/slides/Sampling/Survey.html#section-64",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3.5. Estimation of Mean and Variance:\n\nStratified sample mean:\n\n\\[\\bar{x}_{st} = \\sum W_h \\bar{x}_h\\]\nwhere \\(W_h\\) is the weight of each stratum, and \\(\\bar{x}_h\\) is the mean of the sample in each stratum.\n\nVariance of the overall mean:\n\n\\[V(\\bar{x}_{st}) = \\sum \\frac{W_h^2 \\sigma_h^2}{n_h}\\]\nwhere \\(\\sigma_h^2\\) is the variance within each stratum."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-65",
    "href": "static/slides/Sampling/Survey.html#section-65",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "4. Cluster Sampling:\n4.1. Definition and Concept:\n\nCluster sampling involves selecting groups (clusters) of units rather than individual units directly. This method is often used when a list of the entire population is unavailable, but a list of clusters (e.g., villages or blocks) is available.\nAfter selecting clusters, all units within selected clusters are surveyed.\n\n4.2. Advantages of Cluster Sampling:\n\nReduces cost and time associated with data collection.\nAllows for more efficient fieldwork as data collection is concentrated in selected clusters.\n\n4.3. Disadvantages of Cluster Sampling:\n\nIncreases variance due to intra-cluster homogeneity (similarity between units within the same cluster)."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-66",
    "href": "static/slides/Sampling/Survey.html#section-66",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "4.4. Estimation in Cluster Sampling:\n\nCluster mean:\n\n\\[\\bar{x}_c = \\frac{1}{n_c} \\sum x_{ij}\\]\nwhere \\(n_c\\) is the number of clusters, and \\(x_{ij}\\) is the value of the \\(j^{th}\\) unit in cluster iii.\n\nVariance of the cluster mean:\n\n\\[V(\\bar{x}_c) = \\frac{\\sigma_c^2}{n_c}\\] where \\(\\sigma_c^2\\) is the variance within clusters."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-67",
    "href": "static/slides/Sampling/Survey.html#section-67",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "5. Systematic Sampling:\n5.1. Definition:\n\nIn systematic sampling, every \\(k^{th}\\) element from a list is selected, starting from a randomly chosen element.\nSampling interval k is calculated as: \\(k = \\frac{N}{n}\\) where N is the population size, and n is the sample size.\n\n5.2. Advantages of Systematic Sampling:\n\nSimple to implement.\nProvides implicit stratification if the population list is ordered according to some variable."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-68",
    "href": "static/slides/Sampling/Survey.html#section-68",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "5.3. Disadvantages:\n\nIf there is periodicity in the data, systematic sampling may result in biased estimates.\n\n5.4. Estimation in Systematic Sampling:\n\nSample mean:\n\n\\[\\bar{x}_{sys} = \\frac{1}{n} \\sum x_i\\]\nwhere \\(x_i\\) are the selected units.\n\nVariance estimation: Systematic sampling can be treated as simple random sampling for variance estimation if there is no periodicity:\n\n\\[V(\\bar{x}_{sys}) = \\frac{s^2}{n}\\]\nwhere \\(s^2\\) is the variance of the sample."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#annex-i-overview-of-sample-survey-design",
    "href": "static/slides/Sampling/Survey.html#annex-i-overview-of-sample-survey-design",
    "title": "Survey Methodology Design",
    "section": "Annex I: Overview of Sample Survey Design",
    "text": "Annex I: Overview of Sample Survey Design\n1. Introduction to Survey Sampling:\n\nSurvey sampling involves selecting a subset of the population to represent the entire group. It allows researchers to estimate population parameters without surveying the entire population.\nTypes of sampling:\n\nProbability Sampling: Each unit in the population has a known, non-zero chance of being selected.\nNon-Probability Sampling: Selection is based on subjective criteria and not every unit has a chance of being selected (focus of this annex is on probability sampling)."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-69",
    "href": "static/slides/Sampling/Survey.html#section-69",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "6. Comparison of Sampling Methods:\n\nSimple Random Sampling (SRS): Every unit has an equal chance of selection. It‚Äôs the baseline method but rarely used in large-scale surveys due to cost and logistical difficulties.\nStratified Sampling: More efficient than SRS when the population has distinct subgroups. It provides better precision, especially when there is high variability between strata.\nCluster Sampling: Cost-effective but increases variance due to similarities within clusters.\nSystematic Sampling: Easy to implement and works well if there is no periodicity in the population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-70",
    "href": "static/slides/Sampling/Survey.html#section-70",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "2. Importance of Sample Design:\n\nSample design is integral to ensuring that survey results are reliable and generalizable to the entire population.\nKey considerations in sample design:\n\nSample size and structure: Influences the precision and reliability of survey estimates.\nCost: Efficient design minimizes cost while ensuring accurate results.\n\n\n3. Stratified Sampling:\n3.1. Definition and Concept:\n\nStratified sampling divides the population into subgroups (strata) that are internally homogeneous but externally heterogeneous.\nThis technique reduces variability within each stratum and increases the precision of survey estimates."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-71",
    "href": "static/slides/Sampling/Survey.html#section-71",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3.2. Advantages of Stratified Sampling:\n\nImproves precision by reducing variability within strata.\nAllows the use of different sampling procedures in different strata.\nUseful for skewed populations where larger sampling fractions are required for certain strata.\n\n3.3. Proportional Allocation:\n\nProportional allocation is used when each stratum‚Äôs sample size is proportional to the size of the stratum in the population.\nFormula: \\[n_h = \\frac{N_h}{N} \\times n\\] where:\n\n\\(n_h\\) = sample size for stratum h\n\\(N_h\\) = population size for stratum h\nN = total population size\nn = total sample size"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-72",
    "href": "static/slides/Sampling/Survey.html#section-72",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3.4. Optimum Allocation:\n\nIn optimum allocation, strata with higher variability receive larger sample sizes to minimize overall variance.\nFormula: \\[n_h = \\frac{W_h s_h}{\\sum W_h s_h} \\times n\\] where:\n\\(W_h\\) = weight of stratum h (proportion of the population in the stratum)\n\\(s_h\\) = standard deviation of stratum h\nn = total sample size"
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-73",
    "href": "static/slides/Sampling/Survey.html#section-73",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "3.5. Estimation of Mean and Variance:\n\nStratified sample mean:\n\n\\[\\bar{x}_{st} = \\sum W_h \\bar{x}_h\\]\nwhere \\(W_h\\) is the weight of each stratum, and \\(\\bar{x}_h\\) is the mean of the sample in each stratum.\n\nVariance of the overall mean:\n\n\\[V(\\bar{x}_{st}) = \\sum \\frac{W_h^2 \\sigma_h^2}{n_h}\\]\nwhere \\(\\sigma_h^2\\) is the variance within each stratum."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-74",
    "href": "static/slides/Sampling/Survey.html#section-74",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "4. Cluster Sampling:\n4.1. Definition and Concept:\n\nCluster sampling involves selecting groups (clusters) of units rather than individual units directly. This method is often used when a list of the entire population is unavailable, but a list of clusters (e.g., villages or blocks) is available.\nAfter selecting clusters, all units within selected clusters are surveyed.\n\n4.2. Advantages of Cluster Sampling:\n\nReduces cost and time associated with data collection.\nAllows for more efficient fieldwork as data collection is concentrated in selected clusters.\n\n4.3. Disadvantages of Cluster Sampling:\n\nIncreases variance due to intra-cluster homogeneity (similarity between units within the same cluster)."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-75",
    "href": "static/slides/Sampling/Survey.html#section-75",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "4.4. Estimation in Cluster Sampling:\n\nCluster mean:\n\n\\[\\bar{x}_c = \\frac{1}{n_c} \\sum x_{ij}\\]\nwhere \\(n_c\\) is the number of clusters, and \\(x_{ij}\\) is the value of the \\(j^{th}\\) unit in cluster iii.\n\nVariance of the cluster mean:\n\n\\[V(\\bar{x}_c) = \\frac{\\sigma_c^2}{n_c}\\] where \\(\\sigma_c^2\\) is the variance within clusters."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-76",
    "href": "static/slides/Sampling/Survey.html#section-76",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "5. Systematic Sampling:\n5.1. Definition:\n\nIn systematic sampling, every \\(k^{th}\\) element from a list is selected, starting from a randomly chosen element.\nSampling interval k is calculated as: \\(k = \\frac{N}{n}\\) where N is the population size, and n is the sample size.\n\n5.2. Advantages of Systematic Sampling:\n\nSimple to implement.\nProvides implicit stratification if the population list is ordered according to some variable."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-77",
    "href": "static/slides/Sampling/Survey.html#section-77",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "5.3. Disadvantages:\n\nIf there is periodicity in the data, systematic sampling may result in biased estimates.\n\n5.4. Estimation in Systematic Sampling:\n\nSample mean:\n\n\\[\\bar{x}_{sys} = \\frac{1}{n} \\sum x_i\\]\nwhere \\(x_i\\) are the selected units.\n\nVariance estimation: Systematic sampling can be treated as simple random sampling for variance estimation if there is no periodicity:\n\n\\[V(\\bar{x}_{sys}) = \\frac{s^2}{n}\\]\nwhere \\(s^2\\) is the variance of the sample."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#section-78",
    "href": "static/slides/Sampling/Survey.html#section-78",
    "title": "Survey Methodology Design",
    "section": "",
    "text": "6. Comparison of Sampling Methods:\n\nSimple Random Sampling (SRS): Every unit has an equal chance of selection. It‚Äôs the baseline method but rarely used in large-scale surveys due to cost and logistical difficulties.\nStratified Sampling: More efficient than SRS when the population has distinct subgroups. It provides better precision, especially when there is high variability between strata.\nCluster Sampling: Cost-effective but increases variance due to similarities within clusters.\nSystematic Sampling: Easy to implement and works well if there is no periodicity in the population."
  },
  {
    "objectID": "static/slides/Sampling/Survey.html#sampling-and-sampling-strategies",
    "href": "static/slides/Sampling/Survey.html#sampling-and-sampling-strategies",
    "title": "Survey Methodology Design",
    "section": "Sampling and Sampling Strategies",
    "text": "Sampling and Sampling Strategies\nWhat is sampling?:\n\nSurveys typically do not involve collecting data from the entire population due to practical constraints.\nInstead, sampling strategies are employed to select a representative subset of the population, allowing researchers to make inferences about the broader population.\nThe goal of sampling is to ensure that the selected sample is representative and that the findings can be generalized with known levels of precision and accuracy."
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#what-is-a-shinydashboard",
    "href": "static/slides/shinydashboard/Shinydashboard.html#what-is-a-shinydashboard",
    "title": "",
    "section": "What is a shinydashboard?",
    "text": "What is a shinydashboard?\n\n\nShinydashboard, is a specialised package for creating dashboards.\noperates on the two fundamental elements of every Shiny app:\n\nthe User Interface (UI): controls the layout and appearance of dashboard\nthe server function: contains instructions needed to build dashboard\n\n\nA shinyApp() is also needed to integrate the UI and server together\nHowever, Shinydashboard distinguishes itself through its unique UI setup, offering a structured, customizable environment for data presentation and interaction.\n\ni.e., UI is defined by dashboardPage() and not fluidPage()\n\n\n\n\n\nCodeui&lt;- dashboardPage(header,sidebar,body)\nserver &lt;- function(input, output){\n    ...\n} \nshinyApp(ui, server)"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#key-components-of-the-user-interface-ui",
    "href": "static/slides/shinydashboard/Shinydashboard.html#key-components-of-the-user-interface-ui",
    "title": "",
    "section": "key components of the user interface ui,",
    "text": "key components of the user interface ui,\n\n\ndashboardHeader, dashboardSidebar, and dashboardBody functions.\nThe UI of a dashboard consists of following three main components and the code shows how they created\n\n\n\nthree main components as:\n\nthe header,\nthe sidebar, and\nthe body.\n\n\n\nCodeheader &lt;- dashboardHeader(...)\nsidebar &lt;- dashboardSidebar(...)\nbody &lt;- dashboardBody(...)\n# a shinydashboard ui\nui &lt;- dashboardPage(header, sidebar, body)\n\n\n\n\n\nThe dashboardPage function then integrates these elements within the UI.\nThe UI is like the restaurant and server is like the waiter\n\nIn particular, the UI can be further broken down into smaller components"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#an-empty-shinydashboard",
    "href": "static/slides/shinydashboard/Shinydashboard.html#an-empty-shinydashboard",
    "title": "",
    "section": "An empty shinydashboard",
    "text": "An empty shinydashboard\n\nCodelibrary(shiny) \nlibrary(shinydashboard)   \nheader &lt;- dashboardHeader(title = \"Basic Dashboard\", titleWidth = 400)  \nsidebar &lt;- dashboardSidebar()  \nbody &lt;- dashboardBody()  \nui&lt;-dashboardPage(header,sidebar,body) \nserver &lt;- function(input, output) {} \nshinyApp(ui, server)"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#dashboard-header-dashboardheader",
    "href": "static/slides/shinydashboard/Shinydashboard.html#dashboard-header-dashboardheader",
    "title": "",
    "section": "Dashboard Header dashboardHeader\n",
    "text": "Dashboard Header dashboardHeader\n\n\nThe dashboard header, created with dashboardHeader, is a visual element in shinydashboard‚Äôs layout.\n\nby default coloured blue.\n\n\nwe can change the color with argument skin = \"green\" in side the dashboardPage(), i.e., dashboardPage(skin = \"green\", header, sidebar, body)\n\n\n\n\nAdjust Title Width in dashboardHeader: The titleWidth argument in dashboardHeader allows customization of the title section‚Äôs width.\n\nA higher value provides more space for longer titles.\n\n\n\nDisabling the Header: To hide the header, use dashboardHeader(disable = TRUE).\n\nThis can be useful for minimalistic designs or specific use cases where the header is not required."
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#dashboard-sidebar-sidebarmenu",
    "href": "static/slides/shinydashboard/Shinydashboard.html#dashboard-sidebar-sidebarmenu",
    "title": "",
    "section": "Dashboard Sidebar sidebarMenu()\n",
    "text": "Dashboard Sidebar sidebarMenu()\n\n\nThe sidebar usually contains a sidebarMenu(), which needs a unique ID and a list of menu items.\nEach menuItem() consists of the title, a tabName that will be used to refer to the tab later, and an icon. There are a list of the available free icons at fontawesome.\nEach menuItem can correspond to different content displayed in the main body of the dashboard.\nIts width can also be adjusted to be in line with the header width.\nWe can also disable the sidebarMenu if it‚Äôs unwanted, by setting specifying: dashboardSidebar(disable = TRUE)"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#dashboard-body-dashboardbody",
    "href": "static/slides/shinydashboard/Shinydashboard.html#dashboard-body-dashboardbody",
    "title": "",
    "section": "Dashboard Body dashboardBody()\n",
    "text": "Dashboard Body dashboardBody()\n\n\nThe main part of the app goes inside dashboardBody().\nIt‚Äôs the area to displays outputs and accommodate user inputs.\nThe tabName has to match the name you used in the sidebar menuItem(), so that tab shows when the user clicks on the corresponding menu item.\n\nBody Structure\n\nTab items can be structured in several ways. At the simplest, you can just list each element after the tabName.\n\n\n\n\nCodetabItem(tabName = \"dashboard\",   \ntextInput(\"given\", \"Given Name\"), \ntextInput(\"surname\", \"Surname\"),\nselectInput(\"pet\", \"What is your favourite pet?\",\n                    c(\"cats\", \"dogs\", \"ferrets\")),\ntextAreaInput(\"bio\", NULL, height = \"100px\", \n    placeholder = \"brief bio\") )"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#static-vs-dynamic-content",
    "href": "static/slides/shinydashboard/Shinydashboard.html#static-vs-dynamic-content",
    "title": "",
    "section": "Static vs Dynamic Content",
    "text": "Static vs Dynamic Content\ninfoBox\n\ninfoBox is a UI component in shinydashboard designed to display key metrics, such as numerical or textual values, along with icons for visual emphasis.\nIt‚Äôs effective for showcasing statistics like user count, sales figures, or progress metrics.\nStatic infoBox displays fixed data, ideal for constants or baseline metrics.\nDynamic infoBox, enabled through infoBoxOutput and renderInfoBox, updates in response to user interactions or live data feeds.\n\nAppearance Customization:\n\nThe fill parameter alters the background fill.\n\nfill=FALSE (default) gives a cleaner look, while fill=TRUE provides a more pronounced, filled background."
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#types-of-inputs-in-shinydashboard",
    "href": "static/slides/shinydashboard/Shinydashboard.html#types-of-inputs-in-shinydashboard",
    "title": "",
    "section": "Types of Inputs in shinydashboard",
    "text": "Types of Inputs in shinydashboard\n\nInputs are ways that users can communicate information to the Shiny app.\n\n1.Free text\nCollect small amounts of text with textInput(), passwords with passwordInput(), and paragraphs of text with textAreaInput().\n\nCode  textInput(\"name\", \"What's your name?\"),\n  passwordInput(\"password\", \"What's your password?\"),\n  textAreaInput(\"story\", \"Tell me about yourself\", rows = 3)"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#outputs",
    "href": "static/slides/shinydashboard/Shinydashboard.html#outputs",
    "title": "",
    "section": "Outputs",
    "text": "Outputs\n\nOutput are ways that the Shiny app can dynamically display information to the user.\nOutputs in the UI create placeholders that are later filled by the server function.\nLike inputs, outputs take a unique ID as their first argument: if your UI specification creates an output with ID ‚Äúplot‚Äù, you‚Äôll access it in the server function with output$plot.\nEach output function on the front end is coupled with a render function in the back end.\nThere are three main types of output, corresponding to the three things you usually include in a report: text, tables, and plots.\nThe following sections show you the basics of the output functions on the front end, along with the corresponding render functions in the back end."
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#plots",
    "href": "static/slides/shinydashboard/Shinydashboard.html#plots",
    "title": "",
    "section": "Plots",
    "text": "Plots\nYou can display any type of R graphic (base, ggplot2, or otherwise) with plotOutput() and renderPlot():\n\nCode# in ui\nplotOutput(\"plot\", width = \"400px\")\n\n\n\nCode# in the server function \noutput$plot &lt;- renderPlot({\n    ggplot(iris, aes(x = Species, y = .data[[input$y]], color = Species)) +  \n        geom_violin(show.legend = FALSE) + stat_summary(show.legend = FALSE) + \n        ylab(input$y) })\n\n\n\nIf you want to create dynamic plots that change with input, note how you need to use y = .data[[input$y]] inside aes(), instead of just y = input$y."
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#images",
    "href": "static/slides/shinydashboard/Shinydashboard.html#images",
    "title": "",
    "section": "Images",
    "text": "Images\n\n\nimageOutput() takes the same arguments as plotOutput().\nYou can leave width and height as their defaults if you are going to set those values in the render function.\n\n\nCode# in the UI function \nimageOutput(\"demo_image\")\n\n\n\n\nrenderImage() needs to return a named list with at least an src with the image path.\nYou can also set the width and height (numeric values are in pixels), class and alt (the alt-text for screen readers).\n\n\nCode# in the server function \noutput$demo_image &lt;- renderImage({\n    list(src = \"images/flower.jpg\",  width = 100, height = 100, \n             alt = \"A flower\") }, deleteFile = FALSE)\n\n\n\nThe deleteFile argument is currently optional, but triggers periodic warnings that it won‚Äôt be optional in the future.\nYou should set it to TRUE if you‚Äôre making a temporary file (this stops unneeded plots using memory) and FALSE if you‚Äôre referencing a file you previously saved."
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#tables",
    "href": "static/slides/shinydashboard/Shinydashboard.html#tables",
    "title": "",
    "section": "Tables",
    "text": "Tables\nThere are two options for displaying data frames in tables:\n\ntableOutput() and renderTable() render a static table of data, showing all the data at once.\ndataTableOutput() and renderDataTable() render a dynamic table, showing a fixed number of rows along with controls to change which rows are visible.\ntableOutput() is most useful for small, fixed summaries (e.g.¬†model coefficients);\ndataTableOutput() is most appropriate if you want to expose a complete data frame to the user.\n\n\nCodeui &lt;- dashboardPage(\n    tableOutput(\"static\"),\n  dataTableOutput(\"dynamic\")\n    )\nserver &lt;- function(input, output) {\n      output$static &lt;- renderTable(head(mtcars))\n      output$dynamic &lt;- renderDataTable(mtcars, options = list(pageLength = 5))\n    }"
  },
  {
    "objectID": "static/slides/shinydashboard/Shinydashboard.html#dynamic-html",
    "href": "static/slides/shinydashboard/Shinydashboard.html#dynamic-html",
    "title": "",
    "section": "Dynamic HTML",
    "text": "Dynamic HTML\n\nIf you want to dynamically create parts of the UI, you can use uiOutput().\n\n\nCode# in the UI function \nuiOutput(\"demo_ui\")\n\n\n\nYou can create the UI using renderUI() to return HTML created.\n\n\nCode# in the server function \noutput$demo_ui &lt;- renderUI({cols &lt;- names(iris)[1:4]   \nselectInput(\"y\", \"Column to plot\", cols, \"Sepal.Length\") })\n\n\n\nThe function htmlOutput() is a synonym for uiOutput(), so you might see that in some code examples, but I use uiOutput() to make the connection with renderUI() clearer, since there is no renderHTML()."
  }
]