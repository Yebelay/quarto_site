<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>  Observational Studies for Impact Evaluation using R </title>
    <meta charset="utf-8" />
    <script src="Impact_evaluation_files/header-attrs/header-attrs.js"></script>
    <link href="Impact_evaluation_files/remark-css/default.css" rel="stylesheet" />
    <link href="Impact_evaluation_files/remark-css/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, title-slide

.title[
# <p><span style = 'font-size: 70%;'> <span style="color:white"> Observational Studies for Impact Evaluation using R </span></p>
]
.subtitle[
## <p><span style = 'font-size: 60%;'> Stellenbosch University</p>
]
.author[
### <p><span style = 'font-size: 80%;'><span style="color:#B00020"> Yebelay Berehan </span></p>
]
.institute[
### <p><span style="font-size: 50%;"> October 25, 2021</span></p>
]

---




<div>
<style type="text/css">.xaringan-extra-logo {
width: 95px;
height: 95px;
z-index: 0;
background-image: url(Image/logo.png);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:1em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>





### Public Health Policies and Interventions

- &lt;span style="color: #3498db"&gt;**Public health policies, programs, or interventions**&lt;/span&gt; are designed to change desired outcomes.
  - &lt;span style="color: blue"&gt;The ultimate goal is to measure the effect of the intervention.&lt;/span&gt;
- However, **estimating the impact of the intervention** is not an easy task.

- &lt;span style="color: #2ecc71"&gt;Ideally, we determine the **actual causal effect of treatment**&lt;/span&gt; or **impact of the intervention** by comparing outcomes under counterfactual conditions.

- The **counterfactual** condition refers to &lt;span style="color: #f39c12"&gt;what would have occurred to the treated unit&lt;/span&gt; in the absence of the intervention.
  - It represents an alternative scenario for comparison.

- The **comparison group** serves as the &lt;span style="color: #9b59b6"&gt;basis for estimating the **counterfactual effect**&lt;/span&gt;.
   - By comparing with this group, we grasp what outcomes might have been without the intervention.

---

- &lt;span style="color: #3498db"&gt;Ensuring the control and treatment groups are similar&lt;/span&gt; in both observed and unobserved aspects is critical but challenging.

- If control and treatment groups are similar, &lt;span style="color: #e74c3c"&gt;differences in outcome after the intervention&lt;/span&gt; can be confidently attributed to the intervention.

- &lt;span style="color: #2ecc71"&gt;Randomized evaluation of the intervention&lt;/span&gt; is the most reliable method for measuring its impact.
- However, &lt;span style="color: #f39c12"&gt;random assignment is not always possible due to ethical and feasibility issues.&lt;/span&gt;

- &lt;span style="color: #9b59b6"&gt;Quasi-experimental research designs&lt;/span&gt; provide alternative ways to measure intervention impact when randomization is not feasible (Hsueh-Sheng Wu, 2020).
---

- A randomized experiment is an experiment with the following properties:

**1. Positivity:** assignment is probabilistic: `\(0 &lt; p_i &lt; 1\)`
  - No deterministic assignment.
  - Making treatment or intervention groups as similar as possible within subgroups.

**2. Unconfoundedness:** `\(P[A_i = 1|Y(1), Y(0)] = P[A_i = 1]\)`
  - Treatment assignment does not depend on any potential outcomes (baseline characteristics and outcomes).
  - Sometimes written as `\(A_i \perp\!\!\!\perp (Y(1), Y(0))\)`

---
Why do Experiments Help?

`$$E[Y_i|D_i = 1]- E[Y_i|D_i = 0]$$`
`$$= E[Y_i(1)|D_i = 1]- E[Y_i(0)|D_i = 0]$$`
`$$= E[Y_i(1)|D_i = 1]- E[Y_i(0)|D_i = 1] + E[Y_i(0)|D_i = 1]- E[Y_i(0)|D_i = 0]$$`
`$$= \underbrace{E[Y_i(1)- Y_i(0)|D_i = 1]}_{ATT}+ \underbrace{E[Y_i(0)|D_i = 1]- E[Y_i(0)|D_i = 0]}_{Selection \, bias}$$`
- In an experiment we know that treatment is randomly assigned. Thus we can do
the following:

`$$E[Y_i(1)|D_i = 1] - E[Y_i(0)|D_i = 0]$$`
`$$= E[Y_i(1)|D_i = 1]- E[Y_i(0)|D_i = 1]$$`
`$$= E[Y_i(1)] - E[Y_i(0)]$$`
- When all goes well, an experiment eliminates selection bias.
---

### Why we need observational studies to evaluate interventions

- Randomized controlled studies are considered the `gold standard` for causal effects estimation but are at times:
  
  1. Unnecessary
  2. Inappropriate - HIV as a causal agent for AIDS; smoking as a cause of lung cancer
  3. Impossible - infrequent outcomes and rare events
  4. Inadequate
  
#### What are the main challenges of causal inferencein observational study?

1. `Lack of control`
   - lack of balance - confounding bias
   - lack of comparability - selection bias
2. `Unmeasured confounders`
3. `Time-varying confounders`

---

### Estimating causal effects from observational data

- Unlike randomized experiments, in observational studies researchers cannot assign study subjects into treatment or control groups using a random mechanism, 
   - makes it very difficult to draw a causal relationship between the treatment and the observed outcomes. 
 
- Therefore, to control confounding and arrive at a causal estimate, broadly there are two approaches
  - **Use statistical adjustment:** (rely on the assumption, no remaining unmeasured confounding) 
  - **Use design-based methods:** (to address unmeasured confounding)

---

- In observational studies treatment is not independent of potential outcomes
- Individuals receiving treatment A may not be comparable to those receiving treatment B 
  - sicker, older, poorer, less adherent 
  - differences in outcomes may be a reflection of these differences
- Difference of observed average response may be a biased estimate of the causal effect

**Solution:**

1. Identify all confounders `\(W\)`
2. Potential responses `\((Y_0; Y_1)\)` are independent of treatment exposure among subject with the same `\(W\)` values
3. Estimate differences within strata
---


### What is confounding?

- Confounding is the bias caused by common causes of the treatment
and outcome.
   - arise when exposure and outcome share an uncontrolled common cause. 
-	Observed confounders refer to confounders for which measures are available in the study data. 

-	Residual confounding is any confounding bias that remains after conditioning on observed confounders, either due to 
  - variables not observed in the data (unmeasured or unobserved confounding),
  - inadequate measurement or 
  - modelling of observed confounders.

- In observational studies, the goal is to avoid confounding inherent in the data.
- No unmeasured confounding assumes that we've measured all sources of confounding.

---

#### The most common methods to reduce the impact of confounding are:
  - **Restricting** the study sample to one level of the confounding variable,
  - **Stratifying** (analysing each gender separately) or 
  - **Matching** (selecting the sample so that the exposed and unexposed groups have the same gender balance). 

#### Other methods for confounder adjustment include:

- **multivariable regression** (including confounders as covariates) and 
- **inverse probability** (or propensity score) weighting.
- **G-methods:** 
  -	`G-computation` uses a statistical model (eg, a regression model) 
     - relies on the statistical model being correctly specified. 
  -	`Marginal structural models ` (commonly using IPWT).
  -	`G-estimation` - predicts the counterfactual outcome at each time point.
  
---

### Causal effects estimation in observational studies

- Analyze the data as if treatment was randomized, conditional on measured covariates.

&lt;span style="color:red"&gt; Condition 1: **exchangeability** &lt;/span&gt;

- In marginally randomized experiments, `\(Y^a|A\)`
- In conditionally randomized experiments, `\(Y^a \perp\!\!\!\perp A|L\)`
- In observational studies
   - Reasons for receiving treatment are likely associated with some outcome predictors
   - Distribution of outcome predictors vary between treated and untreated groups. 
   - Conditional exchangeability will not hold if there exits unmeasured independent predictors U of outcome such that probability of receiving treatment A depends on U with strata L
   - Exchangeability is not verifiable in observational studies
   
---

&lt;span style="color:red"&gt; Condition 2: **positivity** &lt;/span&gt;

- There is probability greater than zero of being assigned to each of the treatment levels `\(Pr[A = a] &gt; 0\)` or `\(Pr[A = a|L = l] &gt; 0,\forall l: Pr[L = l] \neq 0\)`

- In observational studies, positivity is not guaranteed, however, it can be sometimes empirically verified

&lt;span style="color:red"&gt; Condition 3: **Consistency** &lt;/span&gt;

- A defined standardized treatment exists with no variation (no multiple versions of the same treatment)
- In observational data, we have no control over the versions of treatments - use restriction

---

### Causality with Unmeasured Confounding

#### Unmeasured Confounding
Consider cases of measured confounding
&lt;img src="Image/fig9.png" width="10%" style="display: block; margin: auto;" /&gt;

- In this case we block the backdoor path `\(T \leftarrow X \rightarrow Y\)` by conditioning on X.
- What happens in the general case where X is unobserved?

-	The above methods rely on an assumption of no unmeasured confounding 
   - (ie, `conditional exchangeability`), **which is often not plausible in observational study designs**.
   
---

-	If there exist **unmeasured confounders** that may be a common cause of both the outcome and the treatment, 

   - impossible to accurately estimate the causal effect using above methods. 
   - We will use Design methods.
   - These designs are often called quasi-experimental designs or natural experiments:    
      - `difference-in-difference (DiD)`, 
      - `regression discontinuity (RD)`, and 
      - `instrumental variables (IV)`
 
#### Problem 
- Subject to certain unprovable assumptions, 
- By exploiting some assignment mechanism 

---

### Difference-in-Differences (DiD)

- The DiD design is a quasi-experimental alternative to the well understood and straight forward RCT design.
   - use data from treatment and control groups to obtain an appropriate counterfactual to estimate a causal effect.

- DiD methods exploit variation in time `(before vs. after)` and across groups `(treated vs. untreated)` to recover causal effects of interest.

- &lt;span style="color:#5B1845"&gt; Pre vs. Post comparisons &lt;/span&gt;
  -  `Compares:` same individuals before and after program.
  -  **Limitation:** Does not account for potential trends in outcomes.
- &lt;span style="color:#5B1845"&gt;Treated vs. Untreated comparisons &lt;/span&gt;
  -  `Compares:` participants to those who have not experienced treatment (at least not yet).
  -  **Limitation:** Selection - is participation driven by other factors?

---

- DiD combines these two approaches to **avoid their pitfalls**.

- The DiD approach includes a before-after comparison for a treatment and control group. 

- This is a combination of:
  - a cross-sectional comparison (= compare a sample that was treated to an non-treated control group)
  - a before-after comparison (= compare treatment group with itself, before and after the treatment)

- The before-after difference in the treatment group gets a correction, by accounting for the before-after difference in the control group, eliminating the trend problem. 

---

#### DiD- Graphically
.pull-left[
![](Impact_evaluation_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]
--
.pull-right[
![](Impact_evaluation_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]
--
.pull-left[
![](Impact_evaluation_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]
--
.pull-right[
![](Impact_evaluation_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

&lt;img src="Image/fig_4.png" width="60%" style="display: block; margin: auto auto auto 0;" /&gt;

- Data requirements:
  - Panel or Repeated cross-sectional data
    - Two groups: control and treatment groups 
    - Two time periods: baseline, follow-up/end line (Kreif, Grieve et al.2016).

---

#### Derivation

- We assume that the outcome is determined by `\(Y_{it}= c_i+d_t+\delta D_{it}+\eta_{it}\)`,
where i indexes the unit of observation and t indexes time. 

- `\(c_i\)` is a variable(s) do not change by time but do change by unit. 

- `\(d_t\)` is a variable(s) do not change by unit but can change by time.

- `\(\eta_{it}\)` is an unexplained, random error.

---

#### Differencing

1) Treated group after and before: 

`\(E[Y_{i1}|D_i=1]-E[Y_{i0}|D_i=1]=c_i+d_1+\delta_1-(c_i+d_0+\delta_0)\)`

2) Control group after and before: 

`\(E[Y_{i1}|D_i= 0]- E[Y_{i0}|D_i = 0]= c_i+d_1-(c_i+d_0)= d_1-d_0\)` 
	
- The difference of the differences (1)-(2) is `\(ATE = d_1-d_0+ \delta -(d_1-d_0) = \delta\)`.

- we can extend our notation to condition for a vector of covariates `\(X_{it}\)`, 
- So now we have: &lt;span style="color:#B00020"&gt; `$$Y_{it}=c_i+d_t+\delta D_{it}+ X_{0it}\beta+\eta_{it}$$` &lt;/span&gt;. 

---

#### Estimation

- We often do a before and after comparison, even when we have more time points.
  - So we only need four means to estimate a DiD design. 
- A before and after comparison of outcome Y for the treated is: `\(E[Y_{tpost}] - E[Y_{tpre}]\)`. 

- We want to compare that difference with the difference in the control: `\(E[Y_{cpost}] - E[Y_{cpre}]\)`. 
- The estimate of interest is: `\(DiD=E[Y_{tpost}]-E[Y_{tpre}]-{E[Y_{cpost}]-E[Y_{cpre}]}\)`.

- No regression model here yet, but we could estimate those four means    
   - parametrically or nonparametrically or semiparametrically. 

- The difference above is the same as: `\(DiD=E[Y_{tpost}]-E[Y_{cpost}]-{E[Y_{tpre}]-E[Y_{cpre}]}\)`

---

### Assumptions of DiD

#### 1. Parallel Trend Assumption

- Without treatment, the average change/trend in the outcome variable would be the same in the two groups (Mora 2015).

&lt;img src="Image/fig_1.png" width="50%" style="display: block; margin: auto;" /&gt;
- To obtain an unbiased estimate of the treatment effect one needs to make **a parallel trend assumption**.
  - i.e. Treatment group would have had the same changes as the control group in absence of the treatment (counterfactual outcome).
  
---

#### 2. Stable unit treatment value assumption (SUTVA)

- Subject's potential outcome depends only on its own treatment status, not by treatment status by the other unit.

#### 3. No Spill-Over Effects assumption 

- The members of the comparison group should not be affected by the intervention.

#### 4. Covariate balance test

On average, both `observable` and `unobservable` characteristics should not vary between both groups.

---

### DiD Practicalities in : 2 Period, 2 Group Design

#### Supported Software
- &lt;span style="color: #3498db"&gt;Difference-in-Differences (DiD) analysis can be performed in major statistical software packages&lt;/span&gt; like R, SAS, SPSS, Stata, etc.

- Shortcut in User-Written Command
   - For example, in Stata, a user-written command "diff" can simplify DiD analysis:
```{}
diff y, t(treated) p(time)
```

---

#### Practical Example: Card &amp; Krueger (1994) Study
- &lt;span style="color: #3498db"&gt;Using the dataset from Card &amp; Krueger (1994) study&lt;/span&gt;.
- The study aimed to &lt;span style="color: #e74c3c"&gt;estimate the effect on employment&lt;/span&gt; in fast food restaurants caused by an increase in the minimum wage (about 19%) in New Jersey in 1992.

- Data obtained from &lt;span style="color: #2ecc71"&gt;February 1992 (before) and November 1992 (after)&lt;/span&gt;.
- They used data from &lt;span style="color: #f39c12"&gt;similar fast food restaurants in Pennsylvania&lt;/span&gt;, which did not experience the minimum wage change.

- The key elements of this study include:
  - Two-period DiD: &lt;span style="color: #9b59b6"&gt;time&lt;/span&gt;
  - A policy change or treatment: &lt;span style="color: #3498db"&gt;increase in minimum wage&lt;/span&gt;
  - A treated group: &lt;span style="color: #e74c3c"&gt;fast food restaurants in New Jersey&lt;/span&gt;

---

Proportion of each fast food chain within each state.

``` r
library(readr)
practicaldata &lt;- read_csv("data_practice.csv")
practicaldata %&gt;% select(chain, state) %&gt;%  table() %&gt;%
  prop.table(margin = 2)  %&gt;%  apply(MARGIN = 2,
        FUN = scales::percent_format(accuracy = 0.1)) %&gt;%  noquote
```

```
##         state
## chain    New Jersey Pennsylvania
##   bk     40.7%      41.3%       
##   kfc    21.4%      14.3%       
##   roys   24.9%      24.6%       
##   wendys 13.1%      19.8%
```
---

- Calculate **pre test means** of certain variables of the first wave of the survey grouped by each state 

``` r
library(tidyr)
practicaldata %&gt;%
  filter(observation == "February 1992") %&gt;%  group_by(state) %&gt;%
  summarise(emptot = mean(emptot, na.rm = TRUE),
            pct_fte  = mean(pct_fte, na.rm = TRUE),
            wage_st = mean(wage_st, na.rm = TRUE),
            hrsopen = mean(hrsopen, na.rm = TRUE)) %&gt;%
  pivot_longer(cols=-state, names_to = "variable") %&gt;%
  pivot_wider(names_from = state, values_from = value)
```

```
## # A tibble: 4 × 3
##   variable `New Jersey` Pennsylvania
##   &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;
## 1 emptot          20.6         23.4 
## 2 pct_fte         34.7         33.3 
## 3 wage_st          4.61         4.65
## 4 hrsopen         14.4         14.2
```
---

- Calculate **post test means** of certain variables of the first wave of the survey grouped by each state 

``` r
practicaldata %&gt;%
  filter(observation == "November 1992") %&gt;%
  group_by(state) %&gt;%
  summarise(emptot = mean(emptot, na.rm = TRUE),
            pct_fte  = mean(pct_fte, na.rm = TRUE),
            wage_st = mean(wage_st, na.rm = TRUE),
            hrsopen = mean(hrsopen, na.rm = TRUE)) %&gt;%
  pivot_longer(cols=-state, names_to = "variable") %&gt;%
  pivot_wider(names_from = state, values_from = value)
```

```
## # A tibble: 4 × 3
##   variable `New Jersey` Pennsylvania
##   &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;
## 1 emptot          21.2         21.6 
## 2 pct_fte         36.0         31.0 
## 3 wage_st          5.09         4.64
## 4 hrsopen         14.3         14.8
```

---

### Calculating the treatment effect


``` r
differences &lt;- practicaldata %&gt;%
  group_by(observation, state) %&gt;%
  summarise(emptot = mean(emptot, na.rm = TRUE))
njfeb &lt;- differences[1,3]# Treatment group (NJ) before treatment
pafeb &lt;- differences[2,3]# Control group (PA) before treatment
njnov &lt;- differences[3,3]# Treatment group (NJ) after treatment
panov &lt;- differences[4,3]# Control group (PA) after treatment
```

- The Average Treatment Effect (ATT) in this setting can be determined in two ways:

calculate the difference between the difference of November and February within NJ and PA or difference between the difference of NJ and PA within November and February.


``` r
round((njnov-njfeb)-(panov-pafeb), 3)
```

```
##   emptot
## 1  2.483
```

---

- The DiD estimator is also obtained using regression model with an interaction between both dummy variables of time and treatment. 



``` r
did_model &lt;- lm(emptot ~ time + treated + time:treated, data = practicaldata)
summary(did_model)
```

```{}
Call:
lm(formula = emptot ~ time + treated + time:treated, data = practicaldata)

Residuals:
    Min      1Q  Median      3Q     Max 
-15.925  -6.102  -1.102   4.280  64.398 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    23.425      1.168  20.055   &lt;2e-16 ***
time           -1.865      1.652  -1.129   0.2593    
treated        -2.823      1.297  -2.177   0.0298 *  
*time:treated    2.483      1.842   1.348   0.1781    
----
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
````

- **&lt;span style="color: #9b59b6"&gt;The finding was quite the contrary- the $0.80 minimum wage increase in New Jersey led to a 2.483 full-time equivalent (FTE) increase in employment.&lt;/span&gt;**
---

#### How Did Estimate Unbiased?

- How can we trust the DID estimate if there is baseline imbalance or selection bias?

- Is including covariate in DID regression sufficient for effect estimation?

- What if groups are not comparable at baseline? Quasi-experimental studies and concerns for internal validity.

- Creating a counterfactual comparison group similar to the treatment group.

- Utilizing &lt;span style="color: blue;"&gt;Propensity Score (PS)&lt;/span&gt; matching and then estimating the treatment effect.

Because there are issues like:
  - &lt;span style="color: green;"&gt;Baseline Imbalance:&lt;/span&gt; Uneven distribution of covariates between treatment and control groups at the beginning.
  - &lt;span style="color: green;"&gt;Selection Bias:&lt;/span&gt; Non-random assignment of subjects to treatment and control groups.
  - &lt;span style="color: green;"&gt;Solution:&lt;/span&gt; Robustness checks, sensitivity analysis, and additional controls.

---

- Baseline Comparability: Ideally, treatment and control groups should be comparable at baseline.
- Internal Validity Concerns: If groups are not comparable, the validity of causal inference can be compromised.
- Addressing Concerns: &lt;span style="color: red;"&gt;Propensity Score matching&lt;/span&gt; can be used to create comparable groups.

#### Propensity Score Matching

- &lt;span style="color: blue;"&gt;Propensity Score (PS):&lt;/span&gt; The probability of receiving treatment given covariates.

- &lt;span style="color: blue;"&gt;PS Matching:&lt;/span&gt; Matching treated and untreated units based on their PS, creating balanced comparison groups.

- Estimation: After matching, we can estimate treatment effects within each matched group.

---

Estimate Propensity Scores: Use the *MatchIt package* to estimate propensity scores for each observation.


``` r
propensity_model &lt;- glm(treated ~ wage_st+pct_fte+hrsopen, data = practicaldata, 
                        family = "binomial")
practicaldata$propensity_score &lt;- predict(propensity_model, type = "response")
summary(practicaldata$propensity_score)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.6303  0.7331  0.8504  0.8047  0.8688  0.9727
```

---

- Perform Matching: Use the MatchIt package to perform matching based on propensity scores.


``` r
library(MatchIt)
matched_data &lt;- matchit(treated ~ propensity_score, data = practicaldata, 
                        method = "nearest")
summary(matched_data)
```

```
## 
## Call:
## matchit(formula = treated ~ propensity_score, data = practicaldata, 
##     method = "nearest")
## 
## Summary of Balance for All Data:
##                  Means Treated Means Control Std. Mean Diff. Var. Ratio
## distance                0.8133        0.7692          0.5375     0.8576
## propensity_score        0.8137        0.7675          0.5532     0.9364
##                  eCDF Mean eCDF Max
## distance            0.1768   0.3483
## propensity_score    0.1768   0.3483
## 
## Summary of Balance for Matched Data:
##                  Means Treated Means Control Std. Mean Diff. Var. Ratio
## distance                0.8805        0.7692          1.3577     0.0189
## propensity_score        0.8899        0.7675          1.4680     0.0572
##                  eCDF Mean eCDF Max Std. Pair Dist.
## distance            0.5353   0.9286          1.3595
## propensity_score    0.5353   0.9286          1.4720
## 
## Sample Sizes:
##           Control Treated
## All           126     519
## Matched       126     126
## Unmatched       0     393
## Discarded       0       0
```

---

DiD Analysis with Matched Data: Perform the DiD analysis using linear regression models on the matched dataset.


``` r
# Extract matched data from the matchit object
matched_data &lt;- match.data(matched_data)
model_matched &lt;- lm(emptot ~ time + treated + time:treated, data = matched_data)
# View the results
summary(model_matched)
```

```
## 
## Call:
## lm(formula = emptot ~ time + treated + time:treated, data = matched_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -17.361  -6.995  -1.560   5.114  47.075 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    23.425      1.273  18.396   &lt;2e-16 ***
## time           -1.865      1.801  -1.036    0.301    
## treated        -1.063      2.325  -0.457    0.648    
## time:treated    1.999      2.839   0.704    0.482    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 10.11 on 248 degrees of freedom
## Multiple R-squared:  0.004323,	Adjusted R-squared:  -0.007721 
## F-statistic: 0.3589 on 3 and 248 DF,  p-value: 0.7827
```

---

### Regression Discontinuity Design

- Regression discontinuity design (RDD) is a quasi-experimental method used to estimate the causal effect of an intervention by examining the impact of a threshold on an outcome variable.
- Many programs determine eligibility through the use of continuous indices or scores:

- Anti-poverty programs `\(\Rightarrow\)` Targets households under a specific income level or poverty line
- Education  `\(\Rightarrow\)` Scholarship for the best students based on a standardized  test
- Agriculture `\(\Rightarrow\)` Fertilizer program targeted to small farms less than given number of hectares)
   - Farmers with `\(\le 50\)` hectares are eligible
   - Farmers with `\(&gt; 50\)` hectares are ineligible

---

#### Regression Discontinuity Design 

.pull-left[
 - **At Baseline**
 
&lt;img src="Image/fig10.png" width="80%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
- **At Post Intervention**
&lt;img src="Image/fig11.png" width="60%" style="display: block; margin: auto;" /&gt;
]

- We need a continuous eligibility index with a defined eligibility cutoff point

- The basic idea is to compare the outcomes of individuals who are just above and below a threshold that determines whether they receive an intervention or not. 

- RDD is useful when it is impossible or unethical to randomly assign individuals to treatment and control groups.

---

- RDD can be used to establish causal inference by controlling for confounding variables that might otherwise distort the causal relationship. 
- In particular, RDD can control for selection bias that can occur when individuals self-select or are selected for an intervention based on their characteristics.

- The key steps in RDD are as follows:

1. **Identify the threshold:** Determine the threshold value of a continuous variable that separates individuals into treatment and control groups.
2. **Determine the outcome variable:** Specify the outcome variable that reflects the impact of the intervention.

- Then estimate the causal effect using a regression model by comparing the outcomes of individuals just above and below the threshold.

- Test the robustness of the results `by varying the bandwidth of the threshold`, controlling for additional covariates, and checking for violations of assumptions.

---

### Practice in R



---

### Instrumental Variables

-	An IV is a variable that causes some variation in the exposure and is unrelated to the outcome except through the exposure.

- IVs are often used to identify the causal effect of a particular exposure or treatment on an outcome of interest. 

- If we have an instrument, we can deal with unmeasured confounding in the treatment-outcome relationship.

- It is going to turn out that the same construction will let us deal with non-compliance in experiments.

---
#### Assumptions for using IV

1. The **IV is correlated with the exposure** or treatment of interest: 
   - The IV should be able to predict the level of the exposure or treatment. 

2. The IV is **not directly related to the outcome**: 
   - The IV should only be associated with the outcome through its effect on the exposure or treatment. 
   
3. The IV is **independent of the error term**: 
   - There should be *no correlation between the IV and the error term* in the outcome equation. 
   - If these assumptions are met, then using IVs can lead to more accurate estimates of the causal effect of interest.

---

#### Assumption

&lt;img src="Image/fig12.png" width="50%" style="display: block; margin: auto;" /&gt;

- In Figure, Z is an IV to the effect of X on outcome. 
- IV relies on three main conditions. 
- A valid IV, Z, must (i) predict treatment status (**relevance**), (ii) only affect outcome through X (**exclusion**), and (iii) be as good as randomly assigned (**independence**). 
- These conditions are met as there's a causal path `\(Z \rightarrow X\)` and no open paths between Z and outcome except through X.

---

### Practice in R
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"ratio": "14:9",
"highlightSpans": true,
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
